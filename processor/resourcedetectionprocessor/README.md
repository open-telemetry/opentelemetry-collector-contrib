# Resource Detection Processor

<!-- status autogenerated section -->
| Status        |           |
| ------------- |-----------|
| Stability     | [development]: profiles   |
|               | [beta]: traces, metrics, logs   |
| Distributions | [contrib], [k8s] |
| Issues        | [![Open issues](https://img.shields.io/github/issues-search/open-telemetry/opentelemetry-collector-contrib?query=is%3Aissue%20is%3Aopen%20label%3Aprocessor%2Fresourcedetection%20&label=open&color=orange&logo=opentelemetry)](https://github.com/open-telemetry/opentelemetry-collector-contrib/issues?q=is%3Aopen+is%3Aissue+label%3Aprocessor%2Fresourcedetection) [![Closed issues](https://img.shields.io/github/issues-search/open-telemetry/opentelemetry-collector-contrib?query=is%3Aissue%20is%3Aclosed%20label%3Aprocessor%2Fresourcedetection%20&label=closed&color=blue&logo=opentelemetry)](https://github.com/open-telemetry/opentelemetry-collector-contrib/issues?q=is%3Aclosed+is%3Aissue+label%3Aprocessor%2Fresourcedetection) |
| Code coverage | [![codecov](https://codecov.io/github/open-telemetry/opentelemetry-collector-contrib/graph/main/badge.svg?component=processor_resourcedetection)](https://app.codecov.io/gh/open-telemetry/opentelemetry-collector-contrib/tree/main/?components%5B0%5D=processor_resourcedetection&displayType=list) |
| [Code Owners](https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/CONTRIBUTING.md#becoming-a-code-owner)    | [@Aneurysm9](https://www.github.com/Aneurysm9), [@dashpole](https://www.github.com/dashpole) \| Seeking more code owners! |

[development]: https://github.com/open-telemetry/opentelemetry-collector/blob/main/docs/component-stability.md#development
[beta]: https://github.com/open-telemetry/opentelemetry-collector/blob/main/docs/component-stability.md#beta
[contrib]: https://github.com/open-telemetry/opentelemetry-collector-releases/tree/main/distributions/otelcol-contrib
[k8s]: https://github.com/open-telemetry/opentelemetry-collector-releases/tree/main/distributions/otelcol-k8s
<!-- end autogenerated section -->

The resource detection processor can be used to detect resource information from the host,
in a format that conforms to the [OpenTelemetry resource semantic conventions](https://github.com/open-telemetry/semantic-conventions/tree/main/docs/resource), and append or
override the resource value in telemetry data with this information.

> **Note**
>
> If a configured resource detector fails in some way, the error it returns to the processor will propagate and stop the collector from starting. This behavior is configurable using a feature gate, however the error behavior of each independent resource detector may vary.
>
> This feature can be controlled with [feature gate](https://github.com/open-telemetry/opentelemetry-collector/tree/main/featuregate) `processor.resourcedetection.propagateerrors`. It is currently enabled by default (beta stage).
>
>  Example of how to disable it:
> ```shell-session
> $ otelcol --config=config.yaml --feature-gates=-processor.resourcedetection.propagateerrors
> ```

## Feature gates

See [documentation.md](./documentation.md) for the complete list of feature gates supported by this processor.

Feature gates can be enabled using the `--feature-gates` flag:

```shell
"--feature-gates=<feature-gate>"
```

## Common Use Cases

### Basic Local Development

Detect host system information and environment variables:

```yaml
processors:
  resourcedetection:
    detectors: [env, system]
    timeout: 2s
```

### AWS EKS Cluster

Detect EKS-specific attributes along with EC2 instance metadata:

```yaml
processors:
  resourcedetection:
    detectors: [env, system, eks, ec2]
    timeout: 15s
    eks:
      resource_attributes:
        k8s.cluster.name:
          enabled: true
    ec2:
      tags:
        - ^kubernetes.io/cluster/.*$
        - ^Environment$
```

### AWS Lambda Function

Detect Lambda-specific attributes:

```yaml
processors:
  resourcedetection:
    detectors: [env, lambda]
    timeout: 200ms
```

### Google Kubernetes Engine (GKE)

Detect GKE and GCP metadata:

```yaml
processors:
  resourcedetection:
    detectors: [env, gcp, system]
    timeout: 2s
```

### Azure AKS Cluster

Detect AKS-specific attributes along with Azure metadata:

```yaml
processors:
  resourcedetection:
    detectors: [env, system, aks, azure]
    timeout: 2s
    aks:
      resource_attributes:
        k8s.cluster.name:
          enabled: true
    azure:
      tags:
        - ^Environment$
        - ^Team$
```

### Docker Container

Detect Docker-specific metadata (Linux only):

```yaml
processors:
  resourcedetection:
    detectors: [env, docker, system]
    timeout: 2s
```

### Multi-Cloud with Fallback

Safely attempt detection across multiple cloud providers:

```yaml
processors:
  resourcedetection:
    # Order matters: more specific detectors first
    detectors: [env, system, eks, aks, gcp]
    timeout: 5s
    override: false
 
    # Run with error propagation disabled to allow partial detection
    # Command: otelcol --config=config.yaml --feature-gates=-processor.resourcedetection.propagateerrors
```
 
### Enrichment without Override
 
Add resource attributes only if they don't already exist:
 
```yaml
processors:
  resourcedetection:
    detectors: [env, system]
    override: false # Preserve existing attributes from instrumentation
```

### Periodic Resource Refresh

Refresh resource detection every 15 minutes (use with caution):

```yaml
processors:
  resourcedetection:
    detectors: [ec2]
    timeout: 5s
    refresh_interval: 15m
    ec2:
      tags:
        - ^.*$ # Refresh all tags
```

## Supported detectors

### Environment Variable

Reads resource information from the `OTEL_RESOURCE_ATTRIBUTES` environment
variable. This is expected to be in the format `<key1>=<value1>,<key2>=<value2>,...`, the
details of which are currently pending confirmation in the OpenTelemetry specification.

Example:

```yaml
processors:
  resourcedetection/env:
    detectors: [env]
    timeout: 2s
    override: false
```

### System metadata

Note: use the Docker detector (see below) if running the Collector as a Docker container.

Queries the host machine to retrieve the system related resource attributes. The list of the
populated resource attributes can be found at [System Detector Resource Attributes](./internal/system/documentation.md).

By default `host.name` is being set to FQDN if possible, and a hostname provided by OS used as fallback.
This logic can be changed with `hostname_sources` configuration which is set to `["dns", "os"]` by default.

Use the following config to avoid getting FQDN and apply hostname provided by OS only:

```yaml
processors:
  resourcedetection/system:
    detectors: ["system"]
    system:
      hostname_sources: ["os"]
```

* all valid options for `hostname_sources`:
    * "dns"
    * "os"
    * "cname"
    * "lookup"

#### Hostname Sources

##### dns

The "dns" hostname source uses multiple sources to get the fully qualified domain name. First, it looks up the
host name in the local machine's `hosts` file. If that fails, it looks up the CNAME. Lastly, if that fails,
it does a reverse DNS query. Note: this hostname source may produce unreliable results on Windows. To produce
a FQDN, Windows hosts might have better results using the "lookup" hostname source, which is mentioned below.

##### os

The "os" hostname source provides the hostname provided by the local machine's kernel.

##### cname

The "cname" hostname source provides the canonical name, as provided by net.LookupCNAME in the Go standard library.
Note: this hostname source may produce unreliable results on Windows.

##### lookup

The "lookup" hostname source does a reverse DNS lookup of the current host's IP address.

### Docker metadata

Queries the Docker daemon to retrieve resource attributes from the host machine.
The list of the populated resource attributes can
be found at [Docker Detector Resource Attributes](./internal/docker/documentation.md).

You need to mount the Docker socket (`/var/run/docker.sock` on Linux) to contact the Docker daemon.
Docker detection does not work on macOS.

Example:

```yaml
processors:
  resourcedetection/docker:
    detectors: [env, docker]
    timeout: 2s
    override: false
```

#### Docker Socket Permissions

Since version 0.40.0, official OpenTelemetry Collector images run as a non-root user. To access the Docker socket, you need to configure appropriate permissions:

- **Linux**: Grant access to the `docker` group (e.g., `--group-add <docker-gid>` or set `runAsGroup` in Kubernetes)
- **Windows**: Ensure appropriate named pipe permissions

For detailed permission configuration options and security considerations, see the [Docker Stats receiver documentation](../../receiver/dockerstatsreceiver/README.md#docker-socket-permissions).

For more information, see [issue #11791](https://github.com/open-telemetry/opentelemetry-collector-contrib/issues/11791).

### Heroku metadata

When [Heroku dyno metadata is active](https://devcenter.heroku.com/articles/dyno-metadata), Heroku applications publish information through environment variables.

We map these environment variables to resource attributes as follows:

| Dyno metadata environment variable | Resource attribute                  |
|------------------------------------|-------------------------------------|
| `HEROKU_APP_ID`                    | `heroku.app.id`                     |
| `HEROKU_APP_NAME`                  | `service.name`                      |
| `HEROKU_DYNO_ID`                   | `service.instance.id`               |
| `HEROKU_RELEASE_CREATED_AT`        | `heroku.release.creation_timestamp` |
| `HEROKU_RELEASE_VERSION`           | `service.version`                   |
| `HEROKU_SLUG_COMMIT`               | `heroku.release.commit`             |

For more information, see the [Heroku cloud provider documentation](https://github.com/open-telemetry/semantic-conventions/blob/main/docs/resource/cloud-provider/heroku.md) under the [OpenTelemetry specification semantic conventions](https://github.com/open-telemetry/semantic-conventions).

The list of the populated resource attributes can be found at [Heroku Detector Resource Attributes](./internal/heroku/documentation.md).

```yaml
processors:
  resourcedetection/heroku:
    detectors: [env, heroku]
    timeout: 2s
    override: false
```

### GCP Metadata

Uses the [Google Cloud Client Libraries for Go](https://github.com/googleapis/google-cloud-go)
to read resource information from the [metadata server](https://cloud.google.com/compute/docs/storing-retrieving-metadata) and environment variables to detect which GCP platform the
application is running on, and detect the appropriate attributes for that platform. Regardless
of the GCP platform the application is running on, use the gcp detector:

Example:

```yaml
processors:
  resourcedetection/gcp:
    detectors: [env, gcp]
    timeout: 2s
    override: false
```

The list of the populated resource attributes can be found at [GCP Detector Resource Attributes](./internal/gcp/documentation.md).

#### GCE Metadata

    * cloud.provider ("gcp")
    * cloud.platform ("gcp_compute_engine")
    * cloud.account.id (project id)
    * cloud.region  (e.g. us-central1)
    * cloud.availability_zone (e.g. us-central1-c)
    * host.id (instance id)
    * host.name (instance name)
    * host.type (machine type)
    * (optional) gcp.gce.instance.hostname
    * (optional) gcp.gce.instance.name

#### GKE Metadata

    * cloud.provider ("gcp")
    * cloud.platform ("gcp_kubernetes_engine")
    * cloud.account.id (project id)
    * cloud.region (only for regional GKE clusters; e.g. "us-central1")
    * cloud.availability_zone (only for zonal GKE clusters; e.g. "us-central1-c")
    * k8s.cluster.name
    * host.id (instance id)
    * host.name (instance name; only when workload identity is disabled)

One known issue is when GKE workload identity is enabled, the GCE metadata endpoints won't be available, thus the GKE resource detector won't be
able to determine `host.name`. In that case, users are encouraged to set `host.name` from either:
- `node.name` through the downward API with the `env` detector
- obtaining the Kubernetes node name from the Kubernetes API (with `k8s.io/client-go`)

#### Google Cloud Run Services Metadata

    * cloud.provider ("gcp")
    * cloud.platform ("gcp_cloud_run")
    * cloud.account.id (project id)
    * cloud.region (e.g. "us-central1")
    * faas.instance (instance id)
    * faas.name (service name)
    * faas.version (service revision)

#### Cloud Run Jobs Metadata

    * cloud.provider ("gcp")
    * cloud.platform ("gcp_cloud_run")
    * cloud.account.id (project id)
    * cloud.region (e.g. "us-central1")
    * faas.instance (instance id)
    * faas.name (service name)
    * gcp.cloud_run.job.execution ("my-service-ajg89")
    * gcp.cloud_run.job.task_index ("0")

#### Google Cloud Functions Metadata

    * cloud.provider ("gcp")
    * cloud.platform ("gcp_cloud_functions")
    * cloud.account.id (project id)
    * cloud.region (e.g. "us-central1")
    * faas.instance (instance id)
    * faas.name (function name)
    * faas.version (function version)

#### Google App Engine Metadata

    * cloud.provider ("gcp")
    * cloud.platform ("gcp_app_engine")
    * cloud.account.id (project id)
    * cloud.region (e.g. "us-central1")
    * cloud.availability_zone (e.g. "us-central1-c")
    * faas.instance (instance id)
    * faas.name (service name)
    * faas.version (service version)

### AWS EC2

Uses [AWS SDK for Go](https://docs.aws.amazon.com/sdk-for-go/api/aws/ec2metadata/) to read resource information
from the [EC2 instance metadata API](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-metadata.html) to retrieve related resource attributes:

The list of the populated resource attributes can be found at [EC2 Detector Resource Attributes](./internal/aws/ec2/documentation.md).

It also can optionally gather tags for the EC2 instance that the collector is running on.
Note that in order to fetch EC2 tags, the IAM role assigned to the EC2 instance must have a policy that includes the `ec2:DescribeTags` permission.

EC2 custom configuration example:
```yaml
processors:
  resourcedetection/ec2:
    detectors: ["ec2"]
    ec2:
      # A list of regex's to match tag keys to add as resource attributes can be specified
      tags:
        - ^tag1$
        - ^tag2$
        - ^label.*$
```

If you are using a proxy server on your EC2 instance, it's important that you exempt requests for instance metadata as [described in the AWS cli user guide](https://github.com/awsdocs/aws-cli-user-guide/blob/a2393582590b64bd2a1d9978af15b350e1f9eb8e/doc_source/cli-configure-proxy.md#using-a-proxy-on-amazon-ec2-instances). Failing to do so can result in proxied or missing instance data.

If the instance is part of AWS ParallelCluster and the detector is failing to connect to the metadata server, check the iptable and make sure the chain `PARALLELCLUSTER_IMDS` contains a rule that allows OTEL user to access `169.254.169.254/32`

In some cases, you might need to change the behavior of the AWS metadata client from the [standard retryer](https://docs.aws.amazon.com/sdk-for-go/v2/developer-guide/configure-retries-timeouts.html)

By default, the client retries 3 times with a max backoff delay of 20s.

We offer a limited set of options to override those defaults specifically, such that you can set the client to retry 10 times, for up to 5 minutes, for example:

```yaml
processors:
  resourcedetection/ec2:
    detectors: ["ec2"]
    ec2:
      max_attempts: 10
      max_backoff: 5m
```

The EC2 detector will report an error in logs if the EC2 metadata endpoint is unavailable. You can configure the detector to instead fail with this flag:

```yaml
processors:
  resourcedetection/ec2:
    detectors: ["ec2"]
    ec2:
      fail_on_missing_metadata: true
```

### Amazon ECS

Queries the [Task Metadata Endpoint](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-metadata-endpoint.html) (TMDE) to record information about the current ECS Task. Only TMDE V4 and V3 are supported.

The list of the populated resource attributes can be found at [ECS Detector Resource Attributes](./internal/aws/ecs/documentation.md).

Example:

```yaml
processors:
  resourcedetection/ecs:
    detectors: [env, ecs]
    timeout: 2s
    override: false
```

### Amazon Elastic Beanstalk

Reads the AWS X-Ray configuration file available on all Beanstalk instances with [X-Ray Enabled](https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environment-configuration-debugging.html).

The list of the populated resource attributes can be found at [Elastic Beanstalk Detector Resource Attributes](./internal/aws/elasticbeanstalk/documentation.md).

Example:

```yaml
processors:
  resourcedetection/elastic_beanstalk:
    detectors: [env, elastic_beanstalk]
    timeout: 2s
    override: false
```

### Amazon EKS

This detector reads resource information from the [EC2 instance metadata service](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-metadata.html) to retrieve related resource attributes.
If IMDS is not available, (example: EKS-AutoMode and POD not on the hostnetwork), it falls back to a combination of [Kubernetes API](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#-strong-kubernetes-api-v1-25-strong-)
and [EC2 API](https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_DescribeInstances.html) to retrieve related resource attributes.

EC2 API requires the `EC2:DescribeInstances` permission to be granted to the IAM role. If IMDS is not accessible, ex: EKS-AutoMode, you can use [POD Identity](https://docs.aws.amazon.com/eks/latest/userguide/pod-identities.html).

The list of the populated resource attributes can be found at [EKS Detector Resource Attributes](./internal/aws/eks/documentation.md).

Example:

```yaml
processors:
  resourcedetection/eks:
    detectors: [env, eks]
    timeout: 15s
    override: false
```

#### Cluster Name

Cluster name detection is disabled by default, and can be enabled with the
following configuration:

```yaml
processors:
  resourcedetection/eks:
    detectors: [env, eks]
    timeout: 15s
    override: false
    eks:
      resource_attributes:
        k8s.cluster.name:
          enabled: true
```

Note: The kubernetes cluster name is only available when running on EC2 instances, and requires permission to run the `EC2:DescribeInstances` [action](https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_DescribeInstances.html).
If you see an error with the message `context deadline exceeded`, please increase the timeout setting in your config.

#### Node Name Env Variable
When using the EC2 API and the Kubernetes API to retrieve resource attributes, the node name is needed. The node name is extracted from the env variable you define on the pod.
The node name env variable that contains the node name value can be set using the `node_from_env_var` option:

```yaml
processors:
  resourcedetection/eks:
    detectors: [eks]
    timeout: 15s
    override: false
    eks:
      node_from_env_var: K8S_NODE_NAME
```
In this example, the env variable `K8S_NODE_NAME` will hold the actual node name and can be set in the pod spec using the downward API.

```yaml
        env:
          - name: K8S_NODE_NAME
            valueFrom:
              fieldRef:
                fieldPath: spec.nodeName
```

### AWS Lambda

Uses the AWS Lambda [runtime environment variables](https://docs.aws.amazon.com/lambda/latest/dg/configuration-envvars.html#configuration-envvars-runtime)
to retrieve related resource attributes.

The list of the populated resource attributes can be found at [Lambda Detector Resource Attributes](./internal/aws/lambda/documentation.md).

Example:

```yaml
processors:
  resourcedetection/lambda:
    detectors: [env, lambda]
    timeout: 0.2s
    override: false
```

### Azure

Queries the [Azure Instance Metadata Service](https://aka.ms/azureimds) to retrieve related attributes.

The list of the populated resource attributes can be found at [Azure Detector Resource Attributes](./internal/azure/documentation.md).

Example:

```yaml
processors:
  resourcedetection/azure:
    detectors: [env, azure]
    timeout: 2s
    override: false
```

It also can optionally gather tags from the Azure instance that the Collector is running on.

Azure custom configuration example:

```yaml
processors:
  resourcedetection/azure:
    detectors: ["azure"]
    azure:
      # A list of regex's to match tag keys to add as resource attributes can be specified
      tags:
        - ^tag1$
        - ^tag2$
        - ^label.*$
```

Matched tags are added as:

    * azure.tags.<tag name>

### Azure AKS

The list of the populated resource attributes can be found at [AKS Detector Resource Attributes](./internal/azure/aks/documentation.md).

```yaml
processors:
  resourcedetection/aks:
    detectors: [env, aks]
    timeout: 2s
    override: false
```

#### Cluster Name

Cluster name detection is disabled by default, and can be enabled with the
following configuration:

```yaml
processors:
  resourcedetection/aks:
    detectors: [aks]
    timeout: 2s
    override: false
    aks:
      resource_attributes:
        k8s.cluster.name:
          enabled: true
```

Azure AKS cluster name is derived from the Azure Instance Metadata Service's (IMDS) infrastructure resource group field. This field contains the resource group and name of the cluster, separated by underscores. e.g: `MC_<resource group>_<cluster name>_<location>`.

Example:
  - Resource group: my-resource-group
  - Cluster name:   my-cluster
  - Location:       eastus
  - Generated name: MC_my-resource-group_my-cluster_eastus

The cluster name is detected if it does not contain underscores and if a custom infrastructure resource group name was not used.

If accurate parsing cannot be performed, the infrastructure resource group value is returned. This value can be used to uniquely identify the cluster, as Azure will not allow users to create multiple clusters with the same infrastructure resource group name.

### Consul

Queries a [consul agent](https://www.consul.io/docs/agent) and reads its [configuration endpoint](https://www.consul.io/api-docs/agent#read-configuration) to retrieve related resource attributes:

The list of the populated resource attributes can be found at [Consul Detector Resource Attributes](./internal/consul/documentation.md).

In addition to:

  * *exploded consul metadata* - reads all key:value pairs in [consul metadata](https://www.consul.io/docs/agent/options#_node_meta) into label:labelvalue pairs.

```yaml
processors:
  resourcedetection/consul:
    detectors: [env, consul]
    timeout: 2s
    override: false
```

### Kubeadm Metadata

Queries the K8S API server to retrieve kubeadm resource attributes:

The list of the populated resource attributes can be found at [kubeadm Detector Resource Attributes](./internal/kubeadm/documentation.md).

---

### Oracle Cloud Infrastructure (OCI) metadata

The OCI detector implements a *fast probe* to the instance metadata service (IMDS) endpoint to quickly verify if the collector is running on OCI. If this probe fails, the detector returns an empty resource and no error. If the probe succeeds, it then fetches instance metadata; if this fetch fails, the detector logs and returns an error so that partial detection is not silently ignored. This behavior makes it possible to differentiate between the case where the collector is not running on OCI, vs it is running on OCI but the IMDS request failed.

Queries the [Oracle Cloud Infrastructure (OCI) metadata service](https://docs.oracle.com/en-us/iaas/Content/Compute/Tasks/gettingmetadata.htm)
to retrieve resource attributes related to the OCI instance environment.

The list of the populated resource attributes can be found at [OracleCloud Detector Resource Attributes](./internal/oraclecloud/documentation.md).

Example:

```yaml
processors:
  resourcedetection/oraclecloud:
    detectors: [env, oraclecloud]
    timeout: 2s
    override: false
```

**Populated resource attributes:**
- `cloud.provider`
- `cloud.platform`
- `cloud.region`
- `cloud.availability_zone`
- `host.id`
- `host.name`
- `host.type`
- `k8s.cluster.name`

See [internal/oraclecloud/documentation.md](./internal/oraclecloud/documentation.md) for detailed attribute definitions.

---

The following permissions are required:
```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: otel-collector
  namespace: kube-system
rules:
  - apiGroups: [""]
    resources: ["configmaps"]
    resourceNames: ["kubeadm-config"]
    verbs: ["get"]
  - apiGroups: [""]
    resources: ["namespaces"]
    resourceNames: ["kube-system"]
    verbs: ["get"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: otel-collector-rolebinding
  namespace: kube-system
subjects:
- kind: ServiceAccount
  name: default
  namespace: default
roleRef:
  kind: Role
  name: otel-collector
  apiGroup: rbac.authorization.k8s.io
```

| Name | Type | Required | Default         | Docs                                                                                                                                                                                                                                   |
| ---- | ---- |----------|-----------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| auth_type | string | No       | `serviceAccount` | How to authenticate to the K8s API server.  This can be one of `none` (for no auth), `serviceAccount` (to use the standard service account token provided to the agent pod), or `kubeConfig` to use credentials from `~/.kube/config`. |

### K8S Node Metadata

Queries the K8S api server to retrieve node resource attributes.

The list of the populated resource attributes can be found at [k8snode Detector Resource Attributes](./internal/k8snode/documentation.md).

The following permissions are required:
```yaml
kind: ClusterRole
metadata:
  name: otel-collector
rules:
  - apiGroups: [""]
    resources: ["nodes"]
    verbs: ["get", "list"]
```

| Name | Type | Required | Default         | Docs                                                                                                                                                                                                                                   |
| ---- | ---- |----------|-----------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| auth_type | string | No       | `serviceAccount` | How to authenticate to the K8s API server.  This can be one of `none` (for no auth), `serviceAccount` (to use the standard service account token provided to the agent pod), or `kubeConfig` to use credentials from `~/.kube/config`. |
| node_from_env_var | string | Yes      | `K8S_NODE_NAME` | The environment variable name that holds the name of the node to retrieve metadata from. Default value is `K8S_NODE_NAME`. You can set the env dynamically on the workload definition using the downward API; see example              |

#### Example using the default `node_from_env_var` option:

```yaml
processors:
  resourcedetection/k8snode:
    detectors: [k8snode]
```
and add this to your workload:
```yaml
        env:
          - name: K8S_NODE_NAME
            valueFrom:
              fieldRef:
                fieldPath: spec.nodeName
```

#### Example using a custom variable `node_from_env_var` option:
```yaml
processors:
  resourcedetection/k8snode:
    detectors: [k8snode]
    k8snode:
      node_from_env_var: "my_custom_var"
```
and add this to your workload:
```yaml
        env:
          - name: my_custom_var
            valueFrom:
              fieldRef:
                fieldPath: spec.nodeName
```

### OpenShift

Queries the OpenShift and Kubernetes API to retrieve related resource attributes.

The list of the populated resource attributes can be found at [OpenShift Detector Resource Attributes](./internal/openshift/documentation.md).

The following permissions are required:
```yaml
kind: ClusterRole
metadata:
  name: otel-collector
rules:
- apiGroups: ["config.openshift.io"]
  resources: ["infrastructures", "infrastructures/status"]
  verbs: ["get", "watch", "list"]
```

By default, the API address is determined from the environment variables `KUBERNETES_SERVICE_HOST`, `KUBERNETES_SERVICE_PORT` and the service token is read from `/var/run/secrets/kubernetes.io/serviceaccount/token`.
If TLS is not explicit disabled and no `ca_file` is configured `/var/run/secrets/kubernetes.io/serviceaccount/ca.crt` is used.
The determination of the API address, ca_file and the service token is skipped if they are set in the configuration.

Example:

```yaml
processors:
  resourcedetection/openshift:
    detectors: [openshift]
    timeout: 2s
    override: false
    openshift: # optional
      address: "https://api.example.com"
      token: "token"
      tls:
        insecure: false
        ca_file: "/var/run/secrets/kubernetes.io/serviceaccount/ca.crt"
```

See: [TLS Configuration Settings](https://github.com/open-telemetry/opentelemetry-collector/blob/main/config/configtls/README.md) for the full set of available options.

### Dynatrace

Loads resource information from the `dt_host_metadata.properties` file which is located in
the `/var/lib/dynatrace/enrichment` (on *nix systems) or `%ProgramData%\dynatrace\enrichment` (on Windows) directories.
If present in the file, the following attributes will be added:

- `dt.entity.host`
- `host.name`
- `dt.smartscape.host`

The Dynatrace detector does not require any additional configuration, other than being added to the list of detectors.

Example:

```yaml
processors:
  resourcedetection/dynatrace:
    override: false
    detectors: [dynatrace]
```

It is strongly recommended to use the `override: false` configuration option, to prevent the detector from overwriting
existing resource attributes.
If the Dynatrace host entity identifier attribute `dt.entity.host`, `host.name`, or `dt.smartscape.host` are already present on incoming data as it is sent from
other sources to the collector, then these describe the monitored entity in the best way.
Overriding these with the collector's own identifier would instead make the telemetry appear as if it was coming from the collector
or the collector's host instead, which might be inaccurate.

### Hetzner

Uses the [Hetzner metadata API](https://docs.hetzner.cloud/reference/cloud#server-metadata) to read resource information from the instance metadata service and populate related resource attributes.

The list of the populated resource attributes can be found at [Hetzner Detector Resource Attributes](./internal/hetzner/documentation.md).

Hetzner custom configuration example:

```yaml
processors:
  resourcedetection/hetzner:
    detectors: ["hetzner"]
```

### Akamai

Uses the [Akamai metadata API](https://techdocs.akamai.com/cloud-computing/docs/metadata-service-api) to read resource information from the instance metadata service and populate related resource attributes.

The list of the populated resource attributes can be found at [Akamai Detector Resource Attributes](./internal/akamai/documentation.md).

Akamai custom configuration example:

```yaml
processors:
  resourcedetection/akamai:
    detectors: ["akamai"]
```

### Scaleway

Uses the Scaleway metadata API to read resource information from the instance metadata service and populate related resource attributes.

The list of the populated resource attributes can be found at [Scaleway Detector Resource Attributes](./internal/scaleway/documentation.md).

Scaleway custom configuration example:

```yaml
processors:
  resourcedetection/scaleway:
    detectors: ["scaleway"]
```

### Upcloud

Uses the [Upcloud metadata API](https://upcloud.com/docs/guides/upcloud-metadata-service/) to read resource information from the instance metadata service and populate related resource attributes.

The list of the populated resource attributes can be found at [Upcloud Detector Resource Attributes](./internal/upcloud/documentation.md).

Upcloud custom configuration example:

```yaml
processors:
  resourcedetection/upcloud:
    detectors: ["upcloud"]
```

The Upcloud detector will report an error in logs if the metadata endpoint is unavailable. You can configure the detector to instead fail with this flag:

```yaml
processors:
  resourcedetection/upcloud:
    detectors: ["upcloud"]
    upcloud:
      fail_on_missing_metadata: true
```

### Vultr

Uses the [Vultr metadata API](https://www.vultr.com/metadata/) to read resource information from the instance metadata service and populate related resource attributes.

The list of the populated resource attributes can be found at [Vultr Detector Resource Attributes](./internal/vultr/documentation.md).

Vultr custom configuration example:

```yaml
processors:
  resourcedetection/vultr:
    detectors: ["vultr"]
```

The Vultr detector will report an error in logs if the metadata endpoint is unavailable. You can configure the detector to instead fail with this flag:

```yaml
processors:
  resourcedetection/vultr:
    detectors: ["vultr"]
    vultr:
      fail_on_missing_metadata: true
```

### Digital Ocean

Uses the [Digital Ocean metadata API](https://docs.digitalocean.com/reference/api/metadata/) to read resource information from the instance metadata service and populate related resource attributes.

The list of the populated resource attributes can be found at [Digital Ocean Detector Resource Attributes](./internal/digitalocean/documentation.md).

Akamai custom configuration example:

```yaml
processors:
  resourcedetection/digitalocean:
    detectors: ["digitalocean"]
```

### Openstack Nova

Uses the [OpenStack Nova metadata API](https://docs.openstack.org/nova/latest/user/metadata.html) to read resource information from the instance metadata service and populate related resource attributes.

The list of the populated resource attributes can be found at [Nova Detector Resource Attributes](./internal/openstack/nova/documentation.md).

It can also optionally capture metadata keys from the `"meta"` section of `meta_data.json` as resource attributes, using regular expressions to match the keys you want.

Nova custom configuration example:
```yaml
processors:
  resourcedetection/nova:
    detectors: ["nova"]
    nova:
      # A list of regex's to match label keys to add as resource attributes can be specified
      labels:
        - ^tag1$
        - ^tag2$
        - ^label.*$
```

The Nova detector will report an error in logs if the metadata endpoint is unavailable. You can configure the detector to instead fail with this flag:

```yaml
processors:
  resourcedetection/nova:
    detectors: ["nova"]
    nova:
      fail_on_missing_metadata: true
```

### Alibaba Cloud ECS

Uses the [Alibaba Cloud metadata API](https://www.alibabacloud.com/help/en/ecs/user-guide/view-instance-metadata/?spm=a2c63.p38356.help-menu-25365.d_0_1_3_4_6.7d2848cfJpcLdU#393b14378evdm) to read resource information from the instance metadata service and populate related resource attributes.

The list of the populated resource attributes can be found at [Alibaba Cloud ECS Detector Resource Attributes](./internal/alibaba/ecs/documentation.md).

Alibaba Cloud ECS custom configuration example:

```yaml
processors:
  resourcedetection/alibaba_ecs:
    detectors: ["alibaba_ecs"]
```

The Alibaba Cloud ECS detector will report an error in logs if the metadata endpoint is unavailable. You can configure the detector to instead fail with this flag:

```yaml
processors:
  resourcedetection/alibaba_ecs:
    detectors: ["alibaba_ecs"]
    alibaba_ecs:
      fail_on_missing_metadata: true
```

## Production Guidance

### Deployment Considerations

The resource detection processor is designed to be stateless and safe for production use. Consider the following when deploying:

#### Scaling

- **Horizontal Scaling**: Each collector instance independently detects resource attributes for its own environment. No coordination between instances is required.
- **Resource Overhead**: Detection typically occurs once at startup (unless `refresh_interval` is configured). CPU and memory overhead is minimal after initial detection.
- **Startup Time**: Initial detection adds latency to collector startup (default timeout: 5 seconds per detector). Consider adjusting `timeout` settings based on your environment.

#### Deployment Best Practices

1. **Detector Selection**: Only enable detectors relevant to your deployment environment. Unnecessary detectors increase startup time without providing value.

2. **Timeout Configuration**: Set appropriate timeouts based on network conditions:
   ```yaml
   processors:
     resourcedetection:
       detectors: [eks, ec2]
       timeout: 10s # Increase for slower networks or complex environments
   ```

3. **Error Handling**: By default, detector failures prevent collector startup (when `processor.resourcedetection.propagateerrors` is enabled). To allow partial detection:
   ```shell
   otelcol --config=config.yaml --feature-gates=-processor.resourcedetection.propagateerrors
   ```

4. **Resource Attribute Conflicts**: When multiple detectors provide the same attribute, the first detector in the list wins. Order detectors intentionally (see [Ordering](#ordering) section).

#### Periodic Refresh Considerations

The `refresh_interval` parameter should be used with caution:

- **Metric Cardinality Impact**: Changes to resource attributes create new time series, significantly impacting storage costs and query performance
- **Recommended Intervals**:  
  - Minimum practical interval: 5 minutes (1 minute or less is strongly discouraged)
  - Default (0): No refresh; detection runs only at startup (recommended for most use cases)
- **Use Cases**: Only enable periodic refresh when resource attributes are expected to change during collector lifetime (e.g., dynamic cloud instance tags, Kubernetes labels)

### State Management

This processor is **stateless** and does not require persistent storage or special shutdown procedures. Resource attributes are:
- Detected on demand (startup or refresh cycle)
- Stored in memory only
- Applied to telemetry as it flows through the pipeline
- Not persisted across restarts

Graceful shutdown is handled automatically by the collector framework. No special configuration is required.

### Cloud Provider Considerations

#### AWS

- **EC2 Tags**: Requires `ec2:DescribeTags` IAM permission
- **EKS Cluster Name**: Requires `EC2:DescribeInstances` permission
- **IMDS Proxy**: Exempt instance metadata endpoint (`169.254.169.254`) from proxy configuration
- **ParallelCluster**: Ensure OTEL user has iptables access to IMDS

#### Azure

- **Instance Tags**: Available through IMDS; no special permissions required
- **AKS Cluster Name**: Derived from infrastructure resource group naming convention

#### GCP

- **GKE Workload Identity**: When enabled, `host.name` may be unavailable. Set via downward API or Kubernetes API

#### Kubernetes

- **RBAC Permissions**: See detector-specific documentation for required ClusterRole/Role permissions
- **Node Metadata**: Use downward API to pass node information via environment variables

## Configuration

### Basic Configuration

```yaml
processors:
  resourcedetection:
    # List of resource detectors to run
    # Valid options: "env", "system", "gcp", "ec2", "ecs", "elastic_beanstalk", "eks", 
    # "lambda", "azure", "aks", "heroku", "openshift", "dynatrace", "consul", "docker", 
    # "k8snode", "kubeadm", "hetzner", "akamai", "scaleway", "vultr", "oraclecloud", 
    # "digitalocean", "nova", "upcloud", "alibaba_ecs"
    detectors: [env, system]
 
    # Determines if existing resource attributes should be overridden or preserved
    # Default: true
    override: true
 
    # Timeout for each detector's detection operation
    # Default: 5s
    timeout: 5s
 
    # How often resource detection should be refreshed
    # Default: 0 (no refresh, detection runs only once at startup)
    # Caution: Frequent refreshes can increase metric cardinality
    refresh_interval: 0
```

### All Configuration Options

| Option | Type | Default | Description |
|--------|------|---------|-------------|
| `detectors` | []string | `["env"]` | Ordered list of detector names to run |
| `override` | bool | `true` | Whether to override existing resource attributes |
| `timeout` | duration | `5s` | Maximum time to wait for detector operations |
| `refresh_interval` | duration | `0` | Interval for periodic resource detection refresh (0 = disabled) |
| `<detector>` | object | varies | Detector-specific configuration (see detector documentation) |

For detailed detector-specific configuration options, see the individual detector sections above.

### Selective Attribute Detection

You have the ability to specify which detector should collect each attribute with `resource_attributes` option. An example of such a configuration is:

```yaml
resourcedetection:
  detectors: [system, ec2]
  system:
    resource_attributes:
      host.name:
        enabled: true
      host.id:
        enabled: false
  ec2:
    resource_attributes:
      host.name:
        enabled: false
      host.id:
        enabled: true
```

### Using the `refresh_interval` parameter

The `refresh_interval` option allows resource attributes to be periodically refreshed without restarting the Collector.

**Important considerations:**

- **Latency**: Newly detected resource attributes will be applied after the next refresh cycle completes (up to `refresh_interval` duration).
- **Metric cardinality**: Changes to resource attributes create new metric time series, which can significantly increase storage costs and query complexity.
- **Performance impact**: Each refresh re-runs all configured detectors. Values below 5 minutes can increase CPU and memory usage. There is no enforced minimum, but intervals below 1 minute are strongly discouraged.

**Recommendation**: In most environments, a single resource detection at startup is sufficient. Periodic refresh should be used only when resource attributes are expected to change during the Collector's lifetime (e.g., Kubernetes pod labels, cloud instance tags).

## Feature Gates

This processor uses feature gates to control certain behaviors. Feature gates can be enabled/disabled using the `--feature-gates` command-line flag.

### `processor.resourcedetection.propagateerrors`

**Status**: Beta (enabled by default)  
**Since**: v0.121.0

Controls whether errors returned from resource detectors propagate in the `Start()` method and prevent the collector from starting.

- **When enabled (default)**: If any configured detector fails, the error propagates and stops the collector from starting. This ensures partial or incorrect resource detection is not silently ignored.
- **When disabled**: Detector errors are logged but do not prevent collector startup. The collector will start with whatever resource attributes were successfully detected.

**Usage**:
```shell
# Disable error propagation (allow collector to start even if detection fails)
otelcol --config=config.yaml --feature-gates=-processor.resourcedetection.propagateerrors

# Explicitly enable error propagation (default behavior)
otelcol --config=config.yaml --feature-gates=processor.resourcedetection.propagateerrors
```

**When to use**: Disable this gate if you prefer collector availability over complete resource detection. This is useful in environments where:
- Some detectors may intermittently fail but the collector should continue operating
- Partial resource detection is acceptable
- You have fallback mechanisms for resource attribution

**Reference**: [Issue #37961](https://github.com/open-telemetry/opentelemetry-collector-contrib/issues/37961)

### `processor.resourcedetection.gcp.removefaasid`

**Status**: Beta  
**Since**: v0.115.0 (approximate)

Controls whether the deprecated `faas.id` attribute is removed from GCP detector output in favor of the semantic convention-compliant `faas.instance` attribute.

- **When enabled**: Only `faas.instance` is populated (recommended for new deployments)
- **When disabled**: Both `faas.id` and `faas.instance` are populated for backward compatibility

**Usage**:
```shell
# Enable removal of faas.id (use faas.instance only)
otelcol --config=config.yaml --feature-gates=processor.resourcedetection.gcp.removefaasid
```

**When to use**: Enable this gate when migrating to semantic convention-compliant attribute names for GCP Cloud Functions, Cloud Run, and App Engine environments.

## External Dependencies and Compatibility

### Cloud Provider SDKs

The resource detection processor relies on official cloud provider SDKs for metadata service interactions:

| Detector | Dependency | Supported Versions | Compatibility Notes |
|----------|------------|-------------------|---------------------|
| **AWS** (ec2, ecs, eks, lambda, elasticbeanstalk) | [AWS SDK for Go v2](https://github.com/aws/aws-sdk-go-v2) | v1.x | Compatible with all AWS regions. Requires IMDSv2 support for EC2/EKS. |
| **GCP** (gcp) | [Google Cloud Client Libraries](https://github.com/googleapis/google-cloud-go) | Latest stable | Compatible with GCE, GKE, Cloud Run, Cloud Functions, App Engine. |
| **Azure** (azure, aks) | Azure Instance Metadata Service (IMDS) | API version 2021-02-01+ | Direct HTTP calls to IMDS endpoint. No SDK dependency. |
| **Kubernetes** (k8snode, kubeadm, openshift) | [client-go](https://github.com/kubernetes/client-go) | v0.28+ | Compatible with Kubernetes 1.24+. Requires appropriate RBAC permissions. |

### Metadata Service Endpoints

Detectors communicate with the following endpoints. Ensure network policies allow access:

| Service | Endpoint | Required Access |
|---------|----------|----------------|
| AWS EC2/EKS IMDS | `http://169.254.169.254` | Link-local, should not be proxied |
| AWS ECS Task Metadata | `http://169.254.170.2`, `http://169.254.169.254/latest/meta-data/` | Set via `ECS_CONTAINER_METADATA_URI_V4` env var |
| GCP Metadata | `http://metadata.google.internal` | GCP-internal, available on GCP instances |
| Azure IMDS | `http://169.254.169.254` | Link-local, should not be proxied |
| Kubernetes API | Cluster-specific (e.g., `https://kubernetes.default.svc`) | Configured via service account or kubeconfig |

### Version Compatibility

**Collector Compatibility**: This processor is compatible with OpenTelemetry Collector v0.100.0+

**Semantic Conventions**: The processor follows [OpenTelemetry Semantic Conventions](https://github.com/open-telemetry/semantic-conventions) for resource attributes:
- **Current version**: 1.27.0+
- **Breaking changes**: Monitored via feature gates (e.g., `processor.resourcedetection.gcp.removefaasid`)
- **Upgrade path**: Feature gates provide opt-in migration for semantic convention changes

**Backward Compatibility Guarantees**:
- Configuration schema is backward compatible within major versions
- New detectors may be added in minor versions
- Detector behavior changes are gated behind feature flags
- Default detector (`env`) is guaranteed stable

### Network Requirements

- **Firewall Rules**: Allow outbound HTTP/HTTPS to metadata service endpoints
- **Proxy Configuration**: Exempt metadata service link-local addresses (169.254.x.x) from HTTP proxy
- **DNS**: Some detectors (system, gcp) perform DNS lookups for hostname resolution
- **Timeouts**: Default 5-second timeout per detector; adjust based on network latency

### Security Considerations

- **Credentials**: Most detectors use instance metadata without explicit credentials
- **IAM/RBAC**: Some features require additional permissions (see detector-specific documentation)
- **Sensitive Data**: Detector results may include internal identifiers; consider data governance policies
- **IMDS Access**: Link-local metadata services should not be accessible from application containers in multi-tenant environments

## Known Limitations and Common Pitfalls

### Limitations

1. **Cloud Metadata Service Availability**
   - Detectors that query cloud metadata services (EC2, Azure, GCP, etc.) will fail if the metadata endpoint is not accessible
   - No automatic retry mechanism exists; detection occurs only once per startup/refresh cycle
   - **Mitigation**: Use appropriate timeouts and consider disabling error propagation feature gate if partial detection is acceptable

2. **Detector Ordering Matters**
   - When multiple detectors set the same attribute, the first detector wins
   - No merging or conflict resolution occurs beyond first-wins
   - **Mitigation**: Carefully order detectors in the `detectors` list (see [Ordering](#ordering) section)

3. **Kubernetes RBAC Requirements**
   - Several detectors (k8snode, kubeadm, openshift, aks) require specific RBAC permissions
   - Permission errors are not always immediately obvious in logs
   - **Mitigation**: Ensure proper ClusterRole/Role configurations before deploying (see detector-specific documentation)

4. **GKE Workload Identity**
   - When GKE workload identity is enabled, IMDS is unavailable and `host.name` cannot be detected via GCP detector
   - **Mitigation**: Use downward API to pass `node.name` or fetch from Kubernetes API

5. **Docker Detection on macOS**
   - Docker detector does not work on macOS
   - **Mitigation**: Use system detector or other appropriate detectors for local development

6. **Schema URL Conflicts**
   - If telemetry arrives with a schema URL and detection adds a different schema URL, the existing one is preserved
   - This maintains semantic convention compatibility but may hide detection results
   - **Mitigation**: Ensure consistent semantic convention versions across your pipeline

### Common Misconfigurations

1. **Enabling All Detectors**
   ```yaml
   #  BAD: Enables detectors for all cloud providers
   detectors: [gcp, azure, ec2, ecs, eks, aks, ...]
   ```
   **Problem**: Unnecessary detectors slow startup and may cause errors  
   **Solution**: Only enable detectors for your actual environment:
   ```yaml
   #  GOOD: Only AWS EKS detectors
   detectors: [env, system, eks, ec2]
   ```

2. **Aggressive Refresh Intervals**
   ```yaml
   #  BAD: Refreshes every 30 seconds
   refresh_interval: 30s
   ```
   **Problem**: Creates excessive metric cardinality and API calls  
   **Solution**: Use refresh only when necessary, with reasonable intervals:
   ```yaml
   #  GOOD: Refresh every 10 minutes if needed at all
   refresh_interval: 10m # or 0 to disable
   ```

3. **Ignoring Detector Failures**
   ```yaml
   #  RISKY: Silently ignores detection errors
   # Command: otelcol --feature-gates=-processor.resourcedetection.propagateerrors
   ```
   **Problem**: May run with missing or incorrect resource attributes  
   **Solution**: Only disable error propagation if you have a specific reason and monitoring in place

4. **Not Configuring Timeouts for Slow Networks**
   ```yaml
   #  BAD: Uses default 5s timeout on slow network
   detectors: [eks] # EKS can be slow
   ```
   **Problem**: Legitimate detection may time out  
   **Solution**: Adjust timeout based on environment:
   ```yaml
   #  GOOD: Longer timeout for complex detection
   detectors: [eks]
   timeout: 15s
   ```

5. **Incorrect Permission for EC2 Tags**
   ```yaml
   # Configuration tries to fetch EC2 tags but IAM role lacks ec2:DescribeTags
   ```
   **Problem**: Silent failure or logged errors; tags not populated  
   **Solution**: Ensure IAM role/policy includes required permissions (see detector documentation)

6. **Wrong Override Setting**
   ```yaml
   #  BAD: Overrides high-quality resource attributes from instrumentation
   override: true
   detectors: [env, system]
   ```
   **Problem**: May replace accurate application-provided attributes with generic host attributes  
   **Solution**: Use `override: false` when instrumentation provides better resource information:
   ```yaml
   #  GOOD: Enriches without overwriting
   override: false
   detectors: [env, system]
   ```

## Performance

### Benchmark Tests

This component includes comprehensive benchmark tests for all stable signals. The benchmarks measure the performance of the processor under different configurations:

- **Traces**: `BenchmarkConsumeTracesDefault` and `BenchmarkConsumeTracesAll`
- **Metrics**: `BenchmarkConsumeMetricsDefault` and `BenchmarkConsumeMetricsAll`
- **Logs**: `BenchmarkConsumeLogsDefault` and `BenchmarkConsumeLogsAll`

To run the benchmarks locally:

```bash
go test -bench=. -benchmem
```

For the latest benchmark results, see the [GitHub Actions workflow runs](https://github.com/open-telemetry/opentelemetry-collector-contrib/actions/workflows/build-and-test.yml).

## Self-Observability

This processor emits internal telemetry to help users detect errors, data loss, and performance issues. The telemetry follows the OpenTelemetry Collector's internal observability standards.

### Metrics

The resourcedetection processor uses the standard `processorhelper` framework which automatically emits the following metrics for all signal types (traces, metrics, logs, profiles):

#### Data Flow Metrics

**Incoming Data** - Measures data received by the processor:
- `otelcol_processor_incoming_spans` (Counter, unit: `{span}`)
- `otelcol_processor_incoming_metric_points` (Counter, unit: `{datapoint}`)
- `otelcol_processor_incoming_log_records` (Counter, unit: `{record}`)

**Outgoing Data** - Measures data successfully processed and forwarded:
- `otelcol_processor_outgoing_spans` (Counter, unit: `{span}`)
- `otelcol_processor_outgoing_metric_points` (Counter, unit: `{datapoint}`)
- `otelcol_processor_outgoing_log_records` (Counter, unit: `{record}`)

**Attributes**: All metrics include:
- `processor`: Set to `"resourcedetection"`
- `service_name`: Collector service name
- `service_version`: Collector version

#### Error Detection

**Data Loss Indicators**:
- Difference between `incoming` and `outgoing` metrics indicates processing failures
- The processor does **not** drop data under normal operation; all incoming data is enriched and forwarded
- If detector errors occur during startup (when `processor.resourcedetection.propagateerrors` is enabled), the collector will not start, preventing silent data loss

**Error Conditions**:
- Detector failures during startup: Logged at ERROR level and prevent collector startup (by default)
- Detector timeouts: Logged at WARN level
- API failures: Logged at ERROR level with detailed error messages

#### Performance Monitoring

The processor has minimal performance impact:
- **Latency**: Resource detection occurs once at startup (or on refresh intervals)
- **Processing overhead**: Simple resource attribute enrichment per data item
- **No queueing**: Data flows through without buffering

To monitor performance:
1. **Startup latency**: Check collector startup time; long startups indicate slow detectors
2. **Processing throughput**: Compare `incoming` vs `outgoing` rates; should be equal under normal operation
3. **Detection duration**: Enable collector tracing to see spans for detector execution

### Logging

The resource detection processor emits structured logs for important events:

#### Log Levels and Messages

**INFO Level** - Normal operation events:
| Message Pattern | When Emitted | Context |
|----------------|--------------|---------|
| `"Detected resource attributes"` | Initial detection success | Includes detected attributes |
| `"refreshing resource attributes"` | Periodic refresh initiated | Only when `refresh_interval` > 0 |
| `"Starting periodic resource detection refresh"` | Refresh goroutine started | Includes refresh interval |

**WARN Level** - Non-fatal issues:
| Message Pattern | When Emitted | Action Required |
|----------------|--------------|-----------------|
| `"detector timed out"` | Detector exceeds timeout | Increase `timeout` configuration |
| `"detector returned empty resource"` | Detector found no attributes | Verify detector is applicable to environment |
| `"metadata endpoint unavailable"` | Cloud metadata service unreachable | Check network connectivity; may be expected if not in cloud |

**ERROR Level** - Fatal issues (when error propagation enabled):
| Message Pattern | When Emitted | Action Required |
|----------------|--------------|-----------------|
| `"failed to detect resource"` | Detector encountered error | Check detector configuration and permissions |
| `"detector initialization failed"` | Detector cannot start | Verify detector-specific requirements (RBAC, IAM, etc.) |
| `"periodic refresh failed"` | Refresh cycle error | Check API rate limits and permissions |

#### Error Details

All ERROR and WARN logs include:
- **Detector name**: Which detector failed
- **Error message**: Detailed error from the detector or cloud API
- **Context**: Relevant configuration or environment details

**Privacy**: Signal data (traces, metrics, logs) is never included in logs for security and privacy reasons.

### Tracing

When the collector's internal tracing is enabled (`traces` telemetry level), the processor emits spans for:

**Startup Phase**:
- Span: `"resourcedetection.Start"` - Overall startup operation
- Child spans for each detector's execution
- Attributes: detector names, detection duration, success/failure

**Processing Phase**:
- Spans for data processing operations are part of the collector's standard tracing
- Resource attribute enrichment time is negligible and not separately traced

**Refresh Phase** (if configured):
- Span: `"resourcedetection.Refresh"` - Periodic refresh operation
- Child spans for each detector re-run
- Attributes: refresh interval, detector results

**Trace Correlation**:
- All detector spans belong to the same trace context
- Failed detector spans have `error=true` attribute
- Span duration indicates detector performance

### Data Characteristics

**Normal Operation**:
- **No data loss**: All incoming data is enriched and forwarded
- **No data creation**: Processor only adds resource attributes, does not create new telemetry
- **No data filtering**: Processor does not drop or filter data
- **Deterministic**: Same input + same detected attributes = same output

**Discrepancies Between Input and Output**:
- None under normal operation; `incoming` = `outgoing` always
- If metrics show discrepancy, indicates a bug or collector pipeline issue (not expected)

**Data Held by Component**:
- **Zero buffering**: Processor does not queue or buffer data
- **Stateless**: No data is held between processing calls
- **Resource attributes cached**: Detected resource attributes are cached in memory (small, constant size)

### Monitoring Dashboard Recommendations

For production monitoring, track these key metrics:

#### Health Indicators
```promql
# Processing rate (should be non-zero when receiving traffic)
rate(otelcol_processor_outgoing_spans{processor="resourcedetection"}[5m])

# Data flow balance (should always be 0, indicating no data loss)
rate(otelcol_processor_incoming_spans{processor="resourcedetection"}[5m]) - 
rate(otelcol_processor_outgoing_spans{processor="resourcedetection"}[5m])
```

#### Error Detection
- **Alert condition**: ERROR logs matching pattern `"failed to detect resource"`
- Indicates: Misconfiguration, permission issues, or API failures
- Response: Check detector configuration and cloud provider permissions

- **Alert condition**: Startup time > 30 seconds
- Indicates: Slow detector or network issues
- Response: Increase timeouts or optimize detector selection

#### Performance
- **Metric**: Collector CPU usage during startup
- Expected: Brief spike during detection, then low baseline
- Issue if: Sustained high CPU usage

- **Metric**: Refresh cycle duration (if using `refresh_interval`)
- Expected: < timeout value per detector
- Issue if: Frequently exceeding timeout

### Telemetry Configuration

The processor respects the collector's telemetry level configuration:

**Basic** (default):
- Standard metrics: incoming/outgoing counters
- ERROR level logs only
- No tracing

**Normal**:
- All metrics
- INFO, WARN, ERROR logs
- Basic tracing

**Detailed**:
- All metrics with detailed attributes
- All log levels with verbose context
- Detailed tracing with all detector spans

Configure via collector's `service.telemetry.logs.level` and `service.telemetry.metrics.level` settings.

## Ordering

Note that if multiple detectors are inserting the same attribute name, the first detector to insert wins. For example if you had `detectors: [eks, ec2]` then `cloud.platform` will be `aws_eks` instead of `ec2`. The below ordering is recommended.

### AWS

* lambda
* elastic_beanstalk
* eks
* ecs
* ec2

The full list of settings exposed for this extension are documented in [config.go](./config.go)
with detailed sample configurations in [testdata/config.yaml](./testdata/config.yaml).

## Configuration Changes and Migration

This section documents configuration changes and provides migration paths for users upgrading from older versions.

### Removed in v0.116.0: `attributes` Configuration Option

**What changed**: The deprecated `attributes` configuration option was removed in favor of `resource_attributes`.

**Migration path**:  

If you were using the old `attributes` configuration:

```yaml
#  OLD (removed in v0.116.0)
processors:
  resourcedetection:
    detectors: [ec2]
    ec2:
      attributes:
        - host.name
        - host.id
```

Migrate to the new `resource_attributes` configuration:

```yaml
#  NEW (current)
processors:
  resourcedetection:
    detectors: [ec2]
    ec2:
      resource_attributes:
        host.name:
          enabled: true
        host.id:
          enabled: true
```

**Benefits of the new format**:
- Explicit enable/disable control per attribute
- Aligns with OpenTelemetry declarative configuration style
- Consistent with other collector components
- Better support for future attribute configuration options

**Timeline**:
- Deprecated: v0.110.0 (November 2024)
- Removal: v0.116.0 (December 2024)

For more details, see:
- [Issue #44610](https://github.com/open-telemetry/opentelemetry-collector-contrib/issues/44610)
- [PR #44616](https://github.com/open-telemetry/opentelemetry-collector-contrib/pull/44616)

### Configuration Stability Guarantee

As a beta component progressing toward stable:

- **Backward compatibility**: Configuration schema changes follow collector stability guidelines
- **Deprecation policy**: Deprecated options are kept for at least one minor version with warning logs
- **Migration time**: Users have at least 6 months or N+2 versions (whichever is later) to migrate
- **Documentation**: All breaking changes are documented in this section with clear migration paths

