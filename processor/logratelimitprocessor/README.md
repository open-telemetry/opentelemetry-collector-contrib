# Log Rate-Limiter Processor

<!-- status autogenerated section -->
| Status        |           |
| ------------- |-----------|
| Stability     | [alpha]: logs   |
| Distributions | [contrib], [k8s] |
| Issues        | [![Open issues](https://img.shields.io/github/issues-search/open-telemetry/opentelemetry-collector-contrib?query=is%3Aissue%20is%3Aopen%20label%3Aprocessor%2Flogratelimit%20&label=open&color=orange&logo=opentelemetry)](https://github.com/open-telemetry/opentelemetry-collector-contrib/issues?q=is%3Aopen+is%3Aissue+label%3Aprocessor%2Flogratelimit) [![Closed issues](https://img.shields.io/github/issues-search/open-telemetry/opentelemetry-collector-contrib?query=is%3Aissue%20is%3Aclosed%20label%3Aprocessor%2Flogratelimit%20&label=closed&color=blue&logo=opentelemetry)](https://github.com/open-telemetry/opentelemetry-collector-contrib/issues?q=is%3Aclosed+is%3Aissue+label%3Aprocessor%2Flogratelimit) |
| [Code Owners](https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/CONTRIBUTING.md#becoming-a-code-owner)    | [@anuraj381](https://www.github.com/anuraj381) |

[alpha]: https://github.com/open-telemetry/opentelemetry-collector/blob/main/docs/component-stability.md#alpha
[contrib]: https://github.com/open-telemetry/opentelemetry-collector-releases/tree/main/distributions/otelcol-contrib
[k8s]: https://github.com/open-telemetry/opentelemetry-collector-releases/tree/main/distributions/otelcol-k8s
<!-- end autogenerated section -->

## Details
The logratelimit processor is useful in the situation where you are collecting logs from multiple services on every otel-collector pod and there 
are one or many services which are logging too many logs such that it increases noise in the entire pipeline and storage (otel backend). So to control 
the log noise/surge from any service/namespace/pod etc. you can use the rate-limit processor, you can give the fields on which you want to limit logs 
and add config for allowed rate and interval.<br>
The processor caches the count of logs in the given interval for each combination of given rate_limit_fields and once logs count starts to exceed the count 
the processor will start dropping the logs till the interval finish in the best effort way. There are no mutex/locks involved, sync.Map and atomic counter is 
used to keep rate-limiter lightweight / easy on resources.

## Configuration
| Field             | Type     | Default | Description                                                                                                                                                                            |
|-------------------|----------|-------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| conditions        | []string | `[]`  | A slice of [OTTL] expressions used to evaluate which logs records to be rate-limited. See [OTTL Boolean Expressions] for more details. |
| allowed_rate      | int      | 30000 | Allowed rate of logs per logs combination of rate_limit_fields in configured `interval`                                                                                                |
| interval          | duration | `60s` | The interval in which rate-limit is applied after `allowed_rate`.                                                                                                                      |
| rate_limit_fields | []string | `[]`  | rate-limit is applied for each cobination of values of these fields                                                                                                                    |

[OTTL]: https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/v0.109.0/pkg/ottl#readme
[OTTL Boolean Expressions]: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/ottl/LANGUAGE.md#boolean-expressions

## Important Note
Before using this processor you should have a rough idea on how many logs lines are coming from any combination of your configured rate_limit_fields values 
on the opentelemetry-collector pods as you need to add a config for allowed_rate, this might be difficult to calculate in case of a daemonset deployment of 
the collector as how many pods of any service will be there on a Kubernetes node is not fixed (generally) so you might get high logs a combination of rate_limit_fields 
on one collector pod and maybe low number on other, this will disturb your calculation of allowed_rate, you might false drop logs. Ideally if you have a daemonset deployment 
strategy for collector then next to that in pipeline maybe add one more deployment (not daemonset, but k8s deployment) of collector and there you can have fair idea of 
allowed_rate and configure accordingly.

### Example Config
The following config is an example configuration for the logratelimit processor. It is configured with an allowed_rate of 30000 in an interval of `60 seconds` for each combination of mentioned rate_limit_fields array.
```yaml
receivers:
    filelog:
        include: [./example/*.log]

processors:
    logratelimit:
        conditions:
          - 'attributes["log_level"] == "error"'
          - 'resource.attributes["k8s.namespace.name"] == "my-k8s-ns-name"'
        allowed_rate: 30000
        interval: 60s
        rate_limit_fields: 
          - attributes.service\.name
          - resource.k8s\.container\.name

exporters:
    kafka:

service:
    pipelines:
        logs:
            receivers: [filelog]
            processors: [logratelimit]
            exporters: [kafka]
```
