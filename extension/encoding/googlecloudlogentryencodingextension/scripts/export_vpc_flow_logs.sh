#!/usr/bin/env bash
# Copyright The OpenTelemetry Authors
# SPDX-License-Identifier: Apache-2.0


set -euox pipefail

SCRIPT_DIR=$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)
ENV_FILE=${ENV_FILE:-"${SCRIPT_DIR}/vpc_flow_fixtures.env"}
if [[ -f "${ENV_FILE}" ]]; then
  echo "Loading environment from ${ENV_FILE}"
  set -a
  # shellcheck disable=SC1090
  source "${ENV_FILE}"
  set +a
fi

# Exports VPC flow logs generated by generate_vpc_flow_fixtures.sh.
# The script fetches raw log entries and organizes them into JSONL files
# for later transformation into fixtures.
#
# Usage:
#   cp scripts/vpc_flow_fixtures.env.example scripts/vpc_flow_fixtures.env
#   scripts/export_vpc_flow_logs.sh
#
# Optional overrides via env vars:
#   OUTPUT_DIR, START_TIME, END_TIME, MAX_RESULTS, ENV_FILE
#
# Optional environment variables:
#   START_TIME   RFC3339 start time (default: 20 minutes ago)
#   END_TIME     RFC3339 end time (default: now)
#   MAX_RESULTS  Number of log entries to fetch (default: 2000)

# Sanity-check required tooling before proceeding.
command -v gcloud >/dev/null || {
  echo "gcloud CLI is required" >&2
  exit 1
}

command -v jq >/dev/null || {
  echo "jq is required" >&2
  exit 1
}

# Core inputs: project hosting the fixtures and the prefix used during generation.
PROJECT_ID=${PROJECT_ID:?PROJECT_ID must be set}
RESOURCE_PREFIX=${RESOURCE_PREFIX:-gcp-fixture}

NETWORK_NAME="${RESOURCE_PREFIX}-network"
SUBNET_NAME="${RESOURCE_PREFIX}-subnet"
OUTPUT_DIR=${OUTPUT_DIR:-./vpc-fixtures-out}

# By default we scrape logs from the past ~20 minutes to cover the traffic window.
if [[ -z "${START_TIME:-}" ]]; then
  if date -u -d '20 minutes ago' >/dev/null 2>&1; then
    START_TIME=$(date -u -d '20 minutes ago' +%Y-%m-%dT%H:%M:%SZ)
  else
    START_TIME=$(TZ=UTC date -u -v -20M +%Y-%m-%dT%H:%M:%SZ)
  fi
fi

if [[ -z "${END_TIME:-}" ]]; then
  if date -u >/dev/null 2>&1; then
    END_TIME=$(date -u +%Y-%m-%dT%H:%M:%SZ)
  else
    END_TIME=$(TZ=UTC date +%Y-%m-%dT%H:%M:%SZ)
  fi
fi
MAX_RESULTS=${MAX_RESULTS:-2000}

mkdir -p "${OUTPUT_DIR}"

# Construct Logging filter clauses to only capture the relevant VPC flow dataset.
BASE_FILTER="logName=\"projects/${PROJECT_ID}/logs/networkmanagement.googleapis.com%2Fvpc_flows\""
SUBNET_FILTER="resource.labels.subnetwork_name=\"${SUBNET_NAME}\""
TIME_FILTER="timestamp >= \"${START_TIME}\" AND timestamp <= \"${END_TIME}\""

echo "Exporting aggregated VPC flow logs to ${OUTPUT_DIR}"
echo "Using network ${NETWORK_NAME} and subnet ${SUBNET_NAME}"
AGG_OUTPUT="${OUTPUT_DIR}/vpc_logs.jsonl"
# Pull the raw log entries in JSON format so they can be post-processed with jq.
gcloud logging read "${SUBNET_FILTER} AND ${TIME_FILTER}" \
  --format=json \
  --project "${PROJECT_ID}" \
  --limit="${MAX_RESULTS}" >"${AGG_OUTPUT}"

echo "Results sent to: ${AGG_OUTPUT}"

cat <<'NEXT'

Review the candidate files to locate log entries with the desired fields.
Suggested commands:
  jq '.["jsonPayload"]["round_trip_time"]' phase1_candidates.jsonl
  jq '.["jsonPayload"]["src_google_service"]' phase2_candidates.jsonl

Promote verified entries into extension testdata (JSON input + golden YAML).

NEXT

