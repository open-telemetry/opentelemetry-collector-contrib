// Code generated by mdatagen. DO NOT EDIT.

package metadata

import (
	"path/filepath"
	"testing"

	"github.com/stretchr/testify/assert"
	"github.com/stretchr/testify/require"
	"go.opentelemetry.io/collector/component"
	"go.opentelemetry.io/collector/confmap/confmaptest"
	"go.opentelemetry.io/collector/pdata/pcommon"
	"go.opentelemetry.io/collector/pdata/pmetric"
	"go.opentelemetry.io/collector/receiver/receivertest"
	"go.uber.org/zap"
	"go.uber.org/zap/zaptest/observer"
)

type testConfigCollection int

const (
	testSetDefault testConfigCollection = iota
	testSetAll
	testSetNone
)

func TestMetricsBuilder(t *testing.T) {
	tests := []struct {
		name      string
		configSet testConfigCollection
	}{
		{
			name:      "default",
			configSet: testSetDefault,
		},
		{
			name:      "all_set",
			configSet: testSetAll,
		},
		{
			name:      "none_set",
			configSet: testSetNone,
		},
	}
	for _, test := range tests {
		t.Run(test.name, func(t *testing.T) {
			start := pcommon.Timestamp(1_000_000_000)
			ts := pcommon.Timestamp(1_000_001_000)
			observedZapCore, observedLogs := observer.New(zap.WarnLevel)
			settings := receivertest.NewNopCreateSettings()
			settings.Logger = zap.New(observedZapCore)
			mb := NewMetricsBuilder(loadConfig(t, test.name), settings, WithStartTime(start))

			expectedWarnings := 0
			assert.Equal(t, expectedWarnings, observedLogs.Len())

			defaultMetricsCount := 0
			allMetricsCount := 0

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksJobsActiveTotalDataPoint(ts, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksJobsRunDurationDataPoint(ts, 1, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksJobsScheduleStatusDataPoint(ts, 1, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksJobsTotalDataPoint(ts, 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkBlockManagerMemoryDiskSpaceUsedDataPoint(ts, 1, "attr-val", "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkBlockManagerMemoryMaxDataPoint(ts, 1, "attr-val", "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkBlockManagerMemoryOffHeapMaxDataPoint(ts, 1, "attr-val", "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkBlockManagerMemoryOffHeapUsedDataPoint(ts, 1, "attr-val", "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkBlockManagerMemoryOnHeapMaxDataPoint(ts, 1, "attr-val", "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkBlockManagerMemoryOnHeapUsedDataPoint(ts, 1, "attr-val", "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkBlockManagerMemoryRemainingDataPoint(ts, 1, "attr-val", "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkBlockManagerMemoryRemainingOffHeapDataPoint(ts, 1, "attr-val", "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkBlockManagerMemoryRemainingOnHeapDataPoint(ts, 1, "attr-val", "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkBlockManagerMemoryUsedDataPoint(ts, 1, "attr-val", "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkCodeGeneratorCompilationTimeDataPoint(ts, 1, "attr-val", "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkCodeGeneratorGeneratedClassSizeDataPoint(ts, 1, "attr-val", "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkCodeGeneratorGeneratedMethodSizeDataPoint(ts, 1, "attr-val", "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkCodeGeneratorSourcecodeSizeDataPoint(ts, 1, "attr-val", "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkDagSchedulerJobsActiveDataPoint(ts, 1, "attr-val", "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkDagSchedulerJobsAllDataPoint(ts, 1, "attr-val", "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkDagSchedulerStagesFailedDataPoint(ts, 1, "attr-val", "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkDagSchedulerStagesRunningDataPoint(ts, 1, "attr-val", "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkDagSchedulerStagesWaitingDataPoint(ts, 1, "attr-val", "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkDatabricksDirectoryCommitAutoVacuumCountDataPoint(ts, 1, "attr-val", "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkDatabricksDirectoryCommitDeletedFilesFilteredDataPoint(ts, 1, "attr-val", "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkDatabricksDirectoryCommitFilterListingCountDataPoint(ts, 1, "attr-val", "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkDatabricksDirectoryCommitJobCommitCompletedDataPoint(ts, 1, "attr-val", "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkDatabricksDirectoryCommitMarkerReadErrorsDataPoint(ts, 1, "attr-val", "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkDatabricksDirectoryCommitMarkerRefreshCountDataPoint(ts, 1, "attr-val", "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkDatabricksDirectoryCommitMarkerRefreshErrorsDataPoint(ts, 1, "attr-val", "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkDatabricksDirectoryCommitMarkersReadDataPoint(ts, 1, "attr-val", "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkDatabricksDirectoryCommitRepeatedListCountDataPoint(ts, 1, "attr-val", "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkDatabricksDirectoryCommitUncommittedFilesFilteredDataPoint(ts, 1, "attr-val", "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkDatabricksDirectoryCommitUntrackedFilesFoundDataPoint(ts, 1, "attr-val", "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkDatabricksDirectoryCommitVacuumCountDataPoint(ts, 1, "attr-val", "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkDatabricksDirectoryCommitVacuumErrorsDataPoint(ts, 1, "attr-val", "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkDatabricksPreemptionChecksCountDataPoint(ts, 1, "attr-val", "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkDatabricksPreemptionPoolsAutoexpiredCountDataPoint(ts, 1, "attr-val", "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkDatabricksPreemptionPoolstarvationTimeDataPoint(ts, 1, "attr-val", "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkDatabricksPreemptionSchedulerOverheadTimeDataPoint(ts, 1, "attr-val", "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkDatabricksPreemptionTaskWastedTimeDataPoint(ts, 1, "attr-val", "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkDatabricksPreemptionTasksPreemptedCountDataPoint(ts, 1, "attr-val", "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkDatabricksTaskSchedulingLanesActivePoolsDataPoint(ts, 1, "attr-val", "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkDatabricksTaskSchedulingLanesBypassLaneActivePoolsDataPoint(ts, 1, "attr-val", "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkDatabricksTaskSchedulingLanesFastLaneActivePoolsDataPoint(ts, 1, "attr-val", "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkDatabricksTaskSchedulingLanesFinishedQueriesTotalTaskTimeDataPoint(ts, 1, "attr-val", "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkDatabricksTaskSchedulingLanesLaneCleanupMarkedPoolsDataPoint(ts, 1, "attr-val", "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkDatabricksTaskSchedulingLanesLaneCleanupTwoPhasePoolsCleanedDataPoint(ts, 1, "attr-val", "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkDatabricksTaskSchedulingLanesLaneCleanupZombiePoolsCleanedDataPoint(ts, 1, "attr-val", "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkDatabricksTaskSchedulingLanesPreemptionSlotTransferSuccessfulPreemptionIterationsCountDataPoint(ts, 1, "attr-val", "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkDatabricksTaskSchedulingLanesPreemptionSlotTransferTasksPreemptedCountDataPoint(ts, 1, "attr-val", "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkDatabricksTaskSchedulingLanesPreemptionSlotTransferWastedTaskTimeDataPoint(ts, 1, "attr-val", "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkDatabricksTaskSchedulingLanesSlotReservationGradualDecreaseCountDataPoint(ts, 1, "attr-val", "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkDatabricksTaskSchedulingLanesSlotReservationQuickDropCountDataPoint(ts, 1, "attr-val", "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkDatabricksTaskSchedulingLanesSlotReservationQuickJumpCountDataPoint(ts, 1, "attr-val", "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkDatabricksTaskSchedulingLanesSlotReservationSlotsReservedDataPoint(ts, 1, "attr-val", "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkDatabricksTaskSchedulingLanesSlowLaneActivePoolsDataPoint(ts, 1, "attr-val", "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkDatabricksTaskSchedulingLanesTotalquerygroupsfinishedDataPoint(ts, 1, "attr-val", "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkExecutorDiskUsedDataPoint(ts, 1, "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkExecutorMaxMemoryDataPoint(ts, 1, "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkExecutorMemoryUsedDataPoint(ts, 1, "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkExecutorTotalInputBytesDataPoint(ts, 1, "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkExecutorTotalShuffleReadDataPoint(ts, 1, "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkExecutorTotalShuffleWriteDataPoint(ts, 1, "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkExecutorMetricsDirectPoolMemoryDataPoint(ts, 1, "attr-val", "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkExecutorMetricsJvmHeapMemoryDataPoint(ts, 1, "attr-val", "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkExecutorMetricsJvmOffHeapMemoryDataPoint(ts, 1, "attr-val", "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkExecutorMetricsMajorGcCountDataPoint(ts, 1, "attr-val", "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkExecutorMetricsMajorGcTimeDataPoint(ts, 1, "attr-val", "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkExecutorMetricsMappedPoolMemoryDataPoint(ts, 1, "attr-val", "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkExecutorMetricsMinorGcCountDataPoint(ts, 1, "attr-val", "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkExecutorMetricsMinorGcTimeDataPoint(ts, 1, "attr-val", "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkExecutorMetricsOffHeapExecutionMemoryDataPoint(ts, 1, "attr-val", "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkExecutorMetricsOffHeapStorageMemoryDataPoint(ts, 1, "attr-val", "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkExecutorMetricsOffHeapUnifiedMemoryDataPoint(ts, 1, "attr-val", "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkExecutorMetricsOnHeapExecutionMemoryDataPoint(ts, 1, "attr-val", "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkExecutorMetricsOnHeapStorageMemoryDataPoint(ts, 1, "attr-val", "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkExecutorMetricsOnHeapUnifiedMemoryDataPoint(ts, 1, "attr-val", "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkExecutorMetricsProcessTreeJvmRssMemoryDataPoint(ts, 1, "attr-val", "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkExecutorMetricsProcessTreeJvmVMemoryDataPoint(ts, 1, "attr-val", "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkExecutorMetricsProcessTreeOtherRssMemoryDataPoint(ts, 1, "attr-val", "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkExecutorMetricsProcessTreeOtherVMemoryDataPoint(ts, 1, "attr-val", "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkExecutorMetricsProcessTreePythonRssMemoryDataPoint(ts, 1, "attr-val", "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkExecutorMetricsProcessTreePythonVMemoryDataPoint(ts, 1, "attr-val", "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkHiveExternalCatalogFileCacheHitsDataPoint(ts, 1, "attr-val", "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkHiveExternalCatalogFilesDiscoveredDataPoint(ts, 1, "attr-val", "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkHiveExternalCatalogHiveClientCallsDataPoint(ts, 1, "attr-val", "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkHiveExternalCatalogParallelListingJobsCountDataPoint(ts, 1, "attr-val", "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkHiveExternalCatalogPartitionsFetchedDataPoint(ts, 1, "attr-val", "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkJobNumActiveStagesDataPoint(ts, 1, "attr-val", 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkJobNumActiveTasksDataPoint(ts, 1, "attr-val", 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkJobNumCompletedStagesDataPoint(ts, 1, "attr-val", 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkJobNumCompletedTasksDataPoint(ts, 1, "attr-val", 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkJobNumFailedStagesDataPoint(ts, 1, "attr-val", 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkJobNumFailedTasksDataPoint(ts, 1, "attr-val", 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkJobNumSkippedStagesDataPoint(ts, 1, "attr-val", 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkJobNumSkippedTasksDataPoint(ts, 1, "attr-val", 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkJobNumTasksDataPoint(ts, 1, "attr-val", 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkJvmCPUTimeDataPoint(ts, 1, "attr-val", "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkLiveListenerBusEventsPostedCountDataPoint(ts, 1, "attr-val", "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkLiveListenerBusQueueAppStatusDroppedEventsCountDataPoint(ts, 1, "attr-val", "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkLiveListenerBusQueueAppstatusSizeDataPoint(ts, 1, "attr-val", "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkLiveListenerBusQueueExecutorManagementDroppedEventsCountDataPoint(ts, 1, "attr-val", "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkLiveListenerBusQueueExecutormanagementSizeDataPoint(ts, 1, "attr-val", "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkLiveListenerBusQueueSharedDroppedEventsCountDataPoint(ts, 1, "attr-val", "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkLiveListenerBusQueueSharedSizeDataPoint(ts, 1, "attr-val", "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkLiveListenerBusQueueStreamsDroppedEventsCountDataPoint(ts, 1, "attr-val", "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkLiveListenerBusQueueStreamsSizeDataPoint(ts, 1, "attr-val", "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkSparkSQLOperationManagerHiveOperationsCountDataPoint(ts, 1, "attr-val", "attr-val", "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkStageDiskBytesSpilledDataPoint(ts, 1, "attr-val", 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkStageExecutorRunTimeDataPoint(ts, 1, "attr-val", 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkStageInputBytesDataPoint(ts, 1, "attr-val", 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkStageInputRecordsDataPoint(ts, 1, "attr-val", 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkStageMemoryBytesSpilledDataPoint(ts, 1, "attr-val", 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkStageOutputBytesDataPoint(ts, 1, "attr-val", 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkStageOutputRecordsDataPoint(ts, 1, "attr-val", 1)

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkTimerDagSchedulerMessageProcessingTimeDataPoint(ts, 1, "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLExecutionStreamingQueryListenerBusTimeDataPoint(ts, 1, "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLExecutionTimeDataPoint(ts, 1, "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLExecutionUISQLAppStatusListenerTimeDataPoint(ts, 1, "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLHiveThriftserverUIHiveThriftServer2listenerTimeDataPoint(ts, 1, "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLSparkSessionTimeDataPoint(ts, 1, "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkSQLUtilExecutionListenerBusTimeDataPoint(ts, 1, "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkStatusAppStatusListenerTimeDataPoint(ts, 1, "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkTimerLiveListenerBusListenerProcessingApacheSparkUtilProfilerEnvTimeDataPoint(ts, 1, "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksBackendDaemonDriverDataPlaneEventListenerTimeDataPoint(ts, 1, "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksBackendDaemonDriverDbcEventLoggingListenerTimeDataPoint(ts, 1, "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksPhotonPhotonCleanupListenerTimeDataPoint(ts, 1, "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSparkUtilExecutorTimeLoggingListenerTimeDataPoint(ts, 1, "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSparkUtilUsageLoggingListenerTimeDataPoint(ts, 1, "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSQLAdviceAdvisorListenerTimeDataPoint(ts, 1, "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSQLDebuggerQueryWatchdogListenerTimeDataPoint(ts, 1, "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSQLExecutionUIIoCacheListenerTimeDataPoint(ts, 1, "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkTimerLiveListenerBusListenerProcessingDatabricksSQLIoCachingRepeatedReadsEstimatorTimeDataPoint(ts, 1, "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkTimerLiveListenerBusQueueAppStatusListenerProcessingTimeDataPoint(ts, 1, "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkTimerLiveListenerBusQueueExecutorManagementListenerProcessingTimeDataPoint(ts, 1, "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkTimerLiveListenerBusQueueSharedListenerProcessingTimeDataPoint(ts, 1, "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksSparkTimerLiveListenerBusQueueStreamsListenerProcessingTimeDataPoint(ts, 1, "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksTasksRunDurationDataPoint(ts, 1, 1, "attr-val")

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordDatabricksTasksScheduleStatusDataPoint(ts, 1, 1, "attr-val", AttributeTaskType(1))

			metrics := mb.Emit(WithDatabricksClusterID("attr-val"), WithDatabricksInstanceName("attr-val"), WithSparkAppID("attr-val"), WithSparkClusterName("attr-val"))

			if test.configSet == testSetNone {
				assert.Equal(t, 0, metrics.ResourceMetrics().Len())
				return
			}

			assert.Equal(t, 1, metrics.ResourceMetrics().Len())
			rm := metrics.ResourceMetrics().At(0)
			attrCount := 0
			enabledAttrCount := 0
			attrVal, ok := rm.Resource().Attributes().Get("databricks.cluster.id")
			attrCount++
			assert.Equal(t, mb.resourceAttributesSettings.DatabricksClusterID.Enabled, ok)
			if mb.resourceAttributesSettings.DatabricksClusterID.Enabled {
				enabledAttrCount++
				assert.EqualValues(t, "attr-val", attrVal.Str())
			}
			attrVal, ok = rm.Resource().Attributes().Get("databricks.instance.name")
			attrCount++
			assert.Equal(t, mb.resourceAttributesSettings.DatabricksInstanceName.Enabled, ok)
			if mb.resourceAttributesSettings.DatabricksInstanceName.Enabled {
				enabledAttrCount++
				assert.EqualValues(t, "attr-val", attrVal.Str())
			}
			attrVal, ok = rm.Resource().Attributes().Get("spark.app.id")
			attrCount++
			assert.Equal(t, mb.resourceAttributesSettings.SparkAppID.Enabled, ok)
			if mb.resourceAttributesSettings.SparkAppID.Enabled {
				enabledAttrCount++
				assert.EqualValues(t, "attr-val", attrVal.Str())
			}
			attrVal, ok = rm.Resource().Attributes().Get("spark.cluster.name")
			attrCount++
			assert.Equal(t, mb.resourceAttributesSettings.SparkClusterName.Enabled, ok)
			if mb.resourceAttributesSettings.SparkClusterName.Enabled {
				enabledAttrCount++
				assert.EqualValues(t, "attr-val", attrVal.Str())
			}
			assert.Equal(t, enabledAttrCount, rm.Resource().Attributes().Len())
			assert.Equal(t, attrCount, 4)

			assert.Equal(t, 1, rm.ScopeMetrics().Len())
			ms := rm.ScopeMetrics().At(0).Metrics()
			if test.configSet == testSetDefault {
				assert.Equal(t, defaultMetricsCount, ms.Len())
			}
			if test.configSet == testSetAll {
				assert.Equal(t, allMetricsCount, ms.Len())
			}
			validatedMetrics := make(map[string]bool)
			for i := 0; i < ms.Len(); i++ {
				switch ms.At(i).Name() {
				case "databricks.jobs.active.total":
					assert.False(t, validatedMetrics["databricks.jobs.active.total"], "Found a duplicate in the metrics slice: databricks.jobs.active.total")
					validatedMetrics["databricks.jobs.active.total"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "A snapshot of the number of active jobs taken at each scrape", ms.At(i).Description())
					assert.Equal(t, "{jobs}", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
				case "databricks.jobs.run.duration":
					assert.False(t, validatedMetrics["databricks.jobs.run.duration"], "Found a duplicate in the metrics slice: databricks.jobs.run.duration")
					validatedMetrics["databricks.jobs.run.duration"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "The execution duration in milliseconds per completed job", ms.At(i).Description())
					assert.Equal(t, "ms", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("job.id")
					assert.True(t, ok)
					assert.EqualValues(t, 1, attrVal.Int())
				case "databricks.jobs.schedule.status":
					assert.False(t, validatedMetrics["databricks.jobs.schedule.status"], "Found a duplicate in the metrics slice: databricks.jobs.schedule.status")
					validatedMetrics["databricks.jobs.schedule.status"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "A snapshot of the pause/run status per job taken at each scrape", ms.At(i).Description())
					assert.Equal(t, "{status}", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("job.id")
					assert.True(t, ok)
					assert.EqualValues(t, 1, attrVal.Int())
				case "databricks.jobs.total":
					assert.False(t, validatedMetrics["databricks.jobs.total"], "Found a duplicate in the metrics slice: databricks.jobs.total")
					validatedMetrics["databricks.jobs.total"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "A snapshot of the total number of jobs registered in the Databricks instance taken at each scrape", ms.At(i).Description())
					assert.Equal(t, "{jobs}", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
				case "databricks.spark.block_manager.memory.disk_space.used":
					assert.False(t, validatedMetrics["databricks.spark.block_manager.memory.disk_space.used"], "Found a duplicate in the metrics slice: databricks.spark.block_manager.memory.disk_space.used")
					validatedMetrics["databricks.spark.block_manager.memory.disk_space.used"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "mb", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeDouble, dp.ValueType())
					assert.Equal(t, float64(1), dp.DoubleValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.name")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.block_manager.memory.max":
					assert.False(t, validatedMetrics["databricks.spark.block_manager.memory.max"], "Found a duplicate in the metrics slice: databricks.spark.block_manager.memory.max")
					validatedMetrics["databricks.spark.block_manager.memory.max"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "mb", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeDouble, dp.ValueType())
					assert.Equal(t, float64(1), dp.DoubleValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.name")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.block_manager.memory.off_heap.max":
					assert.False(t, validatedMetrics["databricks.spark.block_manager.memory.off_heap.max"], "Found a duplicate in the metrics slice: databricks.spark.block_manager.memory.off_heap.max")
					validatedMetrics["databricks.spark.block_manager.memory.off_heap.max"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "mb", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeDouble, dp.ValueType())
					assert.Equal(t, float64(1), dp.DoubleValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.name")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.block_manager.memory.off_heap.used":
					assert.False(t, validatedMetrics["databricks.spark.block_manager.memory.off_heap.used"], "Found a duplicate in the metrics slice: databricks.spark.block_manager.memory.off_heap.used")
					validatedMetrics["databricks.spark.block_manager.memory.off_heap.used"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "mb", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeDouble, dp.ValueType())
					assert.Equal(t, float64(1), dp.DoubleValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.name")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.block_manager.memory.on_heap.max":
					assert.False(t, validatedMetrics["databricks.spark.block_manager.memory.on_heap.max"], "Found a duplicate in the metrics slice: databricks.spark.block_manager.memory.on_heap.max")
					validatedMetrics["databricks.spark.block_manager.memory.on_heap.max"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "mb", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeDouble, dp.ValueType())
					assert.Equal(t, float64(1), dp.DoubleValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.name")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.block_manager.memory.on_heap.used":
					assert.False(t, validatedMetrics["databricks.spark.block_manager.memory.on_heap.used"], "Found a duplicate in the metrics slice: databricks.spark.block_manager.memory.on_heap.used")
					validatedMetrics["databricks.spark.block_manager.memory.on_heap.used"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "mb", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeDouble, dp.ValueType())
					assert.Equal(t, float64(1), dp.DoubleValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.name")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.block_manager.memory.remaining":
					assert.False(t, validatedMetrics["databricks.spark.block_manager.memory.remaining"], "Found a duplicate in the metrics slice: databricks.spark.block_manager.memory.remaining")
					validatedMetrics["databricks.spark.block_manager.memory.remaining"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "mb", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeDouble, dp.ValueType())
					assert.Equal(t, float64(1), dp.DoubleValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.name")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.block_manager.memory.remaining.off_heap":
					assert.False(t, validatedMetrics["databricks.spark.block_manager.memory.remaining.off_heap"], "Found a duplicate in the metrics slice: databricks.spark.block_manager.memory.remaining.off_heap")
					validatedMetrics["databricks.spark.block_manager.memory.remaining.off_heap"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "mb", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeDouble, dp.ValueType())
					assert.Equal(t, float64(1), dp.DoubleValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.name")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.block_manager.memory.remaining.on_heap":
					assert.False(t, validatedMetrics["databricks.spark.block_manager.memory.remaining.on_heap"], "Found a duplicate in the metrics slice: databricks.spark.block_manager.memory.remaining.on_heap")
					validatedMetrics["databricks.spark.block_manager.memory.remaining.on_heap"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "mb", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeDouble, dp.ValueType())
					assert.Equal(t, float64(1), dp.DoubleValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.name")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.block_manager.memory.used":
					assert.False(t, validatedMetrics["databricks.spark.block_manager.memory.used"], "Found a duplicate in the metrics slice: databricks.spark.block_manager.memory.used")
					validatedMetrics["databricks.spark.block_manager.memory.used"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "mb", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeDouble, dp.ValueType())
					assert.Equal(t, float64(1), dp.DoubleValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.name")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.code_generator.compilation.time":
					assert.False(t, validatedMetrics["databricks.spark.code_generator.compilation.time"], "Found a duplicate in the metrics slice: databricks.spark.code_generator.compilation.time")
					validatedMetrics["databricks.spark.code_generator.compilation.time"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "This value comes from the 'mean' field in a histogram returned by the /metrics/json/ endpoint.", ms.At(i).Description())
					assert.Equal(t, "", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeDouble, dp.ValueType())
					assert.Equal(t, float64(1), dp.DoubleValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.name")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.code_generator.generated_class_size":
					assert.False(t, validatedMetrics["databricks.spark.code_generator.generated_class_size"], "Found a duplicate in the metrics slice: databricks.spark.code_generator.generated_class_size")
					validatedMetrics["databricks.spark.code_generator.generated_class_size"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "This value comes from the 'mean' field in a histogram returned by the /metrics/json/ endpoint.", ms.At(i).Description())
					assert.Equal(t, "", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeDouble, dp.ValueType())
					assert.Equal(t, float64(1), dp.DoubleValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.name")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.code_generator.generated_method_size":
					assert.False(t, validatedMetrics["databricks.spark.code_generator.generated_method_size"], "Found a duplicate in the metrics slice: databricks.spark.code_generator.generated_method_size")
					validatedMetrics["databricks.spark.code_generator.generated_method_size"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "This value comes from the 'mean' field in a histogram returned by the /metrics/json/ endpoint.", ms.At(i).Description())
					assert.Equal(t, "", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeDouble, dp.ValueType())
					assert.Equal(t, float64(1), dp.DoubleValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.name")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.code_generator.sourcecode_size":
					assert.False(t, validatedMetrics["databricks.spark.code_generator.sourcecode_size"], "Found a duplicate in the metrics slice: databricks.spark.code_generator.sourcecode_size")
					validatedMetrics["databricks.spark.code_generator.sourcecode_size"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "This value comes from the 'mean' field in a histogram returned by the /metrics/json/ endpoint.", ms.At(i).Description())
					assert.Equal(t, "", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeDouble, dp.ValueType())
					assert.Equal(t, float64(1), dp.DoubleValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.name")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.dag_scheduler.jobs.active":
					assert.False(t, validatedMetrics["databricks.spark.dag_scheduler.jobs.active"], "Found a duplicate in the metrics slice: databricks.spark.dag_scheduler.jobs.active")
					validatedMetrics["databricks.spark.dag_scheduler.jobs.active"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeDouble, dp.ValueType())
					assert.Equal(t, float64(1), dp.DoubleValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.name")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.dag_scheduler.jobs.all":
					assert.False(t, validatedMetrics["databricks.spark.dag_scheduler.jobs.all"], "Found a duplicate in the metrics slice: databricks.spark.dag_scheduler.jobs.all")
					validatedMetrics["databricks.spark.dag_scheduler.jobs.all"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeDouble, dp.ValueType())
					assert.Equal(t, float64(1), dp.DoubleValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.name")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.dag_scheduler.stages.failed":
					assert.False(t, validatedMetrics["databricks.spark.dag_scheduler.stages.failed"], "Found a duplicate in the metrics slice: databricks.spark.dag_scheduler.stages.failed")
					validatedMetrics["databricks.spark.dag_scheduler.stages.failed"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeDouble, dp.ValueType())
					assert.Equal(t, float64(1), dp.DoubleValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.name")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.dag_scheduler.stages.running":
					assert.False(t, validatedMetrics["databricks.spark.dag_scheduler.stages.running"], "Found a duplicate in the metrics slice: databricks.spark.dag_scheduler.stages.running")
					validatedMetrics["databricks.spark.dag_scheduler.stages.running"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeDouble, dp.ValueType())
					assert.Equal(t, float64(1), dp.DoubleValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.name")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.dag_scheduler.stages.waiting":
					assert.False(t, validatedMetrics["databricks.spark.dag_scheduler.stages.waiting"], "Found a duplicate in the metrics slice: databricks.spark.dag_scheduler.stages.waiting")
					validatedMetrics["databricks.spark.dag_scheduler.stages.waiting"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeDouble, dp.ValueType())
					assert.Equal(t, float64(1), dp.DoubleValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.name")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.databricks.directory_commit.auto_vacuum.count":
					assert.False(t, validatedMetrics["databricks.spark.databricks.directory_commit.auto_vacuum.count"], "Found a duplicate in the metrics slice: databricks.spark.databricks.directory_commit.auto_vacuum.count")
					validatedMetrics["databricks.spark.databricks.directory_commit.auto_vacuum.count"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "", ms.At(i).Unit())
					assert.Equal(t, false, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.name")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.databricks.directory_commit.deleted_files_filtered":
					assert.False(t, validatedMetrics["databricks.spark.databricks.directory_commit.deleted_files_filtered"], "Found a duplicate in the metrics slice: databricks.spark.databricks.directory_commit.deleted_files_filtered")
					validatedMetrics["databricks.spark.databricks.directory_commit.deleted_files_filtered"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "", ms.At(i).Unit())
					assert.Equal(t, false, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.name")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.databricks.directory_commit.filter_listing.count":
					assert.False(t, validatedMetrics["databricks.spark.databricks.directory_commit.filter_listing.count"], "Found a duplicate in the metrics slice: databricks.spark.databricks.directory_commit.filter_listing.count")
					validatedMetrics["databricks.spark.databricks.directory_commit.filter_listing.count"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "", ms.At(i).Unit())
					assert.Equal(t, false, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.name")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.databricks.directory_commit.job_commit_completed":
					assert.False(t, validatedMetrics["databricks.spark.databricks.directory_commit.job_commit_completed"], "Found a duplicate in the metrics slice: databricks.spark.databricks.directory_commit.job_commit_completed")
					validatedMetrics["databricks.spark.databricks.directory_commit.job_commit_completed"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "", ms.At(i).Unit())
					assert.Equal(t, false, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.name")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.databricks.directory_commit.marker_read.errors":
					assert.False(t, validatedMetrics["databricks.spark.databricks.directory_commit.marker_read.errors"], "Found a duplicate in the metrics slice: databricks.spark.databricks.directory_commit.marker_read.errors")
					validatedMetrics["databricks.spark.databricks.directory_commit.marker_read.errors"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "", ms.At(i).Unit())
					assert.Equal(t, false, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.name")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.databricks.directory_commit.marker_refresh.count":
					assert.False(t, validatedMetrics["databricks.spark.databricks.directory_commit.marker_refresh.count"], "Found a duplicate in the metrics slice: databricks.spark.databricks.directory_commit.marker_refresh.count")
					validatedMetrics["databricks.spark.databricks.directory_commit.marker_refresh.count"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "", ms.At(i).Unit())
					assert.Equal(t, false, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.name")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.databricks.directory_commit.marker_refresh.errors":
					assert.False(t, validatedMetrics["databricks.spark.databricks.directory_commit.marker_refresh.errors"], "Found a duplicate in the metrics slice: databricks.spark.databricks.directory_commit.marker_refresh.errors")
					validatedMetrics["databricks.spark.databricks.directory_commit.marker_refresh.errors"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "", ms.At(i).Unit())
					assert.Equal(t, false, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.name")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.databricks.directory_commit.markers.read":
					assert.False(t, validatedMetrics["databricks.spark.databricks.directory_commit.markers.read"], "Found a duplicate in the metrics slice: databricks.spark.databricks.directory_commit.markers.read")
					validatedMetrics["databricks.spark.databricks.directory_commit.markers.read"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "", ms.At(i).Unit())
					assert.Equal(t, false, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.name")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.databricks.directory_commit.repeated_list.count":
					assert.False(t, validatedMetrics["databricks.spark.databricks.directory_commit.repeated_list.count"], "Found a duplicate in the metrics slice: databricks.spark.databricks.directory_commit.repeated_list.count")
					validatedMetrics["databricks.spark.databricks.directory_commit.repeated_list.count"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "", ms.At(i).Unit())
					assert.Equal(t, false, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.name")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.databricks.directory_commit.uncommitted_files.filtered":
					assert.False(t, validatedMetrics["databricks.spark.databricks.directory_commit.uncommitted_files.filtered"], "Found a duplicate in the metrics slice: databricks.spark.databricks.directory_commit.uncommitted_files.filtered")
					validatedMetrics["databricks.spark.databricks.directory_commit.uncommitted_files.filtered"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "", ms.At(i).Unit())
					assert.Equal(t, false, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.name")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.databricks.directory_commit.untracked_files.found":
					assert.False(t, validatedMetrics["databricks.spark.databricks.directory_commit.untracked_files.found"], "Found a duplicate in the metrics slice: databricks.spark.databricks.directory_commit.untracked_files.found")
					validatedMetrics["databricks.spark.databricks.directory_commit.untracked_files.found"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "", ms.At(i).Unit())
					assert.Equal(t, false, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.name")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.databricks.directory_commit.vacuum.count":
					assert.False(t, validatedMetrics["databricks.spark.databricks.directory_commit.vacuum.count"], "Found a duplicate in the metrics slice: databricks.spark.databricks.directory_commit.vacuum.count")
					validatedMetrics["databricks.spark.databricks.directory_commit.vacuum.count"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "", ms.At(i).Unit())
					assert.Equal(t, false, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.name")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.databricks.directory_commit.vacuum.errors":
					assert.False(t, validatedMetrics["databricks.spark.databricks.directory_commit.vacuum.errors"], "Found a duplicate in the metrics slice: databricks.spark.databricks.directory_commit.vacuum.errors")
					validatedMetrics["databricks.spark.databricks.directory_commit.vacuum.errors"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "", ms.At(i).Unit())
					assert.Equal(t, false, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.name")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.databricks.preemption.checks.count":
					assert.False(t, validatedMetrics["databricks.spark.databricks.preemption.checks.count"], "Found a duplicate in the metrics slice: databricks.spark.databricks.preemption.checks.count")
					validatedMetrics["databricks.spark.databricks.preemption.checks.count"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "", ms.At(i).Unit())
					assert.Equal(t, false, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.name")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.databricks.preemption.pools_autoexpired.count":
					assert.False(t, validatedMetrics["databricks.spark.databricks.preemption.pools_autoexpired.count"], "Found a duplicate in the metrics slice: databricks.spark.databricks.preemption.pools_autoexpired.count")
					validatedMetrics["databricks.spark.databricks.preemption.pools_autoexpired.count"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "", ms.At(i).Unit())
					assert.Equal(t, false, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.name")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.databricks.preemption.poolstarvation.time":
					assert.False(t, validatedMetrics["databricks.spark.databricks.preemption.poolstarvation.time"], "Found a duplicate in the metrics slice: databricks.spark.databricks.preemption.poolstarvation.time")
					validatedMetrics["databricks.spark.databricks.preemption.poolstarvation.time"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "ms", ms.At(i).Unit())
					assert.Equal(t, false, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.name")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.databricks.preemption.scheduler_overhead.time":
					assert.False(t, validatedMetrics["databricks.spark.databricks.preemption.scheduler_overhead.time"], "Found a duplicate in the metrics slice: databricks.spark.databricks.preemption.scheduler_overhead.time")
					validatedMetrics["databricks.spark.databricks.preemption.scheduler_overhead.time"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "ns", ms.At(i).Unit())
					assert.Equal(t, false, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.name")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.databricks.preemption.task_wasted.time":
					assert.False(t, validatedMetrics["databricks.spark.databricks.preemption.task_wasted.time"], "Found a duplicate in the metrics slice: databricks.spark.databricks.preemption.task_wasted.time")
					validatedMetrics["databricks.spark.databricks.preemption.task_wasted.time"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "ms", ms.At(i).Unit())
					assert.Equal(t, false, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.name")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.databricks.preemption.tasks_preempted.count":
					assert.False(t, validatedMetrics["databricks.spark.databricks.preemption.tasks_preempted.count"], "Found a duplicate in the metrics slice: databricks.spark.databricks.preemption.tasks_preempted.count")
					validatedMetrics["databricks.spark.databricks.preemption.tasks_preempted.count"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "", ms.At(i).Unit())
					assert.Equal(t, false, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.name")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.databricks.task_scheduling_lanes.active_pools":
					assert.False(t, validatedMetrics["databricks.spark.databricks.task_scheduling_lanes.active_pools"], "Found a duplicate in the metrics slice: databricks.spark.databricks.task_scheduling_lanes.active_pools")
					validatedMetrics["databricks.spark.databricks.task_scheduling_lanes.active_pools"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "", ms.At(i).Unit())
					assert.Equal(t, false, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.name")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.databricks.task_scheduling_lanes.bypass_lane_active_pools":
					assert.False(t, validatedMetrics["databricks.spark.databricks.task_scheduling_lanes.bypass_lane_active_pools"], "Found a duplicate in the metrics slice: databricks.spark.databricks.task_scheduling_lanes.bypass_lane_active_pools")
					validatedMetrics["databricks.spark.databricks.task_scheduling_lanes.bypass_lane_active_pools"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "", ms.At(i).Unit())
					assert.Equal(t, false, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.name")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.databricks.task_scheduling_lanes.fast_lane_active_pools":
					assert.False(t, validatedMetrics["databricks.spark.databricks.task_scheduling_lanes.fast_lane_active_pools"], "Found a duplicate in the metrics slice: databricks.spark.databricks.task_scheduling_lanes.fast_lane_active_pools")
					validatedMetrics["databricks.spark.databricks.task_scheduling_lanes.fast_lane_active_pools"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "", ms.At(i).Unit())
					assert.Equal(t, false, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.name")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.databricks.task_scheduling_lanes.finished_queries_total_task.time":
					assert.False(t, validatedMetrics["databricks.spark.databricks.task_scheduling_lanes.finished_queries_total_task.time"], "Found a duplicate in the metrics slice: databricks.spark.databricks.task_scheduling_lanes.finished_queries_total_task.time")
					validatedMetrics["databricks.spark.databricks.task_scheduling_lanes.finished_queries_total_task.time"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "ns", ms.At(i).Unit())
					assert.Equal(t, false, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.name")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.databricks.task_scheduling_lanes.lane_cleanup.marked_pools":
					assert.False(t, validatedMetrics["databricks.spark.databricks.task_scheduling_lanes.lane_cleanup.marked_pools"], "Found a duplicate in the metrics slice: databricks.spark.databricks.task_scheduling_lanes.lane_cleanup.marked_pools")
					validatedMetrics["databricks.spark.databricks.task_scheduling_lanes.lane_cleanup.marked_pools"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "", ms.At(i).Unit())
					assert.Equal(t, false, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.name")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.databricks.task_scheduling_lanes.lane_cleanup.two_phase_pools_cleaned":
					assert.False(t, validatedMetrics["databricks.spark.databricks.task_scheduling_lanes.lane_cleanup.two_phase_pools_cleaned"], "Found a duplicate in the metrics slice: databricks.spark.databricks.task_scheduling_lanes.lane_cleanup.two_phase_pools_cleaned")
					validatedMetrics["databricks.spark.databricks.task_scheduling_lanes.lane_cleanup.two_phase_pools_cleaned"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "", ms.At(i).Unit())
					assert.Equal(t, false, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.name")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.databricks.task_scheduling_lanes.lane_cleanup.zombie_pools_cleaned":
					assert.False(t, validatedMetrics["databricks.spark.databricks.task_scheduling_lanes.lane_cleanup.zombie_pools_cleaned"], "Found a duplicate in the metrics slice: databricks.spark.databricks.task_scheduling_lanes.lane_cleanup.zombie_pools_cleaned")
					validatedMetrics["databricks.spark.databricks.task_scheduling_lanes.lane_cleanup.zombie_pools_cleaned"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "", ms.At(i).Unit())
					assert.Equal(t, false, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.name")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.databricks.task_scheduling_lanes.preemption.slot_transfer_successful_preemption_iterations.count":
					assert.False(t, validatedMetrics["databricks.spark.databricks.task_scheduling_lanes.preemption.slot_transfer_successful_preemption_iterations.count"], "Found a duplicate in the metrics slice: databricks.spark.databricks.task_scheduling_lanes.preemption.slot_transfer_successful_preemption_iterations.count")
					validatedMetrics["databricks.spark.databricks.task_scheduling_lanes.preemption.slot_transfer_successful_preemption_iterations.count"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "", ms.At(i).Unit())
					assert.Equal(t, false, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.name")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.databricks.task_scheduling_lanes.preemption.slot_transfer_tasks_preempted.count":
					assert.False(t, validatedMetrics["databricks.spark.databricks.task_scheduling_lanes.preemption.slot_transfer_tasks_preempted.count"], "Found a duplicate in the metrics slice: databricks.spark.databricks.task_scheduling_lanes.preemption.slot_transfer_tasks_preempted.count")
					validatedMetrics["databricks.spark.databricks.task_scheduling_lanes.preemption.slot_transfer_tasks_preempted.count"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "", ms.At(i).Unit())
					assert.Equal(t, false, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.name")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.databricks.task_scheduling_lanes.preemption.slot_transfer_wasted_task.time":
					assert.False(t, validatedMetrics["databricks.spark.databricks.task_scheduling_lanes.preemption.slot_transfer_wasted_task.time"], "Found a duplicate in the metrics slice: databricks.spark.databricks.task_scheduling_lanes.preemption.slot_transfer_wasted_task.time")
					validatedMetrics["databricks.spark.databricks.task_scheduling_lanes.preemption.slot_transfer_wasted_task.time"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "ns", ms.At(i).Unit())
					assert.Equal(t, false, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.name")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.databricks.task_scheduling_lanes.slot_reservation.gradual_decrease.count":
					assert.False(t, validatedMetrics["databricks.spark.databricks.task_scheduling_lanes.slot_reservation.gradual_decrease.count"], "Found a duplicate in the metrics slice: databricks.spark.databricks.task_scheduling_lanes.slot_reservation.gradual_decrease.count")
					validatedMetrics["databricks.spark.databricks.task_scheduling_lanes.slot_reservation.gradual_decrease.count"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "", ms.At(i).Unit())
					assert.Equal(t, false, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.name")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.databricks.task_scheduling_lanes.slot_reservation.quick_drop.count":
					assert.False(t, validatedMetrics["databricks.spark.databricks.task_scheduling_lanes.slot_reservation.quick_drop.count"], "Found a duplicate in the metrics slice: databricks.spark.databricks.task_scheduling_lanes.slot_reservation.quick_drop.count")
					validatedMetrics["databricks.spark.databricks.task_scheduling_lanes.slot_reservation.quick_drop.count"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "", ms.At(i).Unit())
					assert.Equal(t, false, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.name")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.databricks.task_scheduling_lanes.slot_reservation.quick_jump.count":
					assert.False(t, validatedMetrics["databricks.spark.databricks.task_scheduling_lanes.slot_reservation.quick_jump.count"], "Found a duplicate in the metrics slice: databricks.spark.databricks.task_scheduling_lanes.slot_reservation.quick_jump.count")
					validatedMetrics["databricks.spark.databricks.task_scheduling_lanes.slot_reservation.quick_jump.count"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "", ms.At(i).Unit())
					assert.Equal(t, false, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.name")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.databricks.task_scheduling_lanes.slot_reservation.slots_reserved":
					assert.False(t, validatedMetrics["databricks.spark.databricks.task_scheduling_lanes.slot_reservation.slots_reserved"], "Found a duplicate in the metrics slice: databricks.spark.databricks.task_scheduling_lanes.slot_reservation.slots_reserved")
					validatedMetrics["databricks.spark.databricks.task_scheduling_lanes.slot_reservation.slots_reserved"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "", ms.At(i).Unit())
					assert.Equal(t, false, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.name")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.databricks.task_scheduling_lanes.slow_lane_active_pools":
					assert.False(t, validatedMetrics["databricks.spark.databricks.task_scheduling_lanes.slow_lane_active_pools"], "Found a duplicate in the metrics slice: databricks.spark.databricks.task_scheduling_lanes.slow_lane_active_pools")
					validatedMetrics["databricks.spark.databricks.task_scheduling_lanes.slow_lane_active_pools"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "", ms.At(i).Unit())
					assert.Equal(t, false, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.name")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.databricks.task_scheduling_lanes.totalquerygroupsfinished":
					assert.False(t, validatedMetrics["databricks.spark.databricks.task_scheduling_lanes.totalquerygroupsfinished"], "Found a duplicate in the metrics slice: databricks.spark.databricks.task_scheduling_lanes.totalquerygroupsfinished")
					validatedMetrics["databricks.spark.databricks.task_scheduling_lanes.totalquerygroupsfinished"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "", ms.At(i).Unit())
					assert.Equal(t, false, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.name")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.executor.disk_used":
					assert.False(t, validatedMetrics["databricks.spark.executor.disk_used"], "Found a duplicate in the metrics slice: databricks.spark.executor.disk_used")
					validatedMetrics["databricks.spark.executor.disk_used"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "By", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("spark.executor.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.executor.max_memory":
					assert.False(t, validatedMetrics["databricks.spark.executor.max_memory"], "Found a duplicate in the metrics slice: databricks.spark.executor.max_memory")
					validatedMetrics["databricks.spark.executor.max_memory"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "By", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("spark.executor.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.executor.memory_used":
					assert.False(t, validatedMetrics["databricks.spark.executor.memory_used"], "Found a duplicate in the metrics slice: databricks.spark.executor.memory_used")
					validatedMetrics["databricks.spark.executor.memory_used"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "By", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("spark.executor.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.executor.total_input_bytes":
					assert.False(t, validatedMetrics["databricks.spark.executor.total_input_bytes"], "Found a duplicate in the metrics slice: databricks.spark.executor.total_input_bytes")
					validatedMetrics["databricks.spark.executor.total_input_bytes"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "By", ms.At(i).Unit())
					assert.Equal(t, false, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("spark.executor.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.executor.total_shuffle_read":
					assert.False(t, validatedMetrics["databricks.spark.executor.total_shuffle_read"], "Found a duplicate in the metrics slice: databricks.spark.executor.total_shuffle_read")
					validatedMetrics["databricks.spark.executor.total_shuffle_read"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "By", ms.At(i).Unit())
					assert.Equal(t, false, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("spark.executor.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.executor.total_shuffle_write":
					assert.False(t, validatedMetrics["databricks.spark.executor.total_shuffle_write"], "Found a duplicate in the metrics slice: databricks.spark.executor.total_shuffle_write")
					validatedMetrics["databricks.spark.executor.total_shuffle_write"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "By", ms.At(i).Unit())
					assert.Equal(t, false, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("spark.executor.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.executor_metrics.direct_pool.memory":
					assert.False(t, validatedMetrics["databricks.spark.executor_metrics.direct_pool.memory"], "Found a duplicate in the metrics slice: databricks.spark.executor_metrics.direct_pool.memory")
					validatedMetrics["databricks.spark.executor_metrics.direct_pool.memory"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeDouble, dp.ValueType())
					assert.Equal(t, float64(1), dp.DoubleValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.name")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.executor_metrics.jvm.heap.memory":
					assert.False(t, validatedMetrics["databricks.spark.executor_metrics.jvm.heap.memory"], "Found a duplicate in the metrics slice: databricks.spark.executor_metrics.jvm.heap.memory")
					validatedMetrics["databricks.spark.executor_metrics.jvm.heap.memory"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeDouble, dp.ValueType())
					assert.Equal(t, float64(1), dp.DoubleValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.name")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.executor_metrics.jvm.off_heap.memory":
					assert.False(t, validatedMetrics["databricks.spark.executor_metrics.jvm.off_heap.memory"], "Found a duplicate in the metrics slice: databricks.spark.executor_metrics.jvm.off_heap.memory")
					validatedMetrics["databricks.spark.executor_metrics.jvm.off_heap.memory"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeDouble, dp.ValueType())
					assert.Equal(t, float64(1), dp.DoubleValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.name")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.executor_metrics.major_gc.count":
					assert.False(t, validatedMetrics["databricks.spark.executor_metrics.major_gc.count"], "Found a duplicate in the metrics slice: databricks.spark.executor_metrics.major_gc.count")
					validatedMetrics["databricks.spark.executor_metrics.major_gc.count"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeDouble, dp.ValueType())
					assert.Equal(t, float64(1), dp.DoubleValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.name")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.executor_metrics.major_gc.time":
					assert.False(t, validatedMetrics["databricks.spark.executor_metrics.major_gc.time"], "Found a duplicate in the metrics slice: databricks.spark.executor_metrics.major_gc.time")
					validatedMetrics["databricks.spark.executor_metrics.major_gc.time"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeDouble, dp.ValueType())
					assert.Equal(t, float64(1), dp.DoubleValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.name")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.executor_metrics.mapped_pool.memory":
					assert.False(t, validatedMetrics["databricks.spark.executor_metrics.mapped_pool.memory"], "Found a duplicate in the metrics slice: databricks.spark.executor_metrics.mapped_pool.memory")
					validatedMetrics["databricks.spark.executor_metrics.mapped_pool.memory"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeDouble, dp.ValueType())
					assert.Equal(t, float64(1), dp.DoubleValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.name")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.executor_metrics.minor_gc.count":
					assert.False(t, validatedMetrics["databricks.spark.executor_metrics.minor_gc.count"], "Found a duplicate in the metrics slice: databricks.spark.executor_metrics.minor_gc.count")
					validatedMetrics["databricks.spark.executor_metrics.minor_gc.count"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeDouble, dp.ValueType())
					assert.Equal(t, float64(1), dp.DoubleValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.name")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.executor_metrics.minor_gc.time":
					assert.False(t, validatedMetrics["databricks.spark.executor_metrics.minor_gc.time"], "Found a duplicate in the metrics slice: databricks.spark.executor_metrics.minor_gc.time")
					validatedMetrics["databricks.spark.executor_metrics.minor_gc.time"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeDouble, dp.ValueType())
					assert.Equal(t, float64(1), dp.DoubleValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.name")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.executor_metrics.off_heap.execution.memory":
					assert.False(t, validatedMetrics["databricks.spark.executor_metrics.off_heap.execution.memory"], "Found a duplicate in the metrics slice: databricks.spark.executor_metrics.off_heap.execution.memory")
					validatedMetrics["databricks.spark.executor_metrics.off_heap.execution.memory"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeDouble, dp.ValueType())
					assert.Equal(t, float64(1), dp.DoubleValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.name")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.executor_metrics.off_heap.storage.memory":
					assert.False(t, validatedMetrics["databricks.spark.executor_metrics.off_heap.storage.memory"], "Found a duplicate in the metrics slice: databricks.spark.executor_metrics.off_heap.storage.memory")
					validatedMetrics["databricks.spark.executor_metrics.off_heap.storage.memory"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeDouble, dp.ValueType())
					assert.Equal(t, float64(1), dp.DoubleValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.name")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.executor_metrics.off_heap.unified.memory":
					assert.False(t, validatedMetrics["databricks.spark.executor_metrics.off_heap.unified.memory"], "Found a duplicate in the metrics slice: databricks.spark.executor_metrics.off_heap.unified.memory")
					validatedMetrics["databricks.spark.executor_metrics.off_heap.unified.memory"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeDouble, dp.ValueType())
					assert.Equal(t, float64(1), dp.DoubleValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.name")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.executor_metrics.on_heap.execution.memory":
					assert.False(t, validatedMetrics["databricks.spark.executor_metrics.on_heap.execution.memory"], "Found a duplicate in the metrics slice: databricks.spark.executor_metrics.on_heap.execution.memory")
					validatedMetrics["databricks.spark.executor_metrics.on_heap.execution.memory"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeDouble, dp.ValueType())
					assert.Equal(t, float64(1), dp.DoubleValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.name")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.executor_metrics.on_heap.storage.memory":
					assert.False(t, validatedMetrics["databricks.spark.executor_metrics.on_heap.storage.memory"], "Found a duplicate in the metrics slice: databricks.spark.executor_metrics.on_heap.storage.memory")
					validatedMetrics["databricks.spark.executor_metrics.on_heap.storage.memory"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeDouble, dp.ValueType())
					assert.Equal(t, float64(1), dp.DoubleValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.name")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.executor_metrics.on_heap.unified.memory":
					assert.False(t, validatedMetrics["databricks.spark.executor_metrics.on_heap.unified.memory"], "Found a duplicate in the metrics slice: databricks.spark.executor_metrics.on_heap.unified.memory")
					validatedMetrics["databricks.spark.executor_metrics.on_heap.unified.memory"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeDouble, dp.ValueType())
					assert.Equal(t, float64(1), dp.DoubleValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.name")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.executor_metrics.process_tree.jvm_rss.memory":
					assert.False(t, validatedMetrics["databricks.spark.executor_metrics.process_tree.jvm_rss.memory"], "Found a duplicate in the metrics slice: databricks.spark.executor_metrics.process_tree.jvm_rss.memory")
					validatedMetrics["databricks.spark.executor_metrics.process_tree.jvm_rss.memory"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeDouble, dp.ValueType())
					assert.Equal(t, float64(1), dp.DoubleValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.name")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.executor_metrics.process_tree.jvm_v.memory":
					assert.False(t, validatedMetrics["databricks.spark.executor_metrics.process_tree.jvm_v.memory"], "Found a duplicate in the metrics slice: databricks.spark.executor_metrics.process_tree.jvm_v.memory")
					validatedMetrics["databricks.spark.executor_metrics.process_tree.jvm_v.memory"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeDouble, dp.ValueType())
					assert.Equal(t, float64(1), dp.DoubleValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.name")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.executor_metrics.process_tree.other_rss.memory":
					assert.False(t, validatedMetrics["databricks.spark.executor_metrics.process_tree.other_rss.memory"], "Found a duplicate in the metrics slice: databricks.spark.executor_metrics.process_tree.other_rss.memory")
					validatedMetrics["databricks.spark.executor_metrics.process_tree.other_rss.memory"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeDouble, dp.ValueType())
					assert.Equal(t, float64(1), dp.DoubleValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.name")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.executor_metrics.process_tree.other_v.memory":
					assert.False(t, validatedMetrics["databricks.spark.executor_metrics.process_tree.other_v.memory"], "Found a duplicate in the metrics slice: databricks.spark.executor_metrics.process_tree.other_v.memory")
					validatedMetrics["databricks.spark.executor_metrics.process_tree.other_v.memory"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeDouble, dp.ValueType())
					assert.Equal(t, float64(1), dp.DoubleValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.name")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.executor_metrics.process_tree.python_rss.memory":
					assert.False(t, validatedMetrics["databricks.spark.executor_metrics.process_tree.python_rss.memory"], "Found a duplicate in the metrics slice: databricks.spark.executor_metrics.process_tree.python_rss.memory")
					validatedMetrics["databricks.spark.executor_metrics.process_tree.python_rss.memory"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeDouble, dp.ValueType())
					assert.Equal(t, float64(1), dp.DoubleValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.name")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.executor_metrics.process_tree.python_v.memory":
					assert.False(t, validatedMetrics["databricks.spark.executor_metrics.process_tree.python_v.memory"], "Found a duplicate in the metrics slice: databricks.spark.executor_metrics.process_tree.python_v.memory")
					validatedMetrics["databricks.spark.executor_metrics.process_tree.python_v.memory"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeDouble, dp.ValueType())
					assert.Equal(t, float64(1), dp.DoubleValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.name")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.hive_external_catalog.file_cache.hits":
					assert.False(t, validatedMetrics["databricks.spark.hive_external_catalog.file_cache.hits"], "Found a duplicate in the metrics slice: databricks.spark.hive_external_catalog.file_cache.hits")
					validatedMetrics["databricks.spark.hive_external_catalog.file_cache.hits"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "", ms.At(i).Unit())
					assert.Equal(t, false, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.name")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.hive_external_catalog.files_discovered":
					assert.False(t, validatedMetrics["databricks.spark.hive_external_catalog.files_discovered"], "Found a duplicate in the metrics slice: databricks.spark.hive_external_catalog.files_discovered")
					validatedMetrics["databricks.spark.hive_external_catalog.files_discovered"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "", ms.At(i).Unit())
					assert.Equal(t, false, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.name")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.hive_external_catalog.hive_client_calls":
					assert.False(t, validatedMetrics["databricks.spark.hive_external_catalog.hive_client_calls"], "Found a duplicate in the metrics slice: databricks.spark.hive_external_catalog.hive_client_calls")
					validatedMetrics["databricks.spark.hive_external_catalog.hive_client_calls"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "", ms.At(i).Unit())
					assert.Equal(t, false, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.name")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.hive_external_catalog.parallel_listing_jobs.count":
					assert.False(t, validatedMetrics["databricks.spark.hive_external_catalog.parallel_listing_jobs.count"], "Found a duplicate in the metrics slice: databricks.spark.hive_external_catalog.parallel_listing_jobs.count")
					validatedMetrics["databricks.spark.hive_external_catalog.parallel_listing_jobs.count"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "", ms.At(i).Unit())
					assert.Equal(t, false, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.name")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.hive_external_catalog.partitions_fetched":
					assert.False(t, validatedMetrics["databricks.spark.hive_external_catalog.partitions_fetched"], "Found a duplicate in the metrics slice: databricks.spark.hive_external_catalog.partitions_fetched")
					validatedMetrics["databricks.spark.hive_external_catalog.partitions_fetched"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "", ms.At(i).Unit())
					assert.Equal(t, false, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.name")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.job.num_active_stages":
					assert.False(t, validatedMetrics["databricks.spark.job.num_active_stages"], "Found a duplicate in the metrics slice: databricks.spark.job.num_active_stages")
					validatedMetrics["databricks.spark.job.num_active_stages"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "{stages}", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("spark.job.id")
					assert.True(t, ok)
					assert.EqualValues(t, 1, attrVal.Int())
				case "databricks.spark.job.num_active_tasks":
					assert.False(t, validatedMetrics["databricks.spark.job.num_active_tasks"], "Found a duplicate in the metrics slice: databricks.spark.job.num_active_tasks")
					validatedMetrics["databricks.spark.job.num_active_tasks"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "{tasks}", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("spark.job.id")
					assert.True(t, ok)
					assert.EqualValues(t, 1, attrVal.Int())
				case "databricks.spark.job.num_completed_stages":
					assert.False(t, validatedMetrics["databricks.spark.job.num_completed_stages"], "Found a duplicate in the metrics slice: databricks.spark.job.num_completed_stages")
					validatedMetrics["databricks.spark.job.num_completed_stages"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "{stages}", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("spark.job.id")
					assert.True(t, ok)
					assert.EqualValues(t, 1, attrVal.Int())
				case "databricks.spark.job.num_completed_tasks":
					assert.False(t, validatedMetrics["databricks.spark.job.num_completed_tasks"], "Found a duplicate in the metrics slice: databricks.spark.job.num_completed_tasks")
					validatedMetrics["databricks.spark.job.num_completed_tasks"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "{tasks}", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("spark.job.id")
					assert.True(t, ok)
					assert.EqualValues(t, 1, attrVal.Int())
				case "databricks.spark.job.num_failed_stages":
					assert.False(t, validatedMetrics["databricks.spark.job.num_failed_stages"], "Found a duplicate in the metrics slice: databricks.spark.job.num_failed_stages")
					validatedMetrics["databricks.spark.job.num_failed_stages"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "{stages}", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("spark.job.id")
					assert.True(t, ok)
					assert.EqualValues(t, 1, attrVal.Int())
				case "databricks.spark.job.num_failed_tasks":
					assert.False(t, validatedMetrics["databricks.spark.job.num_failed_tasks"], "Found a duplicate in the metrics slice: databricks.spark.job.num_failed_tasks")
					validatedMetrics["databricks.spark.job.num_failed_tasks"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "{tasks}", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("spark.job.id")
					assert.True(t, ok)
					assert.EqualValues(t, 1, attrVal.Int())
				case "databricks.spark.job.num_skipped_stages":
					assert.False(t, validatedMetrics["databricks.spark.job.num_skipped_stages"], "Found a duplicate in the metrics slice: databricks.spark.job.num_skipped_stages")
					validatedMetrics["databricks.spark.job.num_skipped_stages"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "{stages}", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("spark.job.id")
					assert.True(t, ok)
					assert.EqualValues(t, 1, attrVal.Int())
				case "databricks.spark.job.num_skipped_tasks":
					assert.False(t, validatedMetrics["databricks.spark.job.num_skipped_tasks"], "Found a duplicate in the metrics slice: databricks.spark.job.num_skipped_tasks")
					validatedMetrics["databricks.spark.job.num_skipped_tasks"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "{tasks}", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("spark.job.id")
					assert.True(t, ok)
					assert.EqualValues(t, 1, attrVal.Int())
				case "databricks.spark.job.num_tasks":
					assert.False(t, validatedMetrics["databricks.spark.job.num_tasks"], "Found a duplicate in the metrics slice: databricks.spark.job.num_tasks")
					validatedMetrics["databricks.spark.job.num_tasks"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "{tasks}", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("spark.job.id")
					assert.True(t, ok)
					assert.EqualValues(t, 1, attrVal.Int())
				case "databricks.spark.jvm.cpu.time":
					assert.False(t, validatedMetrics["databricks.spark.jvm.cpu.time"], "Found a duplicate in the metrics slice: databricks.spark.jvm.cpu.time")
					validatedMetrics["databricks.spark.jvm.cpu.time"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "", ms.At(i).Unit())
					assert.Equal(t, false, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeDouble, dp.ValueType())
					assert.Equal(t, float64(1), dp.DoubleValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.name")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.live_listener_bus.events_posted.count":
					assert.False(t, validatedMetrics["databricks.spark.live_listener_bus.events_posted.count"], "Found a duplicate in the metrics slice: databricks.spark.live_listener_bus.events_posted.count")
					validatedMetrics["databricks.spark.live_listener_bus.events_posted.count"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "", ms.At(i).Unit())
					assert.Equal(t, false, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.name")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.live_listener_bus.queue.app_status.dropped_events.count":
					assert.False(t, validatedMetrics["databricks.spark.live_listener_bus.queue.app_status.dropped_events.count"], "Found a duplicate in the metrics slice: databricks.spark.live_listener_bus.queue.app_status.dropped_events.count")
					validatedMetrics["databricks.spark.live_listener_bus.queue.app_status.dropped_events.count"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "", ms.At(i).Unit())
					assert.Equal(t, false, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.name")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.live_listener_bus.queue.appstatus.size":
					assert.False(t, validatedMetrics["databricks.spark.live_listener_bus.queue.appstatus.size"], "Found a duplicate in the metrics slice: databricks.spark.live_listener_bus.queue.appstatus.size")
					validatedMetrics["databricks.spark.live_listener_bus.queue.appstatus.size"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeDouble, dp.ValueType())
					assert.Equal(t, float64(1), dp.DoubleValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.name")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.live_listener_bus.queue.executor_management.dropped_events.count":
					assert.False(t, validatedMetrics["databricks.spark.live_listener_bus.queue.executor_management.dropped_events.count"], "Found a duplicate in the metrics slice: databricks.spark.live_listener_bus.queue.executor_management.dropped_events.count")
					validatedMetrics["databricks.spark.live_listener_bus.queue.executor_management.dropped_events.count"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "", ms.At(i).Unit())
					assert.Equal(t, false, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.name")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.live_listener_bus.queue.executormanagement.size":
					assert.False(t, validatedMetrics["databricks.spark.live_listener_bus.queue.executormanagement.size"], "Found a duplicate in the metrics slice: databricks.spark.live_listener_bus.queue.executormanagement.size")
					validatedMetrics["databricks.spark.live_listener_bus.queue.executormanagement.size"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeDouble, dp.ValueType())
					assert.Equal(t, float64(1), dp.DoubleValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.name")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.live_listener_bus.queue.shared.dropped_events.count":
					assert.False(t, validatedMetrics["databricks.spark.live_listener_bus.queue.shared.dropped_events.count"], "Found a duplicate in the metrics slice: databricks.spark.live_listener_bus.queue.shared.dropped_events.count")
					validatedMetrics["databricks.spark.live_listener_bus.queue.shared.dropped_events.count"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "", ms.At(i).Unit())
					assert.Equal(t, false, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.name")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.live_listener_bus.queue.shared.size":
					assert.False(t, validatedMetrics["databricks.spark.live_listener_bus.queue.shared.size"], "Found a duplicate in the metrics slice: databricks.spark.live_listener_bus.queue.shared.size")
					validatedMetrics["databricks.spark.live_listener_bus.queue.shared.size"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeDouble, dp.ValueType())
					assert.Equal(t, float64(1), dp.DoubleValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.name")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.live_listener_bus.queue.streams.dropped_events.count":
					assert.False(t, validatedMetrics["databricks.spark.live_listener_bus.queue.streams.dropped_events.count"], "Found a duplicate in the metrics slice: databricks.spark.live_listener_bus.queue.streams.dropped_events.count")
					validatedMetrics["databricks.spark.live_listener_bus.queue.streams.dropped_events.count"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "", ms.At(i).Unit())
					assert.Equal(t, false, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.name")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.live_listener_bus.queue.streams.size":
					assert.False(t, validatedMetrics["databricks.spark.live_listener_bus.queue.streams.size"], "Found a duplicate in the metrics slice: databricks.spark.live_listener_bus.queue.streams.size")
					validatedMetrics["databricks.spark.live_listener_bus.queue.streams.size"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeDouble, dp.ValueType())
					assert.Equal(t, float64(1), dp.DoubleValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.name")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.spark_sql_operation_manager.hive_operations.count":
					assert.False(t, validatedMetrics["databricks.spark.spark_sql_operation_manager.hive_operations.count"], "Found a duplicate in the metrics slice: databricks.spark.spark_sql_operation_manager.hive_operations.count")
					validatedMetrics["databricks.spark.spark_sql_operation_manager.hive_operations.count"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeDouble, dp.ValueType())
					assert.Equal(t, float64(1), dp.DoubleValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("pipeline.name")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.stage.disk_bytes_spilled":
					assert.False(t, validatedMetrics["databricks.spark.stage.disk_bytes_spilled"], "Found a duplicate in the metrics slice: databricks.spark.stage.disk_bytes_spilled")
					validatedMetrics["databricks.spark.stage.disk_bytes_spilled"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "By", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("spark.job.id")
					assert.True(t, ok)
					assert.EqualValues(t, 1, attrVal.Int())
				case "databricks.spark.stage.executor_run_time":
					assert.False(t, validatedMetrics["databricks.spark.stage.executor_run_time"], "Found a duplicate in the metrics slice: databricks.spark.stage.executor_run_time")
					validatedMetrics["databricks.spark.stage.executor_run_time"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("spark.job.id")
					assert.True(t, ok)
					assert.EqualValues(t, 1, attrVal.Int())
				case "databricks.spark.stage.input_bytes":
					assert.False(t, validatedMetrics["databricks.spark.stage.input_bytes"], "Found a duplicate in the metrics slice: databricks.spark.stage.input_bytes")
					validatedMetrics["databricks.spark.stage.input_bytes"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("spark.job.id")
					assert.True(t, ok)
					assert.EqualValues(t, 1, attrVal.Int())
				case "databricks.spark.stage.input_records":
					assert.False(t, validatedMetrics["databricks.spark.stage.input_records"], "Found a duplicate in the metrics slice: databricks.spark.stage.input_records")
					validatedMetrics["databricks.spark.stage.input_records"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("spark.job.id")
					assert.True(t, ok)
					assert.EqualValues(t, 1, attrVal.Int())
				case "databricks.spark.stage.memory_bytes_spilled":
					assert.False(t, validatedMetrics["databricks.spark.stage.memory_bytes_spilled"], "Found a duplicate in the metrics slice: databricks.spark.stage.memory_bytes_spilled")
					validatedMetrics["databricks.spark.stage.memory_bytes_spilled"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "By", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("spark.job.id")
					assert.True(t, ok)
					assert.EqualValues(t, 1, attrVal.Int())
				case "databricks.spark.stage.output_bytes":
					assert.False(t, validatedMetrics["databricks.spark.stage.output_bytes"], "Found a duplicate in the metrics slice: databricks.spark.stage.output_bytes")
					validatedMetrics["databricks.spark.stage.output_bytes"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "By", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("spark.job.id")
					assert.True(t, ok)
					assert.EqualValues(t, 1, attrVal.Int())
				case "databricks.spark.stage.output_records":
					assert.False(t, validatedMetrics["databricks.spark.stage.output_records"], "Found a duplicate in the metrics slice: databricks.spark.stage.output_records")
					validatedMetrics["databricks.spark.stage.output_records"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("spark.job.id")
					assert.True(t, ok)
					assert.EqualValues(t, 1, attrVal.Int())
				case "databricks.spark.timer.dag_scheduler.message_processing.time":
					assert.False(t, validatedMetrics["databricks.spark.timer.dag_scheduler.message_processing.time"], "Found a duplicate in the metrics slice: databricks.spark.timer.dag_scheduler.message_processing.time")
					validatedMetrics["databricks.spark.timer.dag_scheduler.message_processing.time"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "ms", ms.At(i).Unit())
					assert.Equal(t, false, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeDouble, dp.ValueType())
					assert.Equal(t, float64(1), dp.DoubleValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.timer.live_listener_bus.listener_processing.apache.spark.sql.execution.streaming.query_listener_bus.time":
					assert.False(t, validatedMetrics["databricks.spark.timer.live_listener_bus.listener_processing.apache.spark.sql.execution.streaming.query_listener_bus.time"], "Found a duplicate in the metrics slice: databricks.spark.timer.live_listener_bus.listener_processing.apache.spark.sql.execution.streaming.query_listener_bus.time")
					validatedMetrics["databricks.spark.timer.live_listener_bus.listener_processing.apache.spark.sql.execution.streaming.query_listener_bus.time"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "ms", ms.At(i).Unit())
					assert.Equal(t, false, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeDouble, dp.ValueType())
					assert.Equal(t, float64(1), dp.DoubleValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.timer.live_listener_bus.listener_processing.apache.spark.sql.execution.time":
					assert.False(t, validatedMetrics["databricks.spark.timer.live_listener_bus.listener_processing.apache.spark.sql.execution.time"], "Found a duplicate in the metrics slice: databricks.spark.timer.live_listener_bus.listener_processing.apache.spark.sql.execution.time")
					validatedMetrics["databricks.spark.timer.live_listener_bus.listener_processing.apache.spark.sql.execution.time"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "ms", ms.At(i).Unit())
					assert.Equal(t, false, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeDouble, dp.ValueType())
					assert.Equal(t, float64(1), dp.DoubleValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.timer.live_listener_bus.listener_processing.apache.spark.sql.execution.ui.sql_app_status_listener.time":
					assert.False(t, validatedMetrics["databricks.spark.timer.live_listener_bus.listener_processing.apache.spark.sql.execution.ui.sql_app_status_listener.time"], "Found a duplicate in the metrics slice: databricks.spark.timer.live_listener_bus.listener_processing.apache.spark.sql.execution.ui.sql_app_status_listener.time")
					validatedMetrics["databricks.spark.timer.live_listener_bus.listener_processing.apache.spark.sql.execution.ui.sql_app_status_listener.time"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "ms", ms.At(i).Unit())
					assert.Equal(t, false, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeDouble, dp.ValueType())
					assert.Equal(t, float64(1), dp.DoubleValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.timer.live_listener_bus.listener_processing.apache.spark.sql.hive.thriftserver.ui.hive_thrift_server2listener.time":
					assert.False(t, validatedMetrics["databricks.spark.timer.live_listener_bus.listener_processing.apache.spark.sql.hive.thriftserver.ui.hive_thrift_server2listener.time"], "Found a duplicate in the metrics slice: databricks.spark.timer.live_listener_bus.listener_processing.apache.spark.sql.hive.thriftserver.ui.hive_thrift_server2listener.time")
					validatedMetrics["databricks.spark.timer.live_listener_bus.listener_processing.apache.spark.sql.hive.thriftserver.ui.hive_thrift_server2listener.time"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "ms", ms.At(i).Unit())
					assert.Equal(t, false, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeDouble, dp.ValueType())
					assert.Equal(t, float64(1), dp.DoubleValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.timer.live_listener_bus.listener_processing.apache.spark.sql.spark_session.time":
					assert.False(t, validatedMetrics["databricks.spark.timer.live_listener_bus.listener_processing.apache.spark.sql.spark_session.time"], "Found a duplicate in the metrics slice: databricks.spark.timer.live_listener_bus.listener_processing.apache.spark.sql.spark_session.time")
					validatedMetrics["databricks.spark.timer.live_listener_bus.listener_processing.apache.spark.sql.spark_session.time"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "ms", ms.At(i).Unit())
					assert.Equal(t, false, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeDouble, dp.ValueType())
					assert.Equal(t, float64(1), dp.DoubleValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.timer.live_listener_bus.listener_processing.apache.spark.sql.util.execution_listener_bus.time":
					assert.False(t, validatedMetrics["databricks.spark.timer.live_listener_bus.listener_processing.apache.spark.sql.util.execution_listener_bus.time"], "Found a duplicate in the metrics slice: databricks.spark.timer.live_listener_bus.listener_processing.apache.spark.sql.util.execution_listener_bus.time")
					validatedMetrics["databricks.spark.timer.live_listener_bus.listener_processing.apache.spark.sql.util.execution_listener_bus.time"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "ms", ms.At(i).Unit())
					assert.Equal(t, false, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeDouble, dp.ValueType())
					assert.Equal(t, float64(1), dp.DoubleValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.timer.live_listener_bus.listener_processing.apache.spark.status.app_status_listener.time":
					assert.False(t, validatedMetrics["databricks.spark.timer.live_listener_bus.listener_processing.apache.spark.status.app_status_listener.time"], "Found a duplicate in the metrics slice: databricks.spark.timer.live_listener_bus.listener_processing.apache.spark.status.app_status_listener.time")
					validatedMetrics["databricks.spark.timer.live_listener_bus.listener_processing.apache.spark.status.app_status_listener.time"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "ms", ms.At(i).Unit())
					assert.Equal(t, false, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeDouble, dp.ValueType())
					assert.Equal(t, float64(1), dp.DoubleValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.timer.live_listener_bus.listener_processing.apache.spark.util.profiler_env.time":
					assert.False(t, validatedMetrics["databricks.spark.timer.live_listener_bus.listener_processing.apache.spark.util.profiler_env.time"], "Found a duplicate in the metrics slice: databricks.spark.timer.live_listener_bus.listener_processing.apache.spark.util.profiler_env.time")
					validatedMetrics["databricks.spark.timer.live_listener_bus.listener_processing.apache.spark.util.profiler_env.time"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "ms", ms.At(i).Unit())
					assert.Equal(t, false, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeDouble, dp.ValueType())
					assert.Equal(t, float64(1), dp.DoubleValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.timer.live_listener_bus.listener_processing.databricks.backend.daemon.driver.data_plane_event_listener.time":
					assert.False(t, validatedMetrics["databricks.spark.timer.live_listener_bus.listener_processing.databricks.backend.daemon.driver.data_plane_event_listener.time"], "Found a duplicate in the metrics slice: databricks.spark.timer.live_listener_bus.listener_processing.databricks.backend.daemon.driver.data_plane_event_listener.time")
					validatedMetrics["databricks.spark.timer.live_listener_bus.listener_processing.databricks.backend.daemon.driver.data_plane_event_listener.time"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "ms", ms.At(i).Unit())
					assert.Equal(t, false, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeDouble, dp.ValueType())
					assert.Equal(t, float64(1), dp.DoubleValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.timer.live_listener_bus.listener_processing.databricks.backend.daemon.driver.dbc_event_logging_listener.time":
					assert.False(t, validatedMetrics["databricks.spark.timer.live_listener_bus.listener_processing.databricks.backend.daemon.driver.dbc_event_logging_listener.time"], "Found a duplicate in the metrics slice: databricks.spark.timer.live_listener_bus.listener_processing.databricks.backend.daemon.driver.dbc_event_logging_listener.time")
					validatedMetrics["databricks.spark.timer.live_listener_bus.listener_processing.databricks.backend.daemon.driver.dbc_event_logging_listener.time"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "ms", ms.At(i).Unit())
					assert.Equal(t, false, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeDouble, dp.ValueType())
					assert.Equal(t, float64(1), dp.DoubleValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.timer.live_listener_bus.listener_processing.databricks.photon.photon_cleanup_listener.time":
					assert.False(t, validatedMetrics["databricks.spark.timer.live_listener_bus.listener_processing.databricks.photon.photon_cleanup_listener.time"], "Found a duplicate in the metrics slice: databricks.spark.timer.live_listener_bus.listener_processing.databricks.photon.photon_cleanup_listener.time")
					validatedMetrics["databricks.spark.timer.live_listener_bus.listener_processing.databricks.photon.photon_cleanup_listener.time"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "ms", ms.At(i).Unit())
					assert.Equal(t, false, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeDouble, dp.ValueType())
					assert.Equal(t, float64(1), dp.DoubleValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.timer.live_listener_bus.listener_processing.databricks.spark.util.executor_time_logging_listener.time":
					assert.False(t, validatedMetrics["databricks.spark.timer.live_listener_bus.listener_processing.databricks.spark.util.executor_time_logging_listener.time"], "Found a duplicate in the metrics slice: databricks.spark.timer.live_listener_bus.listener_processing.databricks.spark.util.executor_time_logging_listener.time")
					validatedMetrics["databricks.spark.timer.live_listener_bus.listener_processing.databricks.spark.util.executor_time_logging_listener.time"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "ms", ms.At(i).Unit())
					assert.Equal(t, false, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeDouble, dp.ValueType())
					assert.Equal(t, float64(1), dp.DoubleValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.timer.live_listener_bus.listener_processing.databricks.spark.util.usage_logging_listener.time":
					assert.False(t, validatedMetrics["databricks.spark.timer.live_listener_bus.listener_processing.databricks.spark.util.usage_logging_listener.time"], "Found a duplicate in the metrics slice: databricks.spark.timer.live_listener_bus.listener_processing.databricks.spark.util.usage_logging_listener.time")
					validatedMetrics["databricks.spark.timer.live_listener_bus.listener_processing.databricks.spark.util.usage_logging_listener.time"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "ms", ms.At(i).Unit())
					assert.Equal(t, false, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeDouble, dp.ValueType())
					assert.Equal(t, float64(1), dp.DoubleValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.timer.live_listener_bus.listener_processing.databricks.sql.advice.advisor_listener.time":
					assert.False(t, validatedMetrics["databricks.spark.timer.live_listener_bus.listener_processing.databricks.sql.advice.advisor_listener.time"], "Found a duplicate in the metrics slice: databricks.spark.timer.live_listener_bus.listener_processing.databricks.sql.advice.advisor_listener.time")
					validatedMetrics["databricks.spark.timer.live_listener_bus.listener_processing.databricks.sql.advice.advisor_listener.time"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "ms", ms.At(i).Unit())
					assert.Equal(t, false, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeDouble, dp.ValueType())
					assert.Equal(t, float64(1), dp.DoubleValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.timer.live_listener_bus.listener_processing.databricks.sql.debugger.query_watchdog_listener.time":
					assert.False(t, validatedMetrics["databricks.spark.timer.live_listener_bus.listener_processing.databricks.sql.debugger.query_watchdog_listener.time"], "Found a duplicate in the metrics slice: databricks.spark.timer.live_listener_bus.listener_processing.databricks.sql.debugger.query_watchdog_listener.time")
					validatedMetrics["databricks.spark.timer.live_listener_bus.listener_processing.databricks.sql.debugger.query_watchdog_listener.time"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "ms", ms.At(i).Unit())
					assert.Equal(t, false, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeDouble, dp.ValueType())
					assert.Equal(t, float64(1), dp.DoubleValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.timer.live_listener_bus.listener_processing.databricks.sql.execution.ui.io_cache_listener.time":
					assert.False(t, validatedMetrics["databricks.spark.timer.live_listener_bus.listener_processing.databricks.sql.execution.ui.io_cache_listener.time"], "Found a duplicate in the metrics slice: databricks.spark.timer.live_listener_bus.listener_processing.databricks.sql.execution.ui.io_cache_listener.time")
					validatedMetrics["databricks.spark.timer.live_listener_bus.listener_processing.databricks.sql.execution.ui.io_cache_listener.time"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "ms", ms.At(i).Unit())
					assert.Equal(t, false, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeDouble, dp.ValueType())
					assert.Equal(t, float64(1), dp.DoubleValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.timer.live_listener_bus.listener_processing.databricks.sql.io.caching.repeated_reads_estimator.time":
					assert.False(t, validatedMetrics["databricks.spark.timer.live_listener_bus.listener_processing.databricks.sql.io.caching.repeated_reads_estimator.time"], "Found a duplicate in the metrics slice: databricks.spark.timer.live_listener_bus.listener_processing.databricks.sql.io.caching.repeated_reads_estimator.time")
					validatedMetrics["databricks.spark.timer.live_listener_bus.listener_processing.databricks.sql.io.caching.repeated_reads_estimator.time"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "ms", ms.At(i).Unit())
					assert.Equal(t, false, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeDouble, dp.ValueType())
					assert.Equal(t, float64(1), dp.DoubleValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.timer.live_listener_bus.queue.app_status.listener_processing.time":
					assert.False(t, validatedMetrics["databricks.spark.timer.live_listener_bus.queue.app_status.listener_processing.time"], "Found a duplicate in the metrics slice: databricks.spark.timer.live_listener_bus.queue.app_status.listener_processing.time")
					validatedMetrics["databricks.spark.timer.live_listener_bus.queue.app_status.listener_processing.time"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "ms", ms.At(i).Unit())
					assert.Equal(t, false, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeDouble, dp.ValueType())
					assert.Equal(t, float64(1), dp.DoubleValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.timer.live_listener_bus.queue.executor_management.listener_processing.time":
					assert.False(t, validatedMetrics["databricks.spark.timer.live_listener_bus.queue.executor_management.listener_processing.time"], "Found a duplicate in the metrics slice: databricks.spark.timer.live_listener_bus.queue.executor_management.listener_processing.time")
					validatedMetrics["databricks.spark.timer.live_listener_bus.queue.executor_management.listener_processing.time"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "ms", ms.At(i).Unit())
					assert.Equal(t, false, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeDouble, dp.ValueType())
					assert.Equal(t, float64(1), dp.DoubleValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.timer.live_listener_bus.queue.shared.listener_processing.time":
					assert.False(t, validatedMetrics["databricks.spark.timer.live_listener_bus.queue.shared.listener_processing.time"], "Found a duplicate in the metrics slice: databricks.spark.timer.live_listener_bus.queue.shared.listener_processing.time")
					validatedMetrics["databricks.spark.timer.live_listener_bus.queue.shared.listener_processing.time"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "ms", ms.At(i).Unit())
					assert.Equal(t, false, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeDouble, dp.ValueType())
					assert.Equal(t, float64(1), dp.DoubleValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.spark.timer.live_listener_bus.queue.streams.listener_processing.time":
					assert.False(t, validatedMetrics["databricks.spark.timer.live_listener_bus.queue.streams.listener_processing.time"], "Found a duplicate in the metrics slice: databricks.spark.timer.live_listener_bus.queue.streams.listener_processing.time")
					validatedMetrics["databricks.spark.timer.live_listener_bus.queue.streams.listener_processing.time"] = true
					assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
					assert.Equal(t, "n/a", ms.At(i).Description())
					assert.Equal(t, "ms", ms.At(i).Unit())
					assert.Equal(t, false, ms.At(i).Sum().IsMonotonic())
					assert.Equal(t, pmetric.AggregationTemporalityDelta, ms.At(i).Sum().AggregationTemporality())
					dp := ms.At(i).Sum().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeDouble, dp.ValueType())
					assert.Equal(t, float64(1), dp.DoubleValue())
					attrVal, ok := dp.Attributes().Get("spark.app.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.tasks.run.duration":
					assert.False(t, validatedMetrics["databricks.tasks.run.duration"], "Found a duplicate in the metrics slice: databricks.tasks.run.duration")
					validatedMetrics["databricks.tasks.run.duration"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "The execution duration in milliseconds per completed task", ms.At(i).Description())
					assert.Equal(t, "ms", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("job.id")
					assert.True(t, ok)
					assert.EqualValues(t, 1, attrVal.Int())
					attrVal, ok = dp.Attributes().Get("task.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
				case "databricks.tasks.schedule.status":
					assert.False(t, validatedMetrics["databricks.tasks.schedule.status"], "Found a duplicate in the metrics slice: databricks.tasks.schedule.status")
					validatedMetrics["databricks.tasks.schedule.status"] = true
					assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
					assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
					assert.Equal(t, "A snapshot of the pause/run status per task taken at each scrape", ms.At(i).Description())
					assert.Equal(t, "{status}", ms.At(i).Unit())
					dp := ms.At(i).Gauge().DataPoints().At(0)
					assert.Equal(t, start, dp.StartTimestamp())
					assert.Equal(t, ts, dp.Timestamp())
					assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
					assert.Equal(t, int64(1), dp.IntValue())
					attrVal, ok := dp.Attributes().Get("job.id")
					assert.True(t, ok)
					assert.EqualValues(t, 1, attrVal.Int())
					attrVal, ok = dp.Attributes().Get("task.id")
					assert.True(t, ok)
					assert.EqualValues(t, "attr-val", attrVal.Str())
					attrVal, ok = dp.Attributes().Get("task.type")
					assert.True(t, ok)
					assert.Equal(t, "NotebookTask", attrVal.Str())
				}
			}
		})
	}
}

func loadConfig(t *testing.T, name string) MetricsBuilderConfig {
	cm, err := confmaptest.LoadConf(filepath.Join("testdata", "config.yaml"))
	require.NoError(t, err)
	sub, err := cm.Sub(name)
	require.NoError(t, err)
	cfg := DefaultMetricsBuilderConfig()
	require.NoError(t, component.UnmarshalConfig(sub, &cfg))
	return cfg
}
