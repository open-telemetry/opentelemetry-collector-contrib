resourceMetrics:
  - resource:
      attributes:
        - key: k8s.cronjob.name
          value:
            stringValue: test-k8scluster-receiver-cronjob
        - key: k8s.cronjob.uid
          value:
            stringValue: ae24e998-d795-43da-95fb-4351733907b3
        - key: k8s.namespace.name
          value:
            stringValue: my-namespace-1
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: The number of actively running jobs for a cronjob
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.cronjob.active_jobs
            unit: '{job}'
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.126.0-dev
  - resource:
      attributes:
        - key: k8s.cronjob.name
          value:
            stringValue: test-k8scluster-receiver-cronjob
        - key: k8s.cronjob.uid
          value:
            stringValue: ca52f89a-e211-4fce-a798-e620773a6808
        - key: k8s.namespace.name
          value:
            stringValue: my-namespace-2
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: The number of actively running jobs for a cronjob
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.cronjob.active_jobs
            unit: '{job}'
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.126.0-dev
  - resource:
      attributes:
        - key: k8s.deployment.name
          value:
            stringValue: otelcol-751183ea
        - key: k8s.deployment.uid
          value:
            stringValue: 2bfe6157-e7f2-45a8-8d67-5653c583dd94
        - key: k8s.namespace.name
          value:
            stringValue: my-namespace-1
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Total number of available pods (ready for at least minReadySeconds) targeted by this deployment
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.deployment.available
            unit: '{pod}'
          - description: Number of desired pods in this deployment
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.deployment.desired
            unit: '{pod}'
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.126.0-dev
  - resource:
      attributes:
        - key: k8s.hpa.name
          value:
            stringValue: test-k8scluster-receiver-hpa
        - key: k8s.hpa.uid
          value:
            stringValue: 29b23972-851a-4fe6-9c2b-c7a2a23a96b5
        - key: k8s.namespace.name
          value:
            stringValue: my-namespace-1
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Current number of pod replicas managed by this autoscaler.
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.hpa.current_replicas
            unit: '{pod}'
          - description: Desired number of pod replicas managed by this autoscaler.
            gauge:
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.hpa.desired_replicas
            unit: '{pod}'
          - description: Maximum number of replicas to which the autoscaler can scale up.
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.hpa.max_replicas
            unit: '{pod}'
          - description: Minimum number of replicas to which the autoscaler can scale up.
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.hpa.min_replicas
            unit: '{pod}'
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.126.0-dev
  - resource:
      attributes:
        - key: k8s.hpa.name
          value:
            stringValue: test-k8scluster-receiver-hpa
        - key: k8s.hpa.uid
          value:
            stringValue: c4490236-2244-411f-adc1-9d86b6a1d0b2
        - key: k8s.namespace.name
          value:
            stringValue: my-namespace-2
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Current number of pod replicas managed by this autoscaler.
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.hpa.current_replicas
            unit: '{pod}'
          - description: Desired number of pod replicas managed by this autoscaler.
            gauge:
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.hpa.desired_replicas
            unit: '{pod}'
          - description: Maximum number of replicas to which the autoscaler can scale up.
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.hpa.max_replicas
            unit: '{pod}'
          - description: Minimum number of replicas to which the autoscaler can scale up.
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.hpa.min_replicas
            unit: '{pod}'
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.126.0-dev
  - resource:
      attributes:
        - key: k8s.job.name
          value:
            stringValue: test-k8scluster-receiver-cronjob-29130119
        - key: k8s.job.uid
          value:
            stringValue: 23db335d-6ef7-4790-a542-ddb0abf47fdc
        - key: k8s.namespace.name
          value:
            stringValue: my-namespace-2
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: The number of actively running pods for a job
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.job.active_pods
            unit: '{pod}'
          - description: The desired number of successfully finished pods the job should be run with
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.job.desired_successful_pods
            unit: '{pod}'
          - description: The number of pods which reached phase Failed for a job
            gauge:
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.job.failed_pods
            unit: '{pod}'
          - description: The max desired number of pods the job should run at any given time
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.job.max_parallel_pods
            unit: '{pod}'
          - description: The number of pods which reached phase Succeeded for a job
            gauge:
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.job.successful_pods
            unit: '{pod}'
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.126.0-dev
  - resource:
      attributes:
        - key: k8s.job.name
          value:
            stringValue: test-k8scluster-receiver-cronjob-29130119
        - key: k8s.job.uid
          value:
            stringValue: ac251b24-1aed-4b9b-9b42-16be72df8706
        - key: k8s.namespace.name
          value:
            stringValue: my-namespace-1
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: The number of actively running pods for a job
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.job.active_pods
            unit: '{pod}'
          - description: The desired number of successfully finished pods the job should be run with
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.job.desired_successful_pods
            unit: '{pod}'
          - description: The number of pods which reached phase Failed for a job
            gauge:
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.job.failed_pods
            unit: '{pod}'
          - description: The max desired number of pods the job should run at any given time
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.job.max_parallel_pods
            unit: '{pod}'
          - description: The number of pods which reached phase Succeeded for a job
            gauge:
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.job.successful_pods
            unit: '{pod}'
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.126.0-dev
  - resource:
      attributes:
        - key: k8s.job.name
          value:
            stringValue: test-k8scluster-receiver-job
        - key: k8s.job.uid
          value:
            stringValue: 23b21c05-527b-4b2f-ab53-e27e013a7042
        - key: k8s.namespace.name
          value:
            stringValue: my-namespace-1
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: The number of actively running pods for a job
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.job.active_pods
            unit: '{pod}'
          - description: The desired number of successfully finished pods the job should be run with
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.job.desired_successful_pods
            unit: '{pod}'
          - description: The number of pods which reached phase Failed for a job
            gauge:
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.job.failed_pods
            unit: '{pod}'
          - description: The max desired number of pods the job should run at any given time
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.job.max_parallel_pods
            unit: '{pod}'
          - description: The number of pods which reached phase Succeeded for a job
            gauge:
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.job.successful_pods
            unit: '{pod}'
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.126.0-dev
  - resource:
      attributes:
        - key: k8s.job.name
          value:
            stringValue: test-k8scluster-receiver-job
        - key: k8s.job.uid
          value:
            stringValue: 325c6f5c-2add-4e28-84ee-630bbd2ad36a
        - key: k8s.namespace.name
          value:
            stringValue: my-namespace-2
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: The number of actively running pods for a job
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.job.active_pods
            unit: '{pod}'
          - description: The desired number of successfully finished pods the job should be run with
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.job.desired_successful_pods
            unit: '{pod}'
          - description: The number of pods which reached phase Failed for a job
            gauge:
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.job.failed_pods
            unit: '{pod}'
          - description: The max desired number of pods the job should run at any given time
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.job.max_parallel_pods
            unit: '{pod}'
          - description: The number of pods which reached phase Succeeded for a job
            gauge:
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.job.successful_pods
            unit: '{pod}'
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.126.0-dev
  - resource:
      attributes:
        - key: k8s.namespace.name
          value:
            stringValue: my-namespace-1
        - key: k8s.replicaset.name
          value:
            stringValue: otelcol-751183ea-7599f87469
        - key: k8s.replicaset.uid
          value:
            stringValue: f83257ca-c1dc-4f9b-9ed8-4652a9eedf3d
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Total number of available pods (ready for at least minReadySeconds) targeted by this replicaset
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.replicaset.available
            unit: '{pod}'
          - description: Number of desired pods in this replicaset
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.replicaset.desired
            unit: '{pod}'
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.126.0-dev
  - resource:
      attributes:
        - key: k8s.namespace.name
          value:
            stringValue: my-namespace-1
        - key: k8s.statefulset.name
          value:
            stringValue: test-k8scluster-receiver-statefulset
        - key: k8s.statefulset.uid
          value:
            stringValue: 07f042a5-4be8-4be5-96a2-329d54cd6a66
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: The number of pods created by the StatefulSet controller from the StatefulSet version
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.statefulset.current_pods
            unit: '{pod}'
          - description: Number of desired pods in the stateful set (the `spec.replicas` field)
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.statefulset.desired_pods
            unit: '{pod}'
          - description: Number of pods created by the stateful set that have the `Ready` condition
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.statefulset.ready_pods
            unit: '{pod}'
          - description: Number of pods created by the StatefulSet controller from the StatefulSet version
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.statefulset.updated_pods
            unit: '{pod}'
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.126.0-dev
  - resource:
      attributes:
        - key: k8s.namespace.name
          value:
            stringValue: my-namespace-2
        - key: k8s.statefulset.name
          value:
            stringValue: test-k8scluster-receiver-statefulset
        - key: k8s.statefulset.uid
          value:
            stringValue: 21b5debc-58b2-4884-b1f2-db4ecb3d1deb
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: The number of pods created by the StatefulSet controller from the StatefulSet version
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.statefulset.current_pods
            unit: '{pod}'
          - description: Number of desired pods in the stateful set (the `spec.replicas` field)
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.statefulset.desired_pods
            unit: '{pod}'
          - description: Number of pods created by the stateful set that have the `Ready` condition
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.statefulset.ready_pods
            unit: '{pod}'
          - description: Number of pods created by the StatefulSet controller from the StatefulSet version
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.statefulset.updated_pods
            unit: '{pod}'
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.126.0-dev
  - resource:
      attributes:
        - key: k8s.namespace.name
          value:
            stringValue: my-namespace-1
        - key: k8s.node.name
          value:
            stringValue: kind-worker
        - key: k8s.pod.name
          value:
            stringValue: otelcol-751183ea-7599f87469-wtw4j
        - key: k8s.pod.uid
          value:
            stringValue: a0c21a67-69e6-428c-96d8-464f30d3f1ca
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Current phase of the pod (1 - Pending, 2 - Running, 3 - Succeeded, 4 - Failed, 5 - Unknown)
            gauge:
              dataPoints:
                - asInt: "2"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.pod.phase
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.126.0-dev
  - resource:
      attributes:
        - key: k8s.namespace.name
          value:
            stringValue: my-namespace-1
        - key: k8s.node.name
          value:
            stringValue: kind-worker
        - key: k8s.pod.name
          value:
            stringValue: test-k8scluster-receiver-cronjob-29130119-db67n
        - key: k8s.pod.uid
          value:
            stringValue: 4cb2af42-2a1d-46f0-ae87-962783d1f99e
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Current phase of the pod (1 - Pending, 2 - Running, 3 - Succeeded, 4 - Failed, 5 - Unknown)
            gauge:
              dataPoints:
                - asInt: "2"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.pod.phase
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.126.0-dev
  - resource:
      attributes:
        - key: k8s.namespace.name
          value:
            stringValue: my-namespace-1
        - key: k8s.node.name
          value:
            stringValue: kind-worker
        - key: k8s.pod.name
          value:
            stringValue: test-k8scluster-receiver-job-j9lwl
        - key: k8s.pod.uid
          value:
            stringValue: 59da87d9-f320-497e-a088-c2e8c1e03d68
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Current phase of the pod (1 - Pending, 2 - Running, 3 - Succeeded, 4 - Failed, 5 - Unknown)
            gauge:
              dataPoints:
                - asInt: "2"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.pod.phase
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.126.0-dev
  - resource:
      attributes:
        - key: k8s.namespace.name
          value:
            stringValue: my-namespace-1
        - key: k8s.node.name
          value:
            stringValue: kind-worker
        - key: k8s.pod.name
          value:
            stringValue: test-k8scluster-receiver-statefulset-0
        - key: k8s.pod.uid
          value:
            stringValue: d3c0c3c9-8d6c-4df1-b37d-c34f58c8c3e0
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Current phase of the pod (1 - Pending, 2 - Running, 3 - Succeeded, 4 - Failed, 5 - Unknown)
            gauge:
              dataPoints:
                - asInt: "2"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.pod.phase
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.126.0-dev
  - resource:
      attributes:
        - key: k8s.namespace.name
          value:
            stringValue: my-namespace-2
        - key: k8s.node.name
          value:
            stringValue: kind-worker
        - key: k8s.pod.name
          value:
            stringValue: test-k8scluster-receiver-cronjob-29130119-7v7kc
        - key: k8s.pod.uid
          value:
            stringValue: 618936b7-6803-4909-8671-143c9e7aa5c9
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Current phase of the pod (1 - Pending, 2 - Running, 3 - Succeeded, 4 - Failed, 5 - Unknown)
            gauge:
              dataPoints:
                - asInt: "2"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.pod.phase
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.126.0-dev
  - resource:
      attributes:
        - key: k8s.namespace.name
          value:
            stringValue: my-namespace-2
        - key: k8s.node.name
          value:
            stringValue: kind-worker
        - key: k8s.pod.name
          value:
            stringValue: test-k8scluster-receiver-job-g4qh6
        - key: k8s.pod.uid
          value:
            stringValue: 25797a0a-6011-4c01-a9ed-b4fb8cb11080
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Current phase of the pod (1 - Pending, 2 - Running, 3 - Succeeded, 4 - Failed, 5 - Unknown)
            gauge:
              dataPoints:
                - asInt: "2"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.pod.phase
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.126.0-dev
  - resource:
      attributes:
        - key: k8s.namespace.name
          value:
            stringValue: my-namespace-2
        - key: k8s.node.name
          value:
            stringValue: kind-worker
        - key: k8s.pod.name
          value:
            stringValue: test-k8scluster-receiver-statefulset-0
        - key: k8s.pod.uid
          value:
            stringValue: 86b99b1b-08b9-4fb4-a100-c426110b81e7
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Current phase of the pod (1 - Pending, 2 - Running, 3 - Succeeded, 4 - Failed, 5 - Unknown)
            gauge:
              dataPoints:
                - asInt: "2"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.pod.phase
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.126.0-dev
  - resource:
      attributes:
        - key: container.id
          value:
            stringValue: 1e1ffc9a551a774a4eea739fcb78e400c59cb507ce695c760cabe78b7abb3fcf
        - key: container.image.name
          value:
            stringValue: localhost/otelcontribcol
        - key: container.image.tag
          value:
            stringValue: e2e-test
        - key: k8s.container.name
          value:
            stringValue: opentelemetry-collector
        - key: k8s.namespace.name
          value:
            stringValue: my-namespace-1
        - key: k8s.node.name
          value:
            stringValue: kind-worker
        - key: k8s.pod.name
          value:
            stringValue: otelcol-751183ea-7599f87469-wtw4j
        - key: k8s.pod.uid
          value:
            stringValue: a0c21a67-69e6-428c-96d8-464f30d3f1ca
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Maximum resource limit set for the container. See https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#resourcerequirements-v1-core for details
            gauge:
              dataPoints:
                - asDouble: 0.128
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.cpu_limit
            unit: '{cpu}'
          - description: Resource requested for the container. See https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#resourcerequirements-v1-core for details
            gauge:
              dataPoints:
                - asDouble: 0.128
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.cpu_request
            unit: '{cpu}'
          - description: Maximum resource limit set for the container. See https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#resourcerequirements-v1-core for details
            gauge:
              dataPoints:
                - asInt: "268435456"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.memory_limit
            unit: By
          - description: Resource requested for the container. See https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#resourcerequirements-v1-core for details
            gauge:
              dataPoints:
                - asInt: "268435456"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.memory_request
            unit: By
          - description: Whether a container has passed its readiness probe (0 for no, 1 for yes)
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.ready
          - description: How many times the container has restarted in the recent past. This value is pulled directly from the K8s API and the value can go indefinitely high and be reset to 0 at any time depending on how your kubelet is configured to prune dead containers. It is best to not depend too much on the exact value but rather look at it as either == 0, in which case you can conclude there were no restarts in the recent past, or > 0, in which case you can conclude there were restarts in the recent past, and not try and analyze the value beyond that.
            gauge:
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.restarts
            unit: '{restart}'
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.126.0-dev
  - resource:
      attributes:
        - key: container.id
          value:
            stringValue: 2948a10e5d4ed4d9027eaa90dd975d17e04b226af12da52545368ce1bbf5e593
        - key: container.image.name
          value:
            stringValue: docker.io/library/nginx
        - key: container.image.tag
          value:
            stringValue: latest
        - key: k8s.container.name
          value:
            stringValue: nginx
        - key: k8s.namespace.name
          value:
            stringValue: my-namespace-1
        - key: k8s.node.name
          value:
            stringValue: kind-worker
        - key: k8s.pod.name
          value:
            stringValue: test-k8scluster-receiver-statefulset-0
        - key: k8s.pod.uid
          value:
            stringValue: d3c0c3c9-8d6c-4df1-b37d-c34f58c8c3e0
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Whether a container has passed its readiness probe (0 for no, 1 for yes)
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.ready
          - description: How many times the container has restarted in the recent past. This value is pulled directly from the K8s API and the value can go indefinitely high and be reset to 0 at any time depending on how your kubelet is configured to prune dead containers. It is best to not depend too much on the exact value but rather look at it as either == 0, in which case you can conclude there were no restarts in the recent past, or > 0, in which case you can conclude there were restarts in the recent past, and not try and analyze the value beyond that.
            gauge:
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.restarts
            unit: '{restart}'
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.126.0-dev
  - resource:
      attributes:
        - key: container.id
          value:
            stringValue: 8958f2e8aa1c337073fad9e920c6c982c012cbeb4447248d41cc0fcca98ed1af
        - key: container.image.name
          value:
            stringValue: docker.io/library/alpine
        - key: container.image.tag
          value:
            stringValue: latest
        - key: k8s.container.name
          value:
            stringValue: alpine
        - key: k8s.namespace.name
          value:
            stringValue: my-namespace-1
        - key: k8s.node.name
          value:
            stringValue: kind-worker
        - key: k8s.pod.name
          value:
            stringValue: test-k8scluster-receiver-cronjob-29130119-db67n
        - key: k8s.pod.uid
          value:
            stringValue: 4cb2af42-2a1d-46f0-ae87-962783d1f99e
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Whether a container has passed its readiness probe (0 for no, 1 for yes)
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.ready
          - description: How many times the container has restarted in the recent past. This value is pulled directly from the K8s API and the value can go indefinitely high and be reset to 0 at any time depending on how your kubelet is configured to prune dead containers. It is best to not depend too much on the exact value but rather look at it as either == 0, in which case you can conclude there were no restarts in the recent past, or > 0, in which case you can conclude there were restarts in the recent past, and not try and analyze the value beyond that.
            gauge:
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.restarts
            unit: '{restart}'
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.126.0-dev
  - resource:
      attributes:
        - key: container.id
          value:
            stringValue: 9161514763fd096cf4792445b4afeca87464532ca0cbdfc8fdc60b6d186b01bd
        - key: container.image.name
          value:
            stringValue: docker.io/library/alpine
        - key: container.image.tag
          value:
            stringValue: latest
        - key: k8s.container.name
          value:
            stringValue: alpine
        - key: k8s.namespace.name
          value:
            stringValue: my-namespace-2
        - key: k8s.node.name
          value:
            stringValue: kind-worker
        - key: k8s.pod.name
          value:
            stringValue: test-k8scluster-receiver-job-g4qh6
        - key: k8s.pod.uid
          value:
            stringValue: 25797a0a-6011-4c01-a9ed-b4fb8cb11080
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Whether a container has passed its readiness probe (0 for no, 1 for yes)
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.ready
          - description: How many times the container has restarted in the recent past. This value is pulled directly from the K8s API and the value can go indefinitely high and be reset to 0 at any time depending on how your kubelet is configured to prune dead containers. It is best to not depend too much on the exact value but rather look at it as either == 0, in which case you can conclude there were no restarts in the recent past, or > 0, in which case you can conclude there were restarts in the recent past, and not try and analyze the value beyond that.
            gauge:
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.restarts
            unit: '{restart}'
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.126.0-dev
  - resource:
      attributes:
        - key: container.id
          value:
            stringValue: b8369bcfc37c06ab32979f57429724b525f6cedc24fc19b4ae01a5023e39c678
        - key: container.image.name
          value:
            stringValue: docker.io/library/alpine
        - key: container.image.tag
          value:
            stringValue: latest
        - key: k8s.container.name
          value:
            stringValue: alpine
        - key: k8s.namespace.name
          value:
            stringValue: my-namespace-2
        - key: k8s.node.name
          value:
            stringValue: kind-worker
        - key: k8s.pod.name
          value:
            stringValue: test-k8scluster-receiver-cronjob-29130119-7v7kc
        - key: k8s.pod.uid
          value:
            stringValue: 618936b7-6803-4909-8671-143c9e7aa5c9
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Whether a container has passed its readiness probe (0 for no, 1 for yes)
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.ready
          - description: How many times the container has restarted in the recent past. This value is pulled directly from the K8s API and the value can go indefinitely high and be reset to 0 at any time depending on how your kubelet is configured to prune dead containers. It is best to not depend too much on the exact value but rather look at it as either == 0, in which case you can conclude there were no restarts in the recent past, or > 0, in which case you can conclude there were restarts in the recent past, and not try and analyze the value beyond that.
            gauge:
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.restarts
            unit: '{restart}'
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.126.0-dev
  - resource:
      attributes:
        - key: container.id
          value:
            stringValue: d63b4bfa5df68fa8ea1e99c2bed10187f8d4d6fdcd4f6e4cd638ee9fdb75f703
        - key: container.image.name
          value:
            stringValue: docker.io/library/alpine
        - key: container.image.tag
          value:
            stringValue: latest
        - key: k8s.container.name
          value:
            stringValue: alpine
        - key: k8s.namespace.name
          value:
            stringValue: my-namespace-1
        - key: k8s.node.name
          value:
            stringValue: kind-worker
        - key: k8s.pod.name
          value:
            stringValue: test-k8scluster-receiver-job-j9lwl
        - key: k8s.pod.uid
          value:
            stringValue: 59da87d9-f320-497e-a088-c2e8c1e03d68
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Whether a container has passed its readiness probe (0 for no, 1 for yes)
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.ready
          - description: How many times the container has restarted in the recent past. This value is pulled directly from the K8s API and the value can go indefinitely high and be reset to 0 at any time depending on how your kubelet is configured to prune dead containers. It is best to not depend too much on the exact value but rather look at it as either == 0, in which case you can conclude there were no restarts in the recent past, or > 0, in which case you can conclude there were restarts in the recent past, and not try and analyze the value beyond that.
            gauge:
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.restarts
            unit: '{restart}'
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.126.0-dev
  - resource:
      attributes:
        - key: container.id
          value:
            stringValue: fd5aa330249d6b6e2736e0e1e084b26f7210fd30286a5d0b64749635b284e37b
        - key: container.image.name
          value:
            stringValue: docker.io/library/nginx
        - key: container.image.tag
          value:
            stringValue: latest
        - key: k8s.container.name
          value:
            stringValue: nginx
        - key: k8s.namespace.name
          value:
            stringValue: my-namespace-2
        - key: k8s.node.name
          value:
            stringValue: kind-worker
        - key: k8s.pod.name
          value:
            stringValue: test-k8scluster-receiver-statefulset-0
        - key: k8s.pod.uid
          value:
            stringValue: 86b99b1b-08b9-4fb4-a100-c426110b81e7
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Whether a container has passed its readiness probe (0 for no, 1 for yes)
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.ready
          - description: How many times the container has restarted in the recent past. This value is pulled directly from the K8s API and the value can go indefinitely high and be reset to 0 at any time depending on how your kubelet is configured to prune dead containers. It is best to not depend too much on the exact value but rather look at it as either == 0, in which case you can conclude there were no restarts in the recent past, or > 0, in which case you can conclude there were restarts in the recent past, and not try and analyze the value beyond that.
            gauge:
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.restarts
            unit: '{restart}'
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.126.0-dev
