resourceMetrics:
  - resource:
      attributes:
        - key: k8s.namespace.name
          value:
            stringValue: default
        - key: k8s.namespace.uid
          value:
            stringValue: 79f9ec63-8403-4dac-8459-a87d71e0dc5f
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: The current phase of namespaces (1 for active and 0 for terminating)
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.namespace.phase
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.144.0-dev
  - resource:
      attributes:
        - key: k8s.namespace.name
          value:
            stringValue: kube-node-lease
        - key: k8s.namespace.uid
          value:
            stringValue: 94a3157d-591a-4fc4-b91f-a3a7af7668a2
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: The current phase of namespaces (1 for active and 0 for terminating)
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.namespace.phase
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.144.0-dev
  - resource:
      attributes:
        - key: k8s.namespace.name
          value:
            stringValue: kube-public
        - key: k8s.namespace.uid
          value:
            stringValue: 789a9453-8d96-41f6-8eb7-f296a6cec8f9
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: The current phase of namespaces (1 for active and 0 for terminating)
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.namespace.phase
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.144.0-dev
  - resource:
      attributes:
        - key: k8s.namespace.name
          value:
            stringValue: kube-system
        - key: k8s.namespace.uid
          value:
            stringValue: 5a2fd18d-35e2-44bb-a5a6-182114f19e30
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: The current phase of namespaces (1 for active and 0 for terminating)
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.namespace.phase
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.144.0-dev
  - resource:
      attributes:
        - key: k8s.namespace.name
          value:
            stringValue: local-path-storage
        - key: k8s.namespace.uid
          value:
            stringValue: ba93ff9c-3566-4aa5-bb1f-0aa703331949
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: The current phase of namespaces (1 for active and 0 for terminating)
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.namespace.phase
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.144.0-dev
  - resource:
      attributes:
        - key: k8s.node.name
          value:
            stringValue: kind-control-plane
        - key: k8s.node.uid
          value:
            stringValue: 5a4b4eed-f9d2-4055-b9f2-96cfcd69ced1
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Ready condition status of the node (true=1, false=0, unknown=-1)
            gauge:
              dataPoints:
                - asInt: "1"
                  timeUnixNano: "1000000"
            name: k8s.node.condition_ready
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.144.0-dev
  - resource:
      attributes:
        - key: k8s.cronjob.name
          value:
            stringValue: test-k8scluster-receiver-cronjob
        - key: k8s.cronjob.uid
          value:
            stringValue: be6bd564-1243-4152-8b50-b56dd46990ba
        - key: k8s.namespace.name
          value:
            stringValue: default
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: The number of actively running jobs for a cronjob
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.cronjob.active_jobs
            unit: '{job}'
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.144.0-dev
  - resource:
      attributes:
        - key: k8s.daemonset.name
          value:
            stringValue: kindnet
        - key: k8s.daemonset.uid
          value:
            stringValue: 34ac7dab-3ed2-4b50-80c8-afcbe3c1c5db
        - key: k8s.namespace.name
          value:
            stringValue: kube-system
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Number of nodes that are running at least 1 daemon pod and are supposed to run the daemon pod
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.daemonset.current_scheduled_nodes
            unit: '{node}'
          - description: Number of nodes that should be running the daemon pod (including nodes currently running the daemon pod)
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.daemonset.desired_scheduled_nodes
            unit: '{node}'
          - description: Number of nodes that are running the daemon pod, but are not supposed to run the daemon pod
            gauge:
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.daemonset.misscheduled_nodes
            unit: '{node}'
          - description: Number of nodes that should be running the daemon pod and have one or more of the daemon pod running and ready
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.daemonset.ready_nodes
            unit: '{node}'
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.144.0-dev
  - resource:
      attributes:
        - key: k8s.daemonset.name
          value:
            stringValue: kube-proxy
        - key: k8s.daemonset.uid
          value:
            stringValue: 1061a4e8-db30-4cc7-a08c-ebf6108e5a3e
        - key: k8s.namespace.name
          value:
            stringValue: kube-system
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Number of nodes that are running at least 1 daemon pod and are supposed to run the daemon pod
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.daemonset.current_scheduled_nodes
            unit: '{node}'
          - description: Number of nodes that should be running the daemon pod (including nodes currently running the daemon pod)
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.daemonset.desired_scheduled_nodes
            unit: '{node}'
          - description: Number of nodes that are running the daemon pod, but are not supposed to run the daemon pod
            gauge:
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.daemonset.misscheduled_nodes
            unit: '{node}'
          - description: Number of nodes that should be running the daemon pod and have one or more of the daemon pod running and ready
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.daemonset.ready_nodes
            unit: '{node}'
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.144.0-dev
  - resource:
      attributes:
        - key: k8s.deployment.name
          value:
            stringValue: coredns
        - key: k8s.deployment.uid
          value:
            stringValue: 3074f637-308d-4d85-bf16-bff2fb07cf04
        - key: k8s.namespace.name
          value:
            stringValue: kube-system
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Total number of available pods (ready for at least minReadySeconds) targeted by this deployment
            gauge:
              dataPoints:
                - asInt: "2"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.deployment.available
            unit: '{pod}'
          - description: Number of desired pods in this deployment
            gauge:
              dataPoints:
                - asInt: "2"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.deployment.desired
            unit: '{pod}'
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.144.0-dev
  - resource:
      attributes:
        - key: k8s.deployment.name
          value:
            stringValue: local-path-provisioner
        - key: k8s.deployment.uid
          value:
            stringValue: d1b85797-1c4e-4629-8419-52de230d40ea
        - key: k8s.namespace.name
          value:
            stringValue: local-path-storage
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Total number of available pods (ready for at least minReadySeconds) targeted by this deployment
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.deployment.available
            unit: '{pod}'
          - description: Number of desired pods in this deployment
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.deployment.desired
            unit: '{pod}'
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.144.0-dev
  - resource:
      attributes:
        - key: k8s.deployment.name
          value:
            stringValue: otelcol-400d4a78
        - key: k8s.deployment.uid
          value:
            stringValue: ab594edc-4a74-4bcf-860a-47a816e1457d
        - key: k8s.namespace.name
          value:
            stringValue: default
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Total number of available pods (ready for at least minReadySeconds) targeted by this deployment
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.deployment.available
            unit: '{pod}'
          - description: Number of desired pods in this deployment
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.deployment.desired
            unit: '{pod}'
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.144.0-dev
  - resource:
      attributes:
        - key: k8s.hpa.name
          value:
            stringValue: test-k8scluster-receiver-hpa
        - key: k8s.hpa.uid
          value:
            stringValue: 89895484-3208-40ac-ba10-e69aa67c7d05
        - key: k8s.namespace.name
          value:
            stringValue: default
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Current number of pod replicas managed by this autoscaler.
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.hpa.current_replicas
            unit: '{pod}'
          - description: Desired number of pod replicas managed by this autoscaler.
            gauge:
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.hpa.desired_replicas
            unit: '{pod}'
          - description: Maximum number of replicas to which the autoscaler can scale up.
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.hpa.max_replicas
            unit: '{pod}'
          - description: Minimum number of replicas to which the autoscaler can scale up.
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.hpa.min_replicas
            unit: '{pod}'
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.144.0-dev
  - resource:
      attributes:
        - key: k8s.job.name
          value:
            stringValue: test-k8scluster-receiver-cronjob-29487109
        - key: k8s.job.uid
          value:
            stringValue: 70c23268-8376-4b75-a202-4aeb38c89da2
        - key: k8s.namespace.name
          value:
            stringValue: default
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: The number of actively running pods for a job
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.job.active_pods
            unit: '{pod}'
          - description: The desired number of successfully finished pods the job should be run with
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.job.desired_successful_pods
            unit: '{pod}'
          - description: The number of pods which reached phase Failed for a job
            gauge:
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.job.failed_pods
            unit: '{pod}'
          - description: The max desired number of pods the job should run at any given time
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.job.max_parallel_pods
            unit: '{pod}'
          - description: The number of pods which reached phase Succeeded for a job
            gauge:
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.job.successful_pods
            unit: '{pod}'
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.144.0-dev
  - resource:
      attributes:
        - key: k8s.job.name
          value:
            stringValue: test-k8scluster-receiver-job
        - key: k8s.job.uid
          value:
            stringValue: fc2ca525-357f-4d10-bc4d-6599c63e740f
        - key: k8s.namespace.name
          value:
            stringValue: default
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: The number of actively running pods for a job
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.job.active_pods
            unit: '{pod}'
          - description: The desired number of successfully finished pods the job should be run with
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.job.desired_successful_pods
            unit: '{pod}'
          - description: The number of pods which reached phase Failed for a job
            gauge:
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.job.failed_pods
            unit: '{pod}'
          - description: The max desired number of pods the job should run at any given time
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.job.max_parallel_pods
            unit: '{pod}'
          - description: The number of pods which reached phase Succeeded for a job
            gauge:
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.job.successful_pods
            unit: '{pod}'
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.144.0-dev
  - resource:
      attributes:
        - key: k8s.namespace.name
          value:
            stringValue: default
        - key: k8s.replicaset.name
          value:
            stringValue: otelcol-400d4a78-f6776f4cd
        - key: k8s.replicaset.uid
          value:
            stringValue: 52aaefd2-d4b6-4600-90b7-d1b84eefd671
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Total number of available pods (ready for at least minReadySeconds) targeted by this replicaset
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.replicaset.available
            unit: '{pod}'
          - description: Number of desired pods in this replicaset
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.replicaset.desired
            unit: '{pod}'
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.144.0-dev
  - resource:
      attributes:
        - key: k8s.namespace.name
          value:
            stringValue: default
        - key: k8s.statefulset.name
          value:
            stringValue: test-k8scluster-receiver-statefulset
        - key: k8s.statefulset.uid
          value:
            stringValue: 65b8c0ad-1a19-4690-b708-c4acfcdf6466
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: The number of pods created by the StatefulSet controller from the StatefulSet version
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.statefulset.current_pods
            unit: '{pod}'
          - description: Number of desired pods in the stateful set (the `spec.replicas` field)
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.statefulset.desired_pods
            unit: '{pod}'
          - description: Number of pods created by the stateful set that have the `Ready` condition
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.statefulset.ready_pods
            unit: '{pod}'
          - description: Number of pods created by the StatefulSet controller from the StatefulSet version
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.statefulset.updated_pods
            unit: '{pod}'
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.144.0-dev
  - resource:
      attributes:
        - key: k8s.namespace.name
          value:
            stringValue: kube-system
        - key: k8s.replicaset.name
          value:
            stringValue: coredns-674b8bbfcf
        - key: k8s.replicaset.uid
          value:
            stringValue: 16414230-48ea-4446-a9df-d770b212eb68
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Total number of available pods (ready for at least minReadySeconds) targeted by this replicaset
            gauge:
              dataPoints:
                - asInt: "2"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.replicaset.available
            unit: '{pod}'
          - description: Number of desired pods in this replicaset
            gauge:
              dataPoints:
                - asInt: "2"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.replicaset.desired
            unit: '{pod}'
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.144.0-dev
  - resource:
      attributes:
        - key: k8s.namespace.name
          value:
            stringValue: local-path-storage
        - key: k8s.replicaset.name
          value:
            stringValue: local-path-provisioner-7dc846544d
        - key: k8s.replicaset.uid
          value:
            stringValue: f0772f91-ca62-499b-90c4-2c94251fe1c7
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Total number of available pods (ready for at least minReadySeconds) targeted by this replicaset
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.replicaset.available
            unit: '{pod}'
          - description: Number of desired pods in this replicaset
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.replicaset.desired
            unit: '{pod}'
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.144.0-dev
  - resource:
      attributes:
        - key: k8s.namespace.name
          value:
            stringValue: default
        - key: k8s.node.name
          value:
            stringValue: kind-control-plane
        - key: k8s.pod.name
          value:
            stringValue: otelcol-400d4a78-f6776f4cd-zd6z8
        - key: k8s.pod.uid
          value:
            stringValue: 6787bb58-1d90-472c-b2a1-c6618e19177d
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Current phase of the pod (1 - Pending, 2 - Running, 3 - Succeeded, 4 - Failed, 5 - Unknown)
            gauge:
              dataPoints:
                - asInt: "2"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.pod.phase
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.144.0-dev
  - resource:
      attributes:
        - key: k8s.namespace.name
          value:
            stringValue: default
        - key: k8s.node.name
          value:
            stringValue: kind-control-plane
        - key: k8s.pod.name
          value:
            stringValue: test-k8scluster-receiver-cronjob-29487109-xcmw9
        - key: k8s.pod.uid
          value:
            stringValue: 16c02129-b9f6-40df-a64a-094126edc3ea
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Current phase of the pod (1 - Pending, 2 - Running, 3 - Succeeded, 4 - Failed, 5 - Unknown)
            gauge:
              dataPoints:
                - asInt: "2"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.pod.phase
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.144.0-dev
  - resource:
      attributes:
        - key: k8s.namespace.name
          value:
            stringValue: default
        - key: k8s.node.name
          value:
            stringValue: kind-control-plane
        - key: k8s.pod.name
          value:
            stringValue: test-k8scluster-receiver-job-knms4
        - key: k8s.pod.uid
          value:
            stringValue: 8df02f1d-f7bf-467c-b371-f87b7f603d22
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Current phase of the pod (1 - Pending, 2 - Running, 3 - Succeeded, 4 - Failed, 5 - Unknown)
            gauge:
              dataPoints:
                - asInt: "2"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.pod.phase
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.144.0-dev
  - resource:
      attributes:
        - key: k8s.namespace.name
          value:
            stringValue: default
        - key: k8s.node.name
          value:
            stringValue: kind-control-plane
        - key: k8s.pod.name
          value:
            stringValue: test-k8scluster-receiver-statefulset-0
        - key: k8s.pod.uid
          value:
            stringValue: 7f495e91-d116-4524-bde9-8ed320e01b45
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Current phase of the pod (1 - Pending, 2 - Running, 3 - Succeeded, 4 - Failed, 5 - Unknown)
            gauge:
              dataPoints:
                - asInt: "2"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.pod.phase
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.144.0-dev
  - resource:
      attributes:
        - key: k8s.namespace.name
          value:
            stringValue: kube-system
        - key: k8s.node.name
          value:
            stringValue: kind-control-plane
        - key: k8s.pod.name
          value:
            stringValue: coredns-674b8bbfcf-2mhk8
        - key: k8s.pod.uid
          value:
            stringValue: 5699a609-3dc1-442d-aa93-e2b54ab049f1
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Current phase of the pod (1 - Pending, 2 - Running, 3 - Succeeded, 4 - Failed, 5 - Unknown)
            gauge:
              dataPoints:
                - asInt: "2"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.pod.phase
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.144.0-dev
  - resource:
      attributes:
        - key: k8s.namespace.name
          value:
            stringValue: kube-system
        - key: k8s.node.name
          value:
            stringValue: kind-control-plane
        - key: k8s.pod.name
          value:
            stringValue: coredns-674b8bbfcf-tcqmj
        - key: k8s.pod.uid
          value:
            stringValue: 0607eacd-aac1-48ab-8aad-a5926fc2f699
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Current phase of the pod (1 - Pending, 2 - Running, 3 - Succeeded, 4 - Failed, 5 - Unknown)
            gauge:
              dataPoints:
                - asInt: "2"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.pod.phase
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.144.0-dev
  - resource:
      attributes:
        - key: k8s.namespace.name
          value:
            stringValue: kube-system
        - key: k8s.node.name
          value:
            stringValue: kind-control-plane
        - key: k8s.pod.name
          value:
            stringValue: etcd-kind-control-plane
        - key: k8s.pod.uid
          value:
            stringValue: cbafd89e-b3db-481f-8f68-25240b380516
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Current phase of the pod (1 - Pending, 2 - Running, 3 - Succeeded, 4 - Failed, 5 - Unknown)
            gauge:
              dataPoints:
                - asInt: "2"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.pod.phase
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.144.0-dev
  - resource:
      attributes:
        - key: k8s.namespace.name
          value:
            stringValue: kube-system
        - key: k8s.node.name
          value:
            stringValue: kind-control-plane
        - key: k8s.pod.name
          value:
            stringValue: kindnet-dfvm6
        - key: k8s.pod.uid
          value:
            stringValue: 7a3e3d02-7d66-44c6-835c-2ffa8b8759e5
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Current phase of the pod (1 - Pending, 2 - Running, 3 - Succeeded, 4 - Failed, 5 - Unknown)
            gauge:
              dataPoints:
                - asInt: "2"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.pod.phase
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.144.0-dev
  - resource:
      attributes:
        - key: k8s.namespace.name
          value:
            stringValue: kube-system
        - key: k8s.node.name
          value:
            stringValue: kind-control-plane
        - key: k8s.pod.name
          value:
            stringValue: kube-apiserver-kind-control-plane
        - key: k8s.pod.uid
          value:
            stringValue: 8855c932-7948-4f97-86bb-bf0c7d9cd068
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Current phase of the pod (1 - Pending, 2 - Running, 3 - Succeeded, 4 - Failed, 5 - Unknown)
            gauge:
              dataPoints:
                - asInt: "2"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.pod.phase
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.144.0-dev
  - resource:
      attributes:
        - key: k8s.namespace.name
          value:
            stringValue: kube-system
        - key: k8s.node.name
          value:
            stringValue: kind-control-plane
        - key: k8s.pod.name
          value:
            stringValue: kube-controller-manager-kind-control-plane
        - key: k8s.pod.uid
          value:
            stringValue: 426f48fb-3d34-4793-a827-1650cbd9303a
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Current phase of the pod (1 - Pending, 2 - Running, 3 - Succeeded, 4 - Failed, 5 - Unknown)
            gauge:
              dataPoints:
                - asInt: "2"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.pod.phase
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.144.0-dev
  - resource:
      attributes:
        - key: k8s.namespace.name
          value:
            stringValue: kube-system
        - key: k8s.node.name
          value:
            stringValue: kind-control-plane
        - key: k8s.pod.name
          value:
            stringValue: kube-proxy-8kc7d
        - key: k8s.pod.uid
          value:
            stringValue: edba691f-ff22-497d-9a8f-a1f284ad3da8
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Current phase of the pod (1 - Pending, 2 - Running, 3 - Succeeded, 4 - Failed, 5 - Unknown)
            gauge:
              dataPoints:
                - asInt: "2"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.pod.phase
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.144.0-dev
  - resource:
      attributes:
        - key: k8s.namespace.name
          value:
            stringValue: kube-system
        - key: k8s.node.name
          value:
            stringValue: kind-control-plane
        - key: k8s.pod.name
          value:
            stringValue: kube-scheduler-kind-control-plane
        - key: k8s.pod.uid
          value:
            stringValue: 6ce0c4ec-fd65-4a93-9bbf-803283ff7635
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Current phase of the pod (1 - Pending, 2 - Running, 3 - Succeeded, 4 - Failed, 5 - Unknown)
            gauge:
              dataPoints:
                - asInt: "2"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.pod.phase
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.144.0-dev
  - resource:
      attributes:
        - key: k8s.namespace.name
          value:
            stringValue: local-path-storage
        - key: k8s.node.name
          value:
            stringValue: kind-control-plane
        - key: k8s.pod.name
          value:
            stringValue: local-path-provisioner-7dc846544d-gxztl
        - key: k8s.pod.uid
          value:
            stringValue: 0191a447-a9a4-414c-96c2-fc906640e0b9
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Current phase of the pod (1 - Pending, 2 - Running, 3 - Succeeded, 4 - Failed, 5 - Unknown)
            gauge:
              dataPoints:
                - asInt: "2"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.pod.phase
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.144.0-dev
  - resource:
      attributes:
        - key: k8s.namespace.name
          value:
            stringValue: default
        - key: k8s.service.name
          value:
            stringValue: kubernetes
        - key: k8s.service.publish_not_ready_addresses
          value:
            boolValue: false
        - key: k8s.service.type
          value:
            stringValue: ClusterIP
        - key: k8s.service.uid
          value:
            stringValue: 29f80554-0383-436f-985c-733bb4c05e4d
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: The number of endpoints for a service, broken down by condition, address type, and zone.
            gauge:
              dataPoints:
                - asInt: "1"
                  attributes:
                    - key: k8s.service.endpoint.address_type
                      value:
                        stringValue: IPv4
                    - key: k8s.service.endpoint.condition
                      value:
                        stringValue: ready
                    - key: k8s.service.endpoint.zone
                      value:
                        stringValue: ""
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
                - asInt: "1"
                  attributes:
                    - key: k8s.service.endpoint.address_type
                      value:
                        stringValue: IPv4
                    - key: k8s.service.endpoint.condition
                      value:
                        stringValue: serving
                    - key: k8s.service.endpoint.zone
                      value:
                        stringValue: ""
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
                - asInt: "0"
                  attributes:
                    - key: k8s.service.endpoint.address_type
                      value:
                        stringValue: IPv4
                    - key: k8s.service.endpoint.condition
                      value:
                        stringValue: terminating
                    - key: k8s.service.endpoint.zone
                      value:
                        stringValue: ""
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.service.endpoint.count
            unit: '{endpoint}'
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.144.0-dev
  - resource:
      attributes:
        - key: k8s.namespace.name
          value:
            stringValue: default
        - key: k8s.service.name
          value:
            stringValue: otelcol-400d4a78
        - key: k8s.service.publish_not_ready_addresses
          value:
            boolValue: false
        - key: k8s.service.type
          value:
            stringValue: ClusterIP
        - key: k8s.service.uid
          value:
            stringValue: 790b315d-1ed0-4088-805c-20928cb3abc0
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: The number of endpoints for a service, broken down by condition, address type, and zone.
            gauge:
              dataPoints:
                - asInt: "1"
                  attributes:
                    - key: k8s.service.endpoint.address_type
                      value:
                        stringValue: IPv4
                    - key: k8s.service.endpoint.condition
                      value:
                        stringValue: ready
                    - key: k8s.service.endpoint.zone
                      value:
                        stringValue: ""
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
                - asInt: "1"
                  attributes:
                    - key: k8s.service.endpoint.address_type
                      value:
                        stringValue: IPv4
                    - key: k8s.service.endpoint.condition
                      value:
                        stringValue: serving
                    - key: k8s.service.endpoint.zone
                      value:
                        stringValue: ""
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
                - asInt: "0"
                  attributes:
                    - key: k8s.service.endpoint.address_type
                      value:
                        stringValue: IPv4
                    - key: k8s.service.endpoint.condition
                      value:
                        stringValue: terminating
                    - key: k8s.service.endpoint.zone
                      value:
                        stringValue: ""
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.service.endpoint.count
            unit: '{endpoint}'
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.144.0-dev
  - resource:
      attributes:
        - key: k8s.namespace.name
          value:
            stringValue: default
        - key: k8s.service.name
          value:
            stringValue: test-k8scluster-receiver-statefulset-service
        - key: k8s.service.publish_not_ready_addresses
          value:
            boolValue: true
        - key: k8s.service.type
          value:
            stringValue: ClusterIP
        - key: k8s.service.uid
          value:
            stringValue: aa43f0c2-4629-4ee3-9c85-d0c6934b6d5c
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: The number of endpoints for a service, broken down by condition, address type, and zone.
            gauge:
              dataPoints:
                - asInt: "1"
                  attributes:
                    - key: k8s.service.endpoint.address_type
                      value:
                        stringValue: IPv4
                    - key: k8s.service.endpoint.condition
                      value:
                        stringValue: ready
                    - key: k8s.service.endpoint.zone
                      value:
                        stringValue: ""
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
                - asInt: "1"
                  attributes:
                    - key: k8s.service.endpoint.address_type
                      value:
                        stringValue: IPv4
                    - key: k8s.service.endpoint.condition
                      value:
                        stringValue: serving
                    - key: k8s.service.endpoint.zone
                      value:
                        stringValue: ""
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
                - asInt: "0"
                  attributes:
                    - key: k8s.service.endpoint.address_type
                      value:
                        stringValue: IPv4
                    - key: k8s.service.endpoint.condition
                      value:
                        stringValue: terminating
                    - key: k8s.service.endpoint.zone
                      value:
                        stringValue: ""
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.service.endpoint.count
            unit: '{endpoint}'
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.144.0-dev
  - resource:
      attributes:
        - key: k8s.namespace.name
          value:
            stringValue: kube-system
        - key: k8s.service.name
          value:
            stringValue: kube-dns
        - key: k8s.service.publish_not_ready_addresses
          value:
            boolValue: false
        - key: k8s.service.type
          value:
            stringValue: ClusterIP
        - key: k8s.service.uid
          value:
            stringValue: 292b39da-f134-41cd-b05e-f7e1b1e3d40e
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: The number of endpoints for a service, broken down by condition, address type, and zone.
            gauge:
              dataPoints:
                - asInt: "2"
                  attributes:
                    - key: k8s.service.endpoint.address_type
                      value:
                        stringValue: IPv4
                    - key: k8s.service.endpoint.condition
                      value:
                        stringValue: ready
                    - key: k8s.service.endpoint.zone
                      value:
                        stringValue: ""
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
                - asInt: "2"
                  attributes:
                    - key: k8s.service.endpoint.address_type
                      value:
                        stringValue: IPv4
                    - key: k8s.service.endpoint.condition
                      value:
                        stringValue: serving
                    - key: k8s.service.endpoint.zone
                      value:
                        stringValue: ""
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
                - asInt: "0"
                  attributes:
                    - key: k8s.service.endpoint.address_type
                      value:
                        stringValue: IPv4
                    - key: k8s.service.endpoint.condition
                      value:
                        stringValue: terminating
                    - key: k8s.service.endpoint.zone
                      value:
                        stringValue: ""
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.service.endpoint.count
            unit: '{endpoint}'
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.144.0-dev
  - resource:
      attributes:
        - key: k8s.namespace.name
          value:
            stringValue: default
        - key: k8s.service.name
          value:
            stringValue: test-service-lb
        - key: k8s.service.publish_not_ready_addresses
          value:
            boolValue: false
        - key: k8s.service.traffic_distribution
          value:
            stringValue: PreferClose
        - key: k8s.service.type
          value:
            stringValue: LoadBalancer
        - key: k8s.service.uid
          value:
            stringValue: 392e3def-6fe4-4bc0-83f5-710d8d0c03b1
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: The number of load balancer ingress points (external IPs/hostnames) assigned to the service.
            gauge:
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.service.load_balancer.ingress.count
            unit: '{ingress}'
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.144.0-dev
  - resource:
      attributes:
        - key: container.id
          value:
            stringValue: 0cd5d8a37e1b776d576d2ecc009416dcd4d0912c10ae2990e6cf944913b55f34
        - key: container.image.name
          value:
            stringValue: docker.io/library/alpine
        - key: container.image.tag
          value:
            stringValue: latest
        - key: k8s.container.name
          value:
            stringValue: alpine
        - key: k8s.namespace.name
          value:
            stringValue: default
        - key: k8s.node.name
          value:
            stringValue: kind-control-plane
        - key: k8s.pod.name
          value:
            stringValue: test-k8scluster-receiver-job-knms4
        - key: k8s.pod.uid
          value:
            stringValue: 8df02f1d-f7bf-467c-b371-f87b7f603d22
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Whether a container has passed its readiness probe (0 for no, 1 for yes)
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.ready
          - description: How many times the container has restarted in the recent past. This value is pulled directly from the K8s API and the value can go indefinitely high and be reset to 0 at any time depending on how your kubelet is configured to prune dead containers. It is best to not depend too much on the exact value but rather look at it as either == 0, in which case you can conclude there were no restarts in the recent past, or > 0, in which case you can conclude there were restarts in the recent past, and not try and analyze the value beyond that.
            gauge:
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.restarts
            unit: '{restart}'
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.144.0-dev
  - resource:
      attributes:
        - key: container.id
          value:
            stringValue: 4ed763c02490dc1e9f28c12965a11a1bc9bfc01ca7ba5eb400371fced731f20e
        - key: container.image.name
          value:
            stringValue: docker.io/library/otelcontribcol
        - key: container.image.tag
          value:
            stringValue: latest
        - key: k8s.container.name
          value:
            stringValue: opentelemetry-collector
        - key: k8s.namespace.name
          value:
            stringValue: default
        - key: k8s.node.name
          value:
            stringValue: kind-control-plane
        - key: k8s.pod.name
          value:
            stringValue: otelcol-400d4a78-f6776f4cd-zd6z8
        - key: k8s.pod.uid
          value:
            stringValue: 6787bb58-1d90-472c-b2a1-c6618e19177d
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Maximum resource limit set for the container. See https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#resourcerequirements-v1-core for details
            gauge:
              dataPoints:
                - asDouble: 0.128
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.cpu_limit
            unit: '{cpu}'
          - description: Resource requested for the container. See https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#resourcerequirements-v1-core for details
            gauge:
              dataPoints:
                - asDouble: 0.128
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.cpu_request
            unit: '{cpu}'
          - description: Maximum resource limit set for the container. See https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#resourcerequirements-v1-core for details
            gauge:
              dataPoints:
                - asInt: "268435456"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.memory_limit
            unit: By
          - description: Resource requested for the container. See https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#resourcerequirements-v1-core for details
            gauge:
              dataPoints:
                - asInt: "268435456"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.memory_request
            unit: By
          - description: Whether a container has passed its readiness probe (0 for no, 1 for yes)
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.ready
          - description: How many times the container has restarted in the recent past. This value is pulled directly from the K8s API and the value can go indefinitely high and be reset to 0 at any time depending on how your kubelet is configured to prune dead containers. It is best to not depend too much on the exact value but rather look at it as either == 0, in which case you can conclude there were no restarts in the recent past, or > 0, in which case you can conclude there were restarts in the recent past, and not try and analyze the value beyond that.
            gauge:
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.restarts
            unit: '{restart}'
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.144.0-dev
  - resource:
      attributes:
        - key: container.id
          value:
            stringValue: 58981bc60064fb86d6c93ae396bfe9798e26d183b749b3241948fcaa3e98c6cb
        - key: container.image.name
          value:
            stringValue: docker.io/library/nginx
        - key: container.image.tag
          value:
            stringValue: latest
        - key: k8s.container.name
          value:
            stringValue: nginx
        - key: k8s.namespace.name
          value:
            stringValue: default
        - key: k8s.node.name
          value:
            stringValue: kind-control-plane
        - key: k8s.pod.name
          value:
            stringValue: test-k8scluster-receiver-statefulset-0
        - key: k8s.pod.uid
          value:
            stringValue: 7f495e91-d116-4524-bde9-8ed320e01b45
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Whether a container has passed its readiness probe (0 for no, 1 for yes)
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.ready
          - description: How many times the container has restarted in the recent past. This value is pulled directly from the K8s API and the value can go indefinitely high and be reset to 0 at any time depending on how your kubelet is configured to prune dead containers. It is best to not depend too much on the exact value but rather look at it as either == 0, in which case you can conclude there were no restarts in the recent past, or > 0, in which case you can conclude there were restarts in the recent past, and not try and analyze the value beyond that.
            gauge:
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.restarts
            unit: '{restart}'
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.144.0-dev
  - resource:
      attributes:
        - key: container.id
          value:
            stringValue: 5c3b6961a365e20da058e3289449f2e234460845a09dc99d105d89702fe602ae
        - key: container.image.name
          value:
            stringValue: registry.k8s.io/kube-apiserver-arm64
        - key: container.image.tag
          value:
            stringValue: v1.33.1
        - key: k8s.container.name
          value:
            stringValue: kube-apiserver
        - key: k8s.namespace.name
          value:
            stringValue: kube-system
        - key: k8s.node.name
          value:
            stringValue: kind-control-plane
        - key: k8s.pod.name
          value:
            stringValue: kube-apiserver-kind-control-plane
        - key: k8s.pod.uid
          value:
            stringValue: 8855c932-7948-4f97-86bb-bf0c7d9cd068
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Resource requested for the container. See https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#resourcerequirements-v1-core for details
            gauge:
              dataPoints:
                - asDouble: 0.25
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.cpu_request
            unit: '{cpu}'
          - description: Whether a container has passed its readiness probe (0 for no, 1 for yes)
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.ready
          - description: How many times the container has restarted in the recent past. This value is pulled directly from the K8s API and the value can go indefinitely high and be reset to 0 at any time depending on how your kubelet is configured to prune dead containers. It is best to not depend too much on the exact value but rather look at it as either == 0, in which case you can conclude there were no restarts in the recent past, or > 0, in which case you can conclude there were restarts in the recent past, and not try and analyze the value beyond that.
            gauge:
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.restarts
            unit: '{restart}'
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.144.0-dev
  - resource:
      attributes:
        - key: container.id
          value:
            stringValue: 6ca8ab850b3169a6767776b1a37489d08077e43f3ef686a9a6ffcfe8d15f37bd
        - key: container.image.name
          value:
            stringValue: registry.k8s.io/kube-scheduler-arm64
        - key: container.image.tag
          value:
            stringValue: v1.33.1
        - key: k8s.container.name
          value:
            stringValue: kube-scheduler
        - key: k8s.namespace.name
          value:
            stringValue: kube-system
        - key: k8s.node.name
          value:
            stringValue: kind-control-plane
        - key: k8s.pod.name
          value:
            stringValue: kube-scheduler-kind-control-plane
        - key: k8s.pod.uid
          value:
            stringValue: 6ce0c4ec-fd65-4a93-9bbf-803283ff7635
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Resource requested for the container. See https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#resourcerequirements-v1-core for details
            gauge:
              dataPoints:
                - asDouble: 0.1
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.cpu_request
            unit: '{cpu}'
          - description: Whether a container has passed its readiness probe (0 for no, 1 for yes)
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.ready
          - description: How many times the container has restarted in the recent past. This value is pulled directly from the K8s API and the value can go indefinitely high and be reset to 0 at any time depending on how your kubelet is configured to prune dead containers. It is best to not depend too much on the exact value but rather look at it as either == 0, in which case you can conclude there were no restarts in the recent past, or > 0, in which case you can conclude there were restarts in the recent past, and not try and analyze the value beyond that.
            gauge:
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.restarts
            unit: '{restart}'
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.144.0-dev
  - resource:
      attributes:
        - key: container.id
          value:
            stringValue: 72bfd6b1016b5f2690a71c362e10af3594447de2d77c18050ad3873300fd109e
        - key: container.image.name
          value:
            stringValue: registry.k8s.io/coredns/coredns
        - key: container.image.tag
          value:
            stringValue: v1.12.0
        - key: k8s.container.name
          value:
            stringValue: coredns
        - key: k8s.namespace.name
          value:
            stringValue: kube-system
        - key: k8s.node.name
          value:
            stringValue: kind-control-plane
        - key: k8s.pod.name
          value:
            stringValue: coredns-674b8bbfcf-2mhk8
        - key: k8s.pod.uid
          value:
            stringValue: 5699a609-3dc1-442d-aa93-e2b54ab049f1
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Resource requested for the container. See https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#resourcerequirements-v1-core for details
            gauge:
              dataPoints:
                - asDouble: 0.1
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.cpu_request
            unit: '{cpu}'
          - description: Maximum resource limit set for the container. See https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#resourcerequirements-v1-core for details
            gauge:
              dataPoints:
                - asInt: "178257920"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.memory_limit
            unit: By
          - description: Resource requested for the container. See https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#resourcerequirements-v1-core for details
            gauge:
              dataPoints:
                - asInt: "73400320"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.memory_request
            unit: By
          - description: Whether a container has passed its readiness probe (0 for no, 1 for yes)
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.ready
          - description: How many times the container has restarted in the recent past. This value is pulled directly from the K8s API and the value can go indefinitely high and be reset to 0 at any time depending on how your kubelet is configured to prune dead containers. It is best to not depend too much on the exact value but rather look at it as either == 0, in which case you can conclude there were no restarts in the recent past, or > 0, in which case you can conclude there were restarts in the recent past, and not try and analyze the value beyond that.
            gauge:
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.restarts
            unit: '{restart}'
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.144.0-dev
  - resource:
      attributes:
        - key: container.id
          value:
            stringValue: 949271f523284ca538bf70a493b722d6a7565dd1baf1b1ab106eb292de056f9b
        - key: container.image.name
          value:
            stringValue: registry.k8s.io/kube-proxy-arm64
        - key: container.image.tag
          value:
            stringValue: v1.33.1
        - key: k8s.container.name
          value:
            stringValue: kube-proxy
        - key: k8s.namespace.name
          value:
            stringValue: kube-system
        - key: k8s.node.name
          value:
            stringValue: kind-control-plane
        - key: k8s.pod.name
          value:
            stringValue: kube-proxy-8kc7d
        - key: k8s.pod.uid
          value:
            stringValue: edba691f-ff22-497d-9a8f-a1f284ad3da8
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Whether a container has passed its readiness probe (0 for no, 1 for yes)
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.ready
          - description: How many times the container has restarted in the recent past. This value is pulled directly from the K8s API and the value can go indefinitely high and be reset to 0 at any time depending on how your kubelet is configured to prune dead containers. It is best to not depend too much on the exact value but rather look at it as either == 0, in which case you can conclude there were no restarts in the recent past, or > 0, in which case you can conclude there were restarts in the recent past, and not try and analyze the value beyond that.
            gauge:
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.restarts
            unit: '{restart}'
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.144.0-dev
  - resource:
      attributes:
        - key: container.id
          value:
            stringValue: 9de9094bfaf8a6acf200c01461921a92ba8d7b99b279d7f550e77b64dbb9f34d
        - key: container.image.name
          value:
            stringValue: registry.k8s.io/kube-controller-manager-arm64
        - key: container.image.tag
          value:
            stringValue: v1.33.1
        - key: k8s.container.name
          value:
            stringValue: kube-controller-manager
        - key: k8s.namespace.name
          value:
            stringValue: kube-system
        - key: k8s.node.name
          value:
            stringValue: kind-control-plane
        - key: k8s.pod.name
          value:
            stringValue: kube-controller-manager-kind-control-plane
        - key: k8s.pod.uid
          value:
            stringValue: 426f48fb-3d34-4793-a827-1650cbd9303a
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Resource requested for the container. See https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#resourcerequirements-v1-core for details
            gauge:
              dataPoints:
                - asDouble: 0.2
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.cpu_request
            unit: '{cpu}'
          - description: Whether a container has passed its readiness probe (0 for no, 1 for yes)
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.ready
          - description: How many times the container has restarted in the recent past. This value is pulled directly from the K8s API and the value can go indefinitely high and be reset to 0 at any time depending on how your kubelet is configured to prune dead containers. It is best to not depend too much on the exact value but rather look at it as either == 0, in which case you can conclude there were no restarts in the recent past, or > 0, in which case you can conclude there were restarts in the recent past, and not try and analyze the value beyond that.
            gauge:
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.restarts
            unit: '{restart}'
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.144.0-dev
  - resource:
      attributes:
        - key: container.id
          value:
            stringValue: aa9b37682b68ed7653f20a36c128c772d3279359d6645f0e4db74343ec9bf4cf
        - key: container.image.name
          value:
            stringValue: docker.io/kindest/kindnetd
        - key: container.image.tag
          value:
            stringValue: v20250512-df8de77b
        - key: k8s.container.name
          value:
            stringValue: kindnet-cni
        - key: k8s.namespace.name
          value:
            stringValue: kube-system
        - key: k8s.node.name
          value:
            stringValue: kind-control-plane
        - key: k8s.pod.name
          value:
            stringValue: kindnet-dfvm6
        - key: k8s.pod.uid
          value:
            stringValue: 7a3e3d02-7d66-44c6-835c-2ffa8b8759e5
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Maximum resource limit set for the container. See https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#resourcerequirements-v1-core for details
            gauge:
              dataPoints:
                - asDouble: 0.1
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.cpu_limit
            unit: '{cpu}'
          - description: Resource requested for the container. See https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#resourcerequirements-v1-core for details
            gauge:
              dataPoints:
                - asDouble: 0.1
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.cpu_request
            unit: '{cpu}'
          - description: Maximum resource limit set for the container. See https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#resourcerequirements-v1-core for details
            gauge:
              dataPoints:
                - asInt: "52428800"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.memory_limit
            unit: By
          - description: Resource requested for the container. See https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#resourcerequirements-v1-core for details
            gauge:
              dataPoints:
                - asInt: "52428800"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.memory_request
            unit: By
          - description: Whether a container has passed its readiness probe (0 for no, 1 for yes)
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.ready
          - description: How many times the container has restarted in the recent past. This value is pulled directly from the K8s API and the value can go indefinitely high and be reset to 0 at any time depending on how your kubelet is configured to prune dead containers. It is best to not depend too much on the exact value but rather look at it as either == 0, in which case you can conclude there were no restarts in the recent past, or > 0, in which case you can conclude there were restarts in the recent past, and not try and analyze the value beyond that.
            gauge:
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.restarts
            unit: '{restart}'
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.144.0-dev
  - resource:
      attributes:
        - key: container.id
          value:
            stringValue: ac267c475d3e67742db1f2847c865dc3c7a6ee3872ed251eddb4189a1327ef91
        - key: container.image.name
          value:
            stringValue: docker.io/kindest/local-path-provisioner
        - key: container.image.tag
          value:
            stringValue: v20250214-acbabc1a
        - key: k8s.container.name
          value:
            stringValue: local-path-provisioner
        - key: k8s.namespace.name
          value:
            stringValue: local-path-storage
        - key: k8s.node.name
          value:
            stringValue: kind-control-plane
        - key: k8s.pod.name
          value:
            stringValue: local-path-provisioner-7dc846544d-gxztl
        - key: k8s.pod.uid
          value:
            stringValue: 0191a447-a9a4-414c-96c2-fc906640e0b9
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Whether a container has passed its readiness probe (0 for no, 1 for yes)
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.ready
          - description: How many times the container has restarted in the recent past. This value is pulled directly from the K8s API and the value can go indefinitely high and be reset to 0 at any time depending on how your kubelet is configured to prune dead containers. It is best to not depend too much on the exact value but rather look at it as either == 0, in which case you can conclude there were no restarts in the recent past, or > 0, in which case you can conclude there were restarts in the recent past, and not try and analyze the value beyond that.
            gauge:
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.restarts
            unit: '{restart}'
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.144.0-dev
  - resource:
      attributes:
        - key: container.id
          value:
            stringValue: b43b5bfc9474d1ea1d421221b9c74bf0fbc315aad84156ac4633776a001844c9
        - key: container.image.name
          value:
            stringValue: registry.k8s.io/coredns/coredns
        - key: container.image.tag
          value:
            stringValue: v1.12.0
        - key: k8s.container.name
          value:
            stringValue: coredns
        - key: k8s.namespace.name
          value:
            stringValue: kube-system
        - key: k8s.node.name
          value:
            stringValue: kind-control-plane
        - key: k8s.pod.name
          value:
            stringValue: coredns-674b8bbfcf-tcqmj
        - key: k8s.pod.uid
          value:
            stringValue: 0607eacd-aac1-48ab-8aad-a5926fc2f699
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Resource requested for the container. See https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#resourcerequirements-v1-core for details
            gauge:
              dataPoints:
                - asDouble: 0.1
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.cpu_request
            unit: '{cpu}'
          - description: Maximum resource limit set for the container. See https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#resourcerequirements-v1-core for details
            gauge:
              dataPoints:
                - asInt: "178257920"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.memory_limit
            unit: By
          - description: Resource requested for the container. See https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#resourcerequirements-v1-core for details
            gauge:
              dataPoints:
                - asInt: "73400320"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.memory_request
            unit: By
          - description: Whether a container has passed its readiness probe (0 for no, 1 for yes)
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.ready
          - description: How many times the container has restarted in the recent past. This value is pulled directly from the K8s API and the value can go indefinitely high and be reset to 0 at any time depending on how your kubelet is configured to prune dead containers. It is best to not depend too much on the exact value but rather look at it as either == 0, in which case you can conclude there were no restarts in the recent past, or > 0, in which case you can conclude there were restarts in the recent past, and not try and analyze the value beyond that.
            gauge:
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.restarts
            unit: '{restart}'
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.144.0-dev
  - resource:
      attributes:
        - key: container.id
          value:
            stringValue: d54dbfc46a2a81caabc75eb9022ff041d4ae1eaf4609fecb90b2fabe880c6a7a
        - key: container.image.name
          value:
            stringValue: docker.io/library/alpine
        - key: container.image.tag
          value:
            stringValue: latest
        - key: k8s.container.name
          value:
            stringValue: alpine
        - key: k8s.namespace.name
          value:
            stringValue: default
        - key: k8s.node.name
          value:
            stringValue: kind-control-plane
        - key: k8s.pod.name
          value:
            stringValue: test-k8scluster-receiver-cronjob-29487109-xcmw9
        - key: k8s.pod.uid
          value:
            stringValue: 16c02129-b9f6-40df-a64a-094126edc3ea
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Whether a container has passed its readiness probe (0 for no, 1 for yes)
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.ready
          - description: How many times the container has restarted in the recent past. This value is pulled directly from the K8s API and the value can go indefinitely high and be reset to 0 at any time depending on how your kubelet is configured to prune dead containers. It is best to not depend too much on the exact value but rather look at it as either == 0, in which case you can conclude there were no restarts in the recent past, or > 0, in which case you can conclude there were restarts in the recent past, and not try and analyze the value beyond that.
            gauge:
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.restarts
            unit: '{restart}'
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.144.0-dev
  - resource:
      attributes:
        - key: container.id
          value:
            stringValue: dc062d964f1e697b8e5547eba018506f9fe5b0de5276a3869cce3a38df3a6615
        - key: container.image.name
          value:
            stringValue: registry.k8s.io/etcd
        - key: container.image.tag
          value:
            stringValue: 3.5.21-0
        - key: k8s.container.name
          value:
            stringValue: etcd
        - key: k8s.namespace.name
          value:
            stringValue: kube-system
        - key: k8s.node.name
          value:
            stringValue: kind-control-plane
        - key: k8s.pod.name
          value:
            stringValue: etcd-kind-control-plane
        - key: k8s.pod.uid
          value:
            stringValue: cbafd89e-b3db-481f-8f68-25240b380516
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Resource requested for the container. See https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#resourcerequirements-v1-core for details
            gauge:
              dataPoints:
                - asDouble: 0.1
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.cpu_request
            unit: '{cpu}'
          - description: Resource requested for the container. See https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#resourcerequirements-v1-core for details
            gauge:
              dataPoints:
                - asInt: "104857600"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.memory_request
            unit: By
          - description: Whether a container has passed its readiness probe (0 for no, 1 for yes)
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.ready
          - description: How many times the container has restarted in the recent past. This value is pulled directly from the K8s API and the value can go indefinitely high and be reset to 0 at any time depending on how your kubelet is configured to prune dead containers. It is best to not depend too much on the exact value but rather look at it as either == 0, in which case you can conclude there were no restarts in the recent past, or > 0, in which case you can conclude there were restarts in the recent past, and not try and analyze the value beyond that.
            gauge:
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.restarts
            unit: '{restart}'
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.144.0-dev
