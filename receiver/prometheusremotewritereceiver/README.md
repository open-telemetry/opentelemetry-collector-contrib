# Prometheus Remote Write Receiver

<!-- status autogenerated section -->
| Status        |           |
| ------------- |-----------|
| Stability     | [development]: metrics   |
| Distributions | [] |
| Issues        | [![Open issues](https://img.shields.io/github/issues-search/open-telemetry/opentelemetry-collector-contrib?query=is%3Aissue%20is%3Aopen%20label%3Areceiver%2Fprometheusremotewrite%20&label=open&color=orange&logo=opentelemetry)](https://github.com/open-telemetry/opentelemetry-collector-contrib/issues?q=is%3Aopen+is%3Aissue+label%3Areceiver%2Fprometheusremotewrite) [![Closed issues](https://img.shields.io/github/issues-search/open-telemetry/opentelemetry-collector-contrib?query=is%3Aissue%20is%3Aclosed%20label%3Areceiver%2Fprometheusremotewrite%20&label=closed&color=blue&logo=opentelemetry)](https://github.com/open-telemetry/opentelemetry-collector-contrib/issues?q=is%3Aclosed+is%3Aissue+label%3Areceiver%2Fprometheusremotewrite) |
| Code coverage | [![codecov](https://codecov.io/github/open-telemetry/opentelemetry-collector-contrib/graph/main/badge.svg?component=receiver_prometheusremotewrite)](https://app.codecov.io/gh/open-telemetry/opentelemetry-collector-contrib/tree/main/?components%5B0%5D=receiver_prometheusremotewrite&displayType=list) |
| [Code Owners](https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/CONTRIBUTING.md#becoming-a-code-owner)    | [@dashpole](https://www.github.com/dashpole), [@ArthurSens](https://www.github.com/ArthurSens), [@perebaj](https://www.github.com/perebaj) |

[development]: https://github.com/open-telemetry/opentelemetry-collector/blob/main/docs/component-stability.md#development
<!-- end autogenerated section -->

## Why Not support Prometheus Remote Write v1?

This component focuses exclusively on [Prometheus Remote Write v2 Protocol](https://prometheus.io/docs/specs/prw/remote_write_spec_2_0/).

We don't support Prometheus Remote Write v1 for a couple of reasons, which are explained below.

### Histogram Atomicity

Prometheus Remote Write v1 was developed before Prometheus Native Histograms were a thing. The original Histogram format, commonly known as Prometheus Classic Histograms, is composed of several separate time series that together work as a whole histogram. Each time series holds a single information, whether the bucket boundaries, the sum of all observations, or the count of observations.

Now, being more specific to Prometheus Remote Write implementation. It was developed using the algorithm [EWMA (Exponential Weighted Moving Average)](https://corporatefinanceinstitute.com/resources/career-map/sell-side/capital-markets/exponentially-weighted-moving-average-ewma/), used to control throughput. Simply put, the more time-series Prometheus is ingesting, the more workers Prometheus spins up to push metrics via Remote Write, and a decrease in worker count also happens when ingestion decreases.

Since Classic Histograms are made of multiple time series, there is a high chance that parts of them are sent to the remote storage in separate remote-write requests. If, for any reason, one of those requests fails, it is impossible for the receiver to know if the time series it received was already enough to assemble and generate a complete histogram.

![Histogram Lack of Atomicity](assets/histogram-lack-atomicity.png)

This problem was solved in Prometheus Remote Write v2 with the introduction of [Native Histograms](https://prometheus.io/docs/specs/native_histograms/).

### Decoupled Metadata

While, officially, Prometheus Remote Write v1 does NOT support sending metadata, e.g., Metric Type, Unit, and Help description. It was developed versions of the protocol where metadata can be sent separately from the metric.

Similarly to the problem mentioned in [Histogram Atomicity](#histogram-atomicity), sending this kind of information separately can cause issues if the data is lost during transport, not to mention the necessity of caching metrics or metric metadata while we wait for the subsequent request that will connect the two.

In Prometheus Remote Write v2, this problem is solved since the time series are sent together with their metadata.

### Lack of Created Timestamp

`Created Timestamp` is a feature in Prometheus that works similarly and is translated to OTel's `StartTimeUnixNano`. Prometheus Remote Write v1 doesn't send Created Timestamps, so we can never populate the StartTimeUnixNano field from that protocol.

## Resource Metrics Cache

`target_info` metrics and "normal" metrics are a match when they have the same job/instance labels (Please read the [specification](https://opentelemetry.io/docs/specs/otel/compatibility/prometheus_and_openmetrics/#resource-attributes-1) for more details). But these metrics do not always come in the same Remote-Write request. For this reason, the receiver uses an internal LRU (Least Recently Used) and stateless cache implementation to store resource metrics across requests.

The cleanup is based on a maximum size of 1000 resource metrics, and it will be cleaned when the limit is reached, cleaning the less recently used resource metrics.

This approach has some limitations, for example:
    - If the process dies or restarts, the cache will be lost.
    - Some inconsistencies can happen according to the order of the requests and the current cache size.
    - The limit of 1000 resource metrics is hardcoded and not configurable for now.
