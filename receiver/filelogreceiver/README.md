# File Log Receiver

<!-- status autogenerated section -->
| Status        |           |
| ------------- |-----------|
| Stability     | [beta]: logs   |
| Distributions | [contrib], [k8s] |
| Issues        | [![Open issues](https://img.shields.io/github/issues-search/open-telemetry/opentelemetry-collector-contrib?query=is%3Aissue%20is%3Aopen%20label%3Areceiver%2Ffilelog%20&label=open&color=orange&logo=opentelemetry)](https://github.com/open-telemetry/opentelemetry-collector-contrib/issues?q=is%3Aopen+is%3Aissue+label%3Areceiver%2Ffilelog) [![Closed issues](https://img.shields.io/github/issues-search/open-telemetry/opentelemetry-collector-contrib?query=is%3Aissue%20is%3Aclosed%20label%3Areceiver%2Ffilelog%20&label=closed&color=blue&logo=opentelemetry)](https://github.com/open-telemetry/opentelemetry-collector-contrib/issues?q=is%3Aclosed+is%3Aissue+label%3Areceiver%2Ffilelog) |
| Code coverage | [![codecov](https://codecov.io/github/open-telemetry/opentelemetry-collector-contrib/graph/main/badge.svg?component=receiver_filelog)](https://app.codecov.io/gh/open-telemetry/opentelemetry-collector-contrib/tree/main/?components%5B0%5D=receiver_filelog&displayType=list) |
| [Code Owners](https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/CONTRIBUTING.md#becoming-a-code-owner)    | [@andrzej-stencel](https://www.github.com/andrzej-stencel) \| Seeking more code owners! |
| Emeritus      | [@djaglowski](https://www.github.com/djaglowski) |

[beta]: https://github.com/open-telemetry/opentelemetry-collector/blob/main/docs/component-stability.md#beta
[contrib]: https://github.com/open-telemetry/opentelemetry-collector-releases/tree/main/distributions/otelcol-contrib
[k8s]: https://github.com/open-telemetry/opentelemetry-collector-releases/tree/main/distributions/otelcol-k8s
<!-- end autogenerated section -->

Tails and parses logs from files.

## Overview

The File Log Receiver tails and parses logs from files using the [Stanza](https://github.com/open-telemetry/opentelemetry-log-collection) log agent. It is designed to efficiently monitor log files on local filesystems, supporting features like:

- Tailing files with automatic rotation detection
- Parsing log lines using various operators (regex, JSON, etc.)
- Multiline log handling
- Persistent offset tracking for fault tolerance
- File fingerprinting to handle rotation and deduplication
- Compression support (gzip)
- Batch processing of multiple files

## Configuration

| Field                                 | Default                              | Description                                                                                                                                                                                                                                                     |
|---------------------------------------|--------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| `include`                             | required                             | A list of file glob patterns that match the file paths to be read.                                                                                                                                                                                              |
| `exclude`                             | []                                   | A list of file glob patterns to exclude from reading. This is applied against the paths matched by `include`.                                                                                                                                                   |
| `exclude_older_than`                  |                                      | Exclude files whose modification time is older than the specified [age](#time-parameters).                                                                                                                                                                      |
| `start_at`                            | `end`                                | At startup, where to start reading logs from the file. Options are `beginning` or `end`.                                                                                                                                                                        |
| `multiline`                           |                                      | A `multiline` configuration block. See [below](#multiline-configuration) for more details.                                                                                                                                                                      |
| `force_flush_period`                  | `500ms`                              | [Time](#time-parameters) since last time new data was found in the file, after which a partial log at the end of the file may be emitted.                                                                                                                       |
| `encoding`                            | `utf-8`                              | The encoding of the file being read. See the list of [supported encodings below](#supported-encodings) for available options.                                                                                                                                   |
| `preserve_leading_whitespaces`        | `false`                              | Whether to preserve leading whitespaces.                                                                                                                                                                                                                        |
| `preserve_trailing_whitespaces`       | `false`                              | Whether to preserve trailing whitespaces.                                                                                                                                                                                                                       |
| `include_file_name`                   | `true`                               | Whether to add the file name as the attribute `log.file.name`.                                                                                                                                                                                                  |
| `include_file_path`                   | `false`                              | Whether to add the file path as the attribute `log.file.path`.                                                                                                                                                                                                  |
| `include_file_name_resolved`          | `false`                              | Whether to add the file name after symlinks resolution as the attribute `log.file.name_resolved`.                                                                                                                                                               |
| `include_file_path_resolved`          | `false`                              | Whether to add the file path after symlinks resolution as the attribute `log.file.path_resolved`.                                                                                                                                                               |
| `include_file_owner_name`             | `false`                              | Whether to add the file owner name as the attribute `log.file.owner.name`. Not supported for windows.                                                                                                                                                           |
| `include_file_owner_group_name`       | `false`                              | Whether to add the file group name as the attribute `log.file.owner.group.name`. Not supported for windows.                                                                                                                                                     |
| `include_file_record_number`          | `false`                              | Whether to add the record number in the file as the attribute `log.file.record_number`.                                                                                                                                                                         |
| `include_file_record_offset`          | `false`                              | Whether to add the record offset in the file as the attribute `log.file.record_offset`                                                                                                                                                                          |
| `poll_interval`                       | 200ms                                | The [duration](#time-parameters) between filesystem polls.                                                                                                                                                                                                      |
| `fingerprint_size`                    | `1kb`                                | The number of bytes with which to identify a file. The first bytes in the file are used as the fingerprint. Decreasing this value at any point will cause existing fingerprints to forgotten, meaning that all files will be read from the beginning (one time) |
| `initial_buffer_size`                 | `16KiB`                              | The initial size of the to read buffer for headers and logs, the buffer will be grown as necessary. Larger values may lead to unnecessary large buffer allocations, and smaller values may lead to lots of copies while growing the buffer.                     |
| `max_log_size`                        | `1MiB`                               | The maximum size of a log entry to read. A log entry will be truncated if it is larger than `max_log_size`. Protects against reading large amounts of data into memory.                                                                                         |
| `max_concurrent_files`                | 1024                                 | The maximum number of log files from which logs will be read concurrently. If the number of files matched in the `include` pattern exceeds this number, then files will be processed in batches.                                                                |
| `max_batches`                         | 0                                    | Only applicable when files must be batched in order to respect `max_concurrent_files`. This value limits the number of batches that will be processed during a single poll interval. A value of 0 indicates no limit.                                           |
| `delete_after_read`                   | `false`                              | If `true`, each log file will be read and then immediately deleted. Requires that the `filelog.allowFileDeletion` feature gate is enabled. Must be `false` when `start_at` is set to `end`.                                                                     |
| `acquire_fs_lock`                     | `false`                              | Whether to attempt to acquire a filesystem lock before reading a file (Unix only).                                                                                                                                                                              |
| `attributes`                          | {}                                   | A map of `key: value` pairs to add to the entry's attributes.                                                                                                                                                                                                   |
| `resource`                            | {}                                   | A map of `key: value` pairs to add to the entry's resource.                                                                                                                                                                                                     |
| `operators`                           | []                                   | An array of [operators](../../pkg/stanza/docs/operators/README.md#what-operators-are-available). See below for more details.                                                                                                                                    |
| `storage`                             | none                                 | The ID of a storage extension to be used to store file offsets. File offsets allow the receiver to pick up where it left off in the case of a collector restart. If no storage extension is used, the receiver will manage offsets in memory only.              |
| `header`                              | nil                                  | Specifies options for parsing header metadata. Requires that the `filelog.allowHeaderMetadataParsing` feature gate is enabled. See below for details. Must not be set when `start_at` is set to `end`.                                                          |
| `header.pattern`                      | required for header metadata parsing | A regex that matches every header line.                                                                                                                                                                                                                         |
| `header.metadata_operators`           | required for header metadata parsing | A list of operators used to parse metadata from the header.                                                                                                                                                                                                     |
| `retry_on_failure.enabled`            | `false`                              | If `true`, the receiver will pause reading a file and attempt to resend the current batch of logs if it encounters an error from downstream components.                                                                                                         |
| `retry_on_failure.initial_interval`   | `1s`                                 | [Time](#time-parameters) to wait after the first failure before retrying.                                                                                                                                                                                       |
| `retry_on_failure.max_interval`       | `30s`                                | Upper bound on retry backoff [interval](#time-parameters). Once this value is reached the delay between consecutive retries will remain constant at the specified value.                                                                                        |
| `retry_on_failure.max_elapsed_time`   | `5m`                                 | Maximum amount of [time](#time-parameters) (including retries) spent trying to send a logs batch to a downstream consumer. Once this value is reached, the data is discarded. Retrying never stops if set to `0`.                                               |
| `ordering_criteria.regex`             |                                      | Regular expression used for sorting, should contain a named capture groups that are to be used in `regex_key`.                                                                                                                                                  |
| `ordering_criteria.group_by`          |                                      | Regular expression used for grouping, which is done pre-sorting. Should contain a named capture groups.                                                                                                                                                         |
| `ordering_criteria.top_n`             | 1                                    | The number of files to track when using file ordering. The top N files are tracked after applying the ordering criteria.                                                                                                                                        |
| `ordering_criteria.sort_by.regex_key` |                                      | Regular expression named capture group defined in `ordering_criteria.regex` to use for sorting.                                                                                                                                                                         |
| `ordering_criteria.sort_by.sort_type` |                                      | Type of sorting to be performed (e.g., `numeric`, `alphabetical`, `timestamp`, `mtime`)                                                                                                                                                                         |
| `ordering_criteria.sort_by.location`  |                                      | Relevant if `sort_type` is set to `timestamp`. Defines the location of the timestamp of the file.                                                                                                                                                               |
| `ordering_criteria.sort_by.format`    |                                      | Relevant if `sort_type` is set to `timestamp`. Defines the strptime format of the timestamp being sorted.                                                                                                                                                       |
| `ordering_criteria.sort_by.ascending` |                                      | Sort direction                                                                                                                                                                                                                                                  |
| `compression`                         |                                      | Indicate the compression format of input files. If set accordingly, files will be read using a reader that uncompresses the file before scanning its content. Options are  ``, `gzip`, or `auto`. `auto` auto-detects file compression type. Currently, gzip files are the only compressed files auto-detected, based on ".gz" filename extension. `auto` option is useful when ingesting a mix of compressed and uncompressed files with the same filelogreceiver.              |
| `polls_to_archive`                    |  `0`                                    | This settings controls the number of poll cycles to store on disk, rather than being discarded. By default, the receiver will purge the record of readers that have existed for 3 generations. Refer [archiving](#archiving) and [polling](../../pkg/stanza/fileconsumer/design.md#polling) for more details. **Note: This feature is experimental.** |

Note that _by default_, no logs will be read from a file that is not actively being written to because `start_at` defaults to `end`.

### Operators

Each [operator](../../pkg/stanza/docs/operators/README.md#what-is-an-operator) performs a simple responsibility, such as parsing a timestamp or JSON. Chain together operators to process logs into a desired format.

- Every operator has a `type`.
- Every operator can be given a unique `id`. If you use the same type of operator more than once in a pipeline, you must specify an `id`. Otherwise, the `id` defaults to the value of `type`.
- Operators will output to the next operator in the pipeline. The last operator in the pipeline will emit from the receiver. Optionally, the `output` parameter can be used to specify the `id` of another operator to which logs will be passed directly.
- Only parsers and general purpose operators should be used.

### Multiline configuration

If set, the `multiline` configuration block instructs the `file_input` operator to split log entries on a pattern other than newlines.

The `multiline` configuration block must contain exactly one of `line_start_pattern` or `line_end_pattern`. These are regex patterns that
match either the beginning of a new log entry, or the end of a log entry.

The `omit_pattern` setting can be used to omit the start/end pattern from each entry.

### Supported encodings

| Key         | Description
| ---         | ---                                                              |
| `nop`       | No encoding validation. Treats the file as a stream of raw bytes |
| `utf-8`     | UTF-8 encoding                                                   |
| `utf-8-raw` | UTF-8 encoding without replacing invalid UTF-8 bytes             |
| `utf-16le`  | UTF-16 encoding with little-endian byte order                    |
| `utf-16be`  | UTF-16 encoding with big-endian byte order                       |
| `ascii`     | ASCII encoding                                                   |
| `big5`      | The Big5 Chinese character encoding                              |

Other less common encodings are supported on a best-effort basis. See [https://www.iana.org/assignments/character-sets/character-sets.xhtml](https://www.iana.org/assignments/character-sets/character-sets.xhtml) for other encodings available.

### Header Metadata Parsing

To enable header metadata parsing, the `filelog.allowHeaderMetadataParsing` feature gate must be set, and `start_at` must be `beginning`.

If set, the file input operator will attempt to read a header from the start of the file. Each header line must match the `header.pattern` pattern. Each line is emitted into a pipeline defined by `header.metadata_operators`. Any attributes on the resultant entry from the embedded pipeline will be merged with the attributes from previous lines (attribute collisions will be resolved with an upsert strategy). After all header lines are read, the final merged header attributes will be present on every log line that is emitted for the file.

The header lines are not emitted by the receiver.

## Additional Terminology and Features

- An [entry](../../pkg/stanza/docs/types/entry.md) is the base representation of log data as it moves through a pipeline. All operators either create, modify, or consume entries.
- A [field](../../pkg/stanza/docs/types/field.md) is used to reference values in an entry.
- A common [expression](../../pkg/stanza/docs/types/expression.md) syntax is used in several operators. For example, expressions can be used to [filter](../../pkg/stanza/docs/operators/filter.md) or [route](../../pkg/stanza/docs/operators/router.md) entries.

### Parsers with Embedded Operations

Many parsers operators can be configured to embed certain followup operations such as timestamp and severity parsing. For more information, see [complex parsers](../../pkg/stanza/docs/types/parsers.md#complex-parsers).

### Time parameters

All time parameters must have the unit of time specified. e.g.: `200ms`, `1s`, `1m`. 

### Log Rotation

File Log Receiver can read files that are being rotated. 

## Example - Tailing a simple json file

Receiver Configuration
```yaml
receivers:
  filelog:
    include: [ /var/log/myservice/*.json ]
    operators:
      - type: json_parser
        timestamp:
          parse_from: attributes.time
          layout: '%Y-%m-%d %H:%M:%S'
```

## Example - Tailing a plaintext file

Receiver Configuration
```yaml
receivers:
  filelog:
    include: [ /simple.log ]
    operators:
      - type: regex_parser
        regex: '^(?P<time>\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}) (?P<sev>[A-Z]*) (?P<msg>.*)$'
        timestamp:
          parse_from: attributes.time
          layout: '%Y-%m-%d %H:%M:%S'
        severity:
          parse_from: attributes.sev
```

The above configuration will read logs from the "simple.log" file. Some examples of logs that it will read:
```
2023-06-19 05:20:50 ERROR This is a test error message
2023-06-20 12:50:00 DEBUG This is a test debug message
```

## Example - Multiline logs parsing

Receiver Configuration
```yaml
receivers:
  filelog:
    include:
    - /var/log/example/multiline.log
    multiline:
      line_start_pattern: ^Exception
```

The above configuration will be able to parse multiline logs, splitting every time the `^Exception` pattern is met.

```
Exception in thread 1 "main" java.lang.NullPointerException
        at com.example.myproject.Book.getTitle(Book.java:16)
        at com.example.myproject.Author.getBookTitles(Author.java:25)
        at com.example.myproject.Bootstrap.main(Bootstrap.java:14)
Exception in thread 2 "main" java.lang.NullPointerException
        at com.example.myproject.Book.getTitle(Book.java:16)
        at com.example.myproject.Author.getBookTitles(Author.java:25)
        at com.example.myproject.Bootstrap.main(Bootstrap.java:44)
```

## Example - Reading compressed log files

Receiver Configuration
```yaml
receivers:
  filelog:
    include:
    - /var/log/example/compressed.log.gz
    compression: gzip
```

The above configuration will be able to read gzip compressed log files by setting the `compression` option to `gzip`.
When this option is set, all files ending with that suffix are scanned using a gzip reader that decompresses the file content
before scanning through it. Please note that if the compressed file is expected to be updated, the additional compressed logs must be appended to the
compressed file, rather than recompressing the whole content and overwriting the previous file.

## Offset tracking

The `storage` setting allows you to define the proper storage extension for storing file offsets.
While the storage parameter can ensure that log files are consumed accurately, it is possible that
logs are dropped while moving downstream through other components in the collector.
For additional resiliency, see [Fault tolerant log collection example](../../examples/fault-tolerant-logs-collection/README.md)

Here is some of the information the file log receiver stores:
- The number of files it is currently tracking (`knownFiles`).
- For each file being tracked:
  - The [fingerprint](../../pkg/stanza/fileconsumer/design.md#fingerprints) of the file (`Fingerprint.first_bytes`).
  - The byte offset from the start of the file, indicating the position in the file from where the
    file log receiver continues reading the file (`Offset`).
  - An arbitrary set of file attributes, such as the name of the file (`FileAttributes`).

Exactly how this information is serialized depends on the type of storage being used.

### Archiving

If `polls_to_archive` setting is used in conjunction with `storage` setting, file offsets older than three poll cycles are stored on disk rather than being discarded. This feature enables the receiver to remember file for a longer period and also aims to use limited amount of memory. 

This is useful when `exclude_older_than` setting is used and the user wants the receiver to remember offsets of files for longer period of times. This helps prevent duplication if a file is modified after the `exclude_older_than` duration has passed.

Note that if the `polls_to_archive` setting is used without specifying `storage`, the receiver will revert to the default behavior i.e. purge the record of readers that have existed for 3 generations.

## Troubleshooting

### Tracking symlinked files
If the receiver is being used to track a symlinked file and the symlink target is expected to change frequently, make sure 
to set the value of the `poll_interval` setting to something lower than the symlink update frequency.

### Telemetry metrics
Enabling [Collector metrics](https://opentelemetry.io/docs/collector/internal-telemetry/#configure-internal-metrics)
will also provide telemetry metrics for the state of the receiver's file consumption.
Specifically, the `otelcol_fileconsumer_open_files` and `otelcol_fileconsumer_reading_files` metrics
are provided.``

## Resource Attributes

The filelog receiver emits logs with the following resource attributes depending on configuration:

| Attribute                      | Type   | Description                                                           | Enabled by Config                  |
|--------------------------------|--------|-----------------------------------------------------------------------|------------------------------------|
| `log.file.name`                | string | The name of the log file                                              | `include_file_name: true` (default)|
| `log.file.path`                | string | The full path of the log file                                         | `include_file_path: true`          |
| `log.file.name_resolved`       | string | The name after symlink resolution                                     | `include_file_name_resolved: true` |
| `log.file.path_resolved`       | string | The path after symlink resolution                                     | `include_file_path_resolved: true` |
| `log.file.owner.name`          | string | The owner of the file (Unix only)                                     | `include_file_owner_name: true`    |
| `log.file.owner.group.name`    | string | The group owner of the file (Unix only)                               | `include_file_owner_group_name: true` |
| `log.file.record_number`       | int    | The sequential record number in the file                              | `include_file_record_number: true` |
| `log.file.record_offset`       | int    | The byte offset of the record in the file                             | `include_file_record_offset: true` |

Additional attributes can be added using the `attributes` and `resource` configuration options.

## Telemetry

### Internal Metrics

The filelog receiver exposes the following metrics for self-observability:

| Metric                                  | Type  | Description                                                    |
|-----------------------------------------|-------|----------------------------------------------------------------|
| `otelcol_fileconsumer_open_files`       | Gauge | Number of files currently being monitored by the receiver      |
| `otelcol_fileconsumer_reading_files`    | Gauge | Number of files actively being read (not idle)                 |

These metrics are only available when [collector internal metrics](https://opentelemetry.io/docs/collector/internal-telemetry/#configure-internal-metrics) are enabled.

## Production Guidance

### Scaling Considerations

**File Count Limits:**
- The `max_concurrent_files` setting (default: 1024) controls how many files are read simultaneously
- When monitoring more files than this limit, files are processed in batches controlled by `max_batches`
- For deployments with >1000 files, consider splitting across multiple collector instances using different include patterns

**Performance Tuning:**
- `poll_interval` (default: 200ms): Reduce for lower latency, increase to reduce CPU usage
- `max_log_size` (default: 1MiB): Set based on your maximum expected log line size
- `fingerprint_size` (default: 1KB): Increase for better file identification if files have similar headers
- `initial_buffer_size` (default: 16KiB): Tune based on average log line length

**Memory Considerations:**
- Memory usage scales with: number of files × buffer sizes × concurrent operations
- Use `polls_to_archive` with `storage` to move old file metadata to disk
- Typical memory usage: ~1-5MB per 100 monitored files

**CPU Considerations:**
- CPU usage primarily driven by:
  - Number of files being polled (`poll_interval`)
  - Complexity of parsing operators
  - File rotation frequency
- Typical CPU usage: 0.1-0.5% per 100 files with simple parsing

### Persistent Storage Configuration

**Stateful Operation:**

The receiver is stateful when configured with a storage extension. It tracks:
- File positions (byte offsets)
- File fingerprints for identification
- Metadata about monitored files

**Configuration Example:**
```yaml
extensions:
  file_storage:
    directory: /var/lib/otelcol/file_storage

receivers:
  filelog:
    include: [/var/log/**/*.log]
    storage: file_storage

service:
  extensions: [file_storage]
  pipelines:
    logs:
      receivers: [filelog]
```

**Graceful Shutdown:**
- The receiver automatically saves state on shutdown
- Use signals (SIGTERM) for graceful shutdown: `kill -TERM <pid>`
- Allow 5-10 seconds for clean shutdown to persist state
- State is flushed periodically during operation

**Restart Behavior:**
- With storage: Resumes from last saved position
- Without storage: Follows `start_at` configuration (default: `end`)

**Storage Requirements:**
- Disk space: ~1-10KB per monitored file
- Write frequency: Every few seconds during active reading
- Ensure storage directory has appropriate permissions

### High Availability Considerations

**Multiple Instances:**
⚠️ **Warning:** Running multiple filelog receiver instances monitoring the same files will cause duplicate log ingestion. The receiver does not support distributed locking or coordination.

**Recommended HA Pattern:**
- Use file-based sharding: Different instances monitor different file patterns
- Deploy as DaemonSet in Kubernetes (one instance per node)
- Use node affinity to ensure consistent assignment

### Known Limitations and Common Pitfalls

**Limitations:**

1. **No Network File Systems:** 
   - NFS, CIFS, and other network filesystems are not officially supported
   - File locking and change detection may not work reliably
   - Use with caution and thorough testing

2. **Large Files:**
   - Files larger than available memory may cause issues
   - The receiver reads files incrementally, but very long single log lines (>1GiB) can cause OOM

3. **Rotation Detection:**
   - Relies on file fingerprinting (first bytes of file)
   - If log files have identical headers, rotation may not be detected correctly
   - Increase `fingerprint_size` if experiencing issues

4. **Windows Limitations:**
   - `acquire_fs_lock` not supported on Windows
   - `include_file_owner_name` and `include_file_owner_group_name` not supported

5. **Header Parsing:**
   - Only works when `start_at: beginning`
   - Headers are re-parsed after each receiver restart

**Common Pitfalls:**

1. **Missing Logs on Restart:**
   - **Cause:** No storage configured + `start_at: end` (default)
   - **Solution:** Configure storage extension or set `start_at: beginning`

2. **Duplicate Logs:**
   - **Cause:** Multiple receiver instances, storage directory shared/corrupted, or fingerprint changes
   - **Solution:** Ensure unique storage per instance, fix permissions, or stabilize fingerprints

3. **High Memory Usage:**
   - **Cause:** Too many concurrent files or large `max_log_size`
   - **Solution:** Reduce `max_concurrent_files`, tune `max_log_size`, use `polls_to_archive`

4. **Logs Not Being Read:**
   - **Cause:** Incorrect glob pattern, permission issues, or `exclude_older_than` filtering
   - **Solution:** Verify patterns match files, check permissions, review exclusion settings

5. **Slow Log Ingestion:**
   - **Cause:** Low `poll_interval`, slow parsing operators, or I/O bottleneck
   - **Solution:** Tune `poll_interval`, simplify operators, check disk I/O

6. **Rotation Not Detected:**
   - **Cause:** Files have identical headers within `fingerprint_size`
   - **Solution:** Increase `fingerprint_size` or ensure logs have unique headers

## Compatibility

### External Dependencies

**Operating Systems:**
- Linux: Fully supported (kernel 3.10+)
- macOS: Fully supported (10.13+)
- Windows: Supported with limitations (see above)
- Other Unix-like systems: Should work but not regularly tested

**Filesystem Requirements:**
- Standard POSIX filesystems (ext4, xfs, btrfs, etc.)
- File change notifications via stat() calls
- Support for file size and modification time queries

**Go Version:**
- Built with Go 1.21+ (matches OpenTelemetry Collector core requirements)

**Storage Extensions:**
- Compatible with any storage extension implementing the storage interface
- Tested with: file_storage, memory_limiter

### Version Compatibility

The filelog receiver maintains backward compatibility with configuration options across minor versions. Breaking changes are only introduced in major versions following the OpenTelemetry Collector's stability guarantees.

**Configuration Stability:**
- Stable config options maintain backward compatibility
- Beta/alpha options may change (marked in documentation)
- Deprecated options trigger warnings but continue to work for at least one major version

## Feature Gates

### `filelog.allowFileDeletion`

**Description:** Enables the `delete_after_read` configuration option, which allows the receiver to delete files after reading them.

**When to use:** 
- Processing one-time batch log files
- Automatic cleanup of processed logs
- Staging directories with transient log files

**Stability:** Beta (enabled by default in v0.90.0+)

**Risks:**
- Data loss if downstream pipeline fails after deletion
- Cannot recover logs after deletion
- Use only with reliable downstream and/or backups

### `filelog.allowHeaderMetadataParsing`

**Description:** Enables header metadata parsing feature, allowing extraction of metadata from file headers.

**When to use:**
- Log files with metadata headers (e.g., CSV with column definitions)
- Consistent metadata across all log lines in a file
- Reducing per-line metadata duplication

**Stability:** Alpha (disabled by default)

**Requirements:**
- Must set `start_at: beginning`
- Cannot be used with `start_at: end`

### `filelog.decompressFingerprint`

**Description:** Computes file fingerprints after decompression for compressed files, enabling more accurate file identification.

**When to use:**
- Monitoring compressed log files with rotation
- More reliable duplicate detection for gzipped files
- When compressed file headers are not unique

**Stability:**
- Alpha (disabled by default) in v0.128.0
- Beta (enabled by default) in v0.133.0
- Stable (cannot be disabled) in v0.142.0

**Migration Impact:**
- Enabling this will cause existing gzipped files to be re-ingested once
- Fingerprints are recalculated using decompressed content
- Plan for duplicate logs during migration

### `stanza.synchronousLogEmitter`

**Description:** Controls whether logs are emitted synchronously or in batches.

**When to use:**
- Synchronous (true): Lower memory usage, potentially higher latency
- Batching (false): Better throughput, higher memory usage

**Default:** false (batching enabled) 