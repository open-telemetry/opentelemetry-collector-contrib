resourceMetrics:
  - resource:
      attributes:
        - key: spark.application.id
          value:
            stringValue: local-1682603253681
        - key: spark.application.name
          value:
            stringValue: streaming-example
    scopeMetrics:
      - metrics:
          - description: Disk space used by the BlockManager.
            name: spark.driver.block_manager.disk.usage
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            unit: mb
          - description: Memory usage for the driver's BlockManager.
            name: spark.driver.block_manager.memory.usage
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "0"
                  attributes:
                    - key: location
                      value:
                        stringValue: off_heap
                    - key: state
                      value:
                        stringValue: free
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
                - asInt: "0"
                  attributes:
                    - key: location
                      value:
                        stringValue: off_heap
                    - key: state
                      value:
                        stringValue: used
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
                - asInt: "434"
                  attributes:
                    - key: location
                      value:
                        stringValue: on_heap
                    - key: state
                      value:
                        stringValue: free
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
                - asInt: "0"
                  attributes:
                    - key: location
                      value:
                        stringValue: on_heap
                    - key: state
                      value:
                        stringValue: used
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            unit: mb
          - description: Average time spent during CodeGenerator source code compilation operations.
            gauge:
              dataPoints:
                - asDouble: 102.34631777515023
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: spark.driver.code_generator.compilation.average_time
            unit: ms
          - description: Number of source code compilation operations performed by the CodeGenerator.
            name: spark.driver.code_generator.compilation.count
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "8"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
              isMonotonic: true
            unit: '{ compilation }'
          - description: Average class size of the classes generated by the CodeGenerator.
            gauge:
              dataPoints:
                - asDouble: 2409.8125357816552
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: spark.driver.code_generator.generated_class.average_size
            unit: bytes
          - description: Number of classes generated by the CodeGenerator.
            name: spark.driver.code_generator.generated_class.count
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "22"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
              isMonotonic: true
            unit: '{ class }'
          - description: Average method size of the classes generated by the CodeGenerator.
            gauge:
              dataPoints:
                - asDouble: 61.34019077461248
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: spark.driver.code_generator.generated_method.average_size
            unit: bytes
          - description: Number of methods generated by the CodeGenerator.
            name: spark.driver.code_generator.generated_method.count
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "81"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
              isMonotonic: true
            unit: '{ method }'
          - description: Average size of the source code generated by a CodeGenerator code generation operation.
            gauge:
              dataPoints:
                - asDouble: 5193.878352804497
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: spark.driver.code_generator.source_code.average_size
            unit: bytes
          - description: Number of source code generation operations performed by the CodeGenerator.
            name: spark.driver.code_generator.source_code.operations
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "8"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
              isMonotonic: true
            unit: '{ operation }'
          - description: Number of active jobs currently being processed by the DAGScheduler.
            name: spark.driver.dag_scheduler.job.active
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            unit: '{ job }'
          - description: Number of jobs that have been submitted to the DAGScheduler.
            name: spark.driver.dag_scheduler.job.count
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
              isMonotonic: true
            unit: '{ job }'
          - description: Number of stages the DAGScheduler is either running or needs to run.
            name: spark.driver.dag_scheduler.stage.count
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "1"
                  attributes:
                    - key: status
                      value:
                        stringValue: running
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
                - asInt: "0"
                  attributes:
                    - key: status
                      value:
                        stringValue: waiting
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            unit: '{ stage }'
          - description: Number of failed stages run by the DAGScheduler.
            name: spark.driver.dag_scheduler.stage.failed
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
              isMonotonic: true
            unit: '{ stage }'
          - description: Number of garbage collection operations performed by the driver.
            name: spark.driver.executor.gc.operations
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "0"
                  attributes:
                    - key: gc_type
                      value:
                        stringValue: major
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
                - asInt: "33"
                  attributes:
                    - key: gc_type
                      value:
                        stringValue: minor
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
              isMonotonic: true
            unit: '{ gc_operation }'
          - description: Total elapsed time during garbage collection operations performed by the driver.
            name: spark.driver.executor.gc.time
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "0"
                  attributes:
                    - key: gc_type
                      value:
                        stringValue: major
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
                - asInt: "97"
                  attributes:
                    - key: gc_type
                      value:
                        stringValue: minor
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
              isMonotonic: true
            unit: ms
          - description: Amount of execution memory currently used by the driver.
            name: spark.driver.executor.memory.execution
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "0"
                  attributes:
                    - key: location
                      value:
                        stringValue: off_heap
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
                - asInt: "0"
                  attributes:
                    - key: location
                      value:
                        stringValue: on_heap
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            unit: bytes
          - description: Amount of memory used by the driver's JVM.
            name: spark.driver.executor.memory.jvm
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "158895192"
                  attributes:
                    - key: location
                      value:
                        stringValue: off_heap
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
                - asInt: "154858904"
                  attributes:
                    - key: location
                      value:
                        stringValue: on_heap
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            unit: bytes
          - description: Amount of pool memory currently used by the driver.
            name: spark.driver.executor.memory.pool
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "32769"
                  attributes:
                    - key: type
                      value:
                        stringValue: direct
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
                - asInt: "0"
                  attributes:
                    - key: type
                      value:
                        stringValue: mapped
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            unit: bytes
          - description: Amount of storage memory currently used by the driver.
            name: spark.driver.executor.memory.storage
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "0"
                  attributes:
                    - key: location
                      value:
                        stringValue: off_heap
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
                - asInt: "578796"
                  attributes:
                    - key: location
                      value:
                        stringValue: on_heap
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            unit: bytes
          - description: Number of file cache hits on the HiveExternalCatalog.
            name: spark.driver.hive_external_catalog.file_cache_hits
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
              isMonotonic: true
            unit: '{ hit }'
          - description: Number of files discovered while listing the partitions of a table in the Hive metastore
            name: spark.driver.hive_external_catalog.files_discovered
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
              isMonotonic: true
            unit: '{ file }'
          - description: Number of calls to the underlying Hive Metastore client made by the Spark application.
            name: spark.driver.hive_external_catalog.hive_client_calls
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
              isMonotonic: true
            unit: '{ call }'
          - description: Number of parallel listing jobs initiated by the HiveExternalCatalog when listing partitions of a table.
            name: spark.driver.hive_external_catalog.parallel_listing_jobs
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
              isMonotonic: true
            unit: '{ listing_job }'
          - description: Table partitions fetched by the HiveExternalCatalog.
            name: spark.driver.hive_external_catalog.partitions_fetched
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
              isMonotonic: true
            unit: '{ partition }'
          - description: Current CPU time taken by the Spark driver.
            name: spark.driver.jvm_cpu_time
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "32742193000"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
              isMonotonic: true
            unit: ns
          - description: Number of events that have been dropped by the LiveListenerBus.
            name: spark.driver.live_listener_bus.dropped
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
              isMonotonic: true
            unit: '{ event }'
          - description: Number of events that have been posted on the LiveListenerBus.
            name: spark.driver.live_listener_bus.posted
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "248"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
              isMonotonic: true
            unit: '{ event }'
          - description: Average time taken for the LiveListenerBus to process an event posted to it.
            gauge:
              dataPoints:
                - asDouble: 2.570765936935471
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: spark.driver.live_listener_bus.processing_time.average
            unit: ms
          - description: Number of events currently waiting to be processed by the LiveListenerBus.
            name: spark.driver.live_listener_bus.queue_size
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            unit: '{ event }'
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/apachesparkreceiver
          version: latest
