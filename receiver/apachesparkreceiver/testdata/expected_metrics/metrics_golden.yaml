resourceMetrics:
  - resource:
      attributes:
        - key: spark.application.id
          value:
            stringValue: local-1682603253681
        - key: spark.application.name
          value:
            stringValue: streaming-example
    scopeMetrics:
      - metrics:
          - description: Disk space used by the BlockManager.
            name: spark.driver.block_manager.disk.usage
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            unit: mb
          - description: Memory usage for the driver's BlockManager.
            name: spark.driver.block_manager.memory.usage
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "0"
                  attributes:
                    - key: location
                      value:
                        stringValue: off_heap
                    - key: state
                      value:
                        stringValue: free
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
                - asInt: "0"
                  attributes:
                    - key: location
                      value:
                        stringValue: off_heap
                    - key: state
                      value:
                        stringValue: used
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
                - asInt: "434"
                  attributes:
                    - key: location
                      value:
                        stringValue: on_heap
                    - key: state
                      value:
                        stringValue: free
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
                - asInt: "0"
                  attributes:
                    - key: location
                      value:
                        stringValue: on_heap
                    - key: state
                      value:
                        stringValue: used
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            unit: mb
          - description: Average time spent during CodeGenerator source code compilation operations.
            gauge:
              dataPoints:
                - asDouble: 102.34631777515023
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: spark.driver.code_generator.compilation.average_time
            unit: ms
          - description: Number of source code compilation operations performed by the CodeGenerator.
            name: spark.driver.code_generator.compilation.count
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "8"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
              isMonotonic: true
            unit: '{ compilation }'
          - description: Average class size of the classes generated by the CodeGenerator.
            gauge:
              dataPoints:
                - asDouble: 2409.8125357816552
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: spark.driver.code_generator.generated_class.average_size
            unit: bytes
          - description: Number of classes generated by the CodeGenerator.
            name: spark.driver.code_generator.generated_class.count
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "22"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
              isMonotonic: true
            unit: '{ class }'
          - description: Average method size of the classes generated by the CodeGenerator.
            gauge:
              dataPoints:
                - asDouble: 61.34019077461248
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: spark.driver.code_generator.generated_method.average_size
            unit: bytes
          - description: Number of methods generated by the CodeGenerator.
            name: spark.driver.code_generator.generated_method.count
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "81"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
              isMonotonic: true
            unit: '{ method }'
          - description: Average size of the source code generated by a CodeGenerator code generation operation.
            gauge:
              dataPoints:
                - asDouble: 5193.878352804497
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: spark.driver.code_generator.source_code.average_size
            unit: bytes
          - description: Number of source code generation operations performed by the CodeGenerator.
            name: spark.driver.code_generator.source_code.operations
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "8"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
              isMonotonic: true
            unit: '{ operation }'
          - description: Number of active jobs currently being processed by the DAGScheduler.
            name: spark.driver.dag_scheduler.job.active
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            unit: '{ job }'
          - description: Number of jobs that have been submitted to the DAGScheduler.
            name: spark.driver.dag_scheduler.job.count
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
              isMonotonic: true
            unit: '{ job }'
          - description: Number of stages the DAGScheduler is either running or needs to run.
            name: spark.driver.dag_scheduler.stage.count
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "1"
                  attributes:
                    - key: status
                      value:
                        stringValue: running
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
                - asInt: "0"
                  attributes:
                    - key: status
                      value:
                        stringValue: waiting
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            unit: '{ stage }'
          - description: Number of failed stages run by the DAGScheduler.
            name: spark.driver.dag_scheduler.stage.failed
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
              isMonotonic: true
            unit: '{ stage }'
          - description: Number of garbage collection operations performed by the driver.
            name: spark.driver.executor.gc.operations
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "0"
                  attributes:
                    - key: gc_type
                      value:
                        stringValue: major
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
                - asInt: "33"
                  attributes:
                    - key: gc_type
                      value:
                        stringValue: minor
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
              isMonotonic: true
            unit: '{ gc_operation }'
          - description: Total elapsed time during garbage collection operations performed by the driver.
            name: spark.driver.executor.gc.time
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "0"
                  attributes:
                    - key: gc_type
                      value:
                        stringValue: major
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
                - asInt: "97"
                  attributes:
                    - key: gc_type
                      value:
                        stringValue: minor
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
              isMonotonic: true
            unit: ms
          - description: Amount of execution memory currently used by the driver.
            name: spark.driver.executor.memory.execution
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "0"
                  attributes:
                    - key: location
                      value:
                        stringValue: off_heap
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
                - asInt: "0"
                  attributes:
                    - key: location
                      value:
                        stringValue: on_heap
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            unit: bytes
          - description: Amount of memory used by the driver's JVM.
            name: spark.driver.executor.memory.jvm
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "158895192"
                  attributes:
                    - key: location
                      value:
                        stringValue: off_heap
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
                - asInt: "154858904"
                  attributes:
                    - key: location
                      value:
                        stringValue: on_heap
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            unit: bytes
          - description: Amount of pool memory currently used by the driver.
            name: spark.driver.executor.memory.pool
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "32769"
                  attributes:
                    - key: type
                      value:
                        stringValue: direct
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
                - asInt: "0"
                  attributes:
                    - key: type
                      value:
                        stringValue: mapped
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            unit: bytes
          - description: Amount of storage memory currently used by the driver.
            name: spark.driver.executor.memory.storage
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "0"
                  attributes:
                    - key: location
                      value:
                        stringValue: off_heap
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
                - asInt: "578796"
                  attributes:
                    - key: location
                      value:
                        stringValue: on_heap
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            unit: bytes
          - description: Number of file cache hits on the HiveExternalCatalog.
            name: spark.driver.hive_external_catalog.file_cache_hits
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
              isMonotonic: true
            unit: '{ hit }'
          - description: Number of files discovered while listing the partitions of a table in the Hive metastore
            name: spark.driver.hive_external_catalog.files_discovered
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
              isMonotonic: true
            unit: '{ file }'
          - description: Number of calls to the underlying Hive Metastore client made by the Spark application.
            name: spark.driver.hive_external_catalog.hive_client_calls
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
              isMonotonic: true
            unit: '{ call }'
          - description: Number of parallel listing jobs initiated by the HiveExternalCatalog when listing partitions of a table.
            name: spark.driver.hive_external_catalog.parallel_listing_jobs
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
              isMonotonic: true
            unit: '{ listing_job }'
          - description: Table partitions fetched by the HiveExternalCatalog.
            name: spark.driver.hive_external_catalog.partitions_fetched
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
              isMonotonic: true
            unit: '{ partition }'
          - description: Current CPU time taken by the Spark driver.
            name: spark.driver.jvm_cpu_time
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "32742193000"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
              isMonotonic: true
            unit: ns
          - description: Number of events that have been dropped by the LiveListenerBus.
            name: spark.driver.live_listener_bus.dropped
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
              isMonotonic: true
            unit: '{ event }'
          - description: Number of events that have been posted on the LiveListenerBus.
            name: spark.driver.live_listener_bus.posted
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "248"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
              isMonotonic: true
            unit: '{ event }'
          - description: Average time taken for the LiveListenerBus to process an event posted to it.
            gauge:
              dataPoints:
                - asDouble: 2.570765936935471
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: spark.driver.live_listener_bus.processing_time.average
            unit: ms
          - description: Number of events currently waiting to be processed by the LiveListenerBus.
            name: spark.driver.live_listener_bus.queue_size
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            unit: '{ event }'
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/apachesparkreceiver
          version: latest
  - resource:
      attributes:
        - key: spark.application.id
          value:
            stringValue: local-1682603253681
        - key: spark.application.name
          value:
            stringValue: streaming-example
        - key: spark.executor.id
          value:
            stringValue: driver
    scopeMetrics:
      - metrics:
          - description: Disk space used by this executor for RDD storage.
            name: spark.executor.disk.usage
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            unit: bytes
          - description: Elapsed time the JVM spent in garbage collection in this executor.
            name: spark.executor.gc_time
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "221"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
              isMonotonic: true
            unit: ms
          - description: Amount of data input for this executor.
            name: spark.executor.input_size
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
              isMonotonic: true
            unit: bytes
          - description: Storage memory used by this executor.
            name: spark.executor.memory.usage
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "100596"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            unit: bytes
          - description: Amount of data written and read during shuffle operations for this executor.
            name: spark.executor.shuffle.io.size
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "0"
                  attributes:
                    - key: direction
                      value:
                        stringValue: in
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
                - asInt: "0"
                  attributes:
                    - key: direction
                      value:
                        stringValue: out
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
              isMonotonic: true
            unit: bytes
          - description: The executor's storage memory usage.
            name: spark.executor.storage_memory.usage
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "0"
                  attributes:
                    - key: location
                      value:
                        stringValue: off_heap
                    - key: state
                      value:
                        stringValue: free
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
                - asInt: "0"
                  attributes:
                    - key: location
                      value:
                        stringValue: off_heap
                    - key: state
                      value:
                        stringValue: used
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
                - asInt: "0"
                  attributes:
                    - key: location
                      value:
                        stringValue: on_heap
                    - key: state
                      value:
                        stringValue: free
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
                - asInt: "0"
                  attributes:
                    - key: location
                      value:
                        stringValue: on_heap
                    - key: state
                      value:
                        stringValue: used
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            unit: bytes
          - description: Number of tasks currently running in this executor.
            name: spark.executor.task.active
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            unit: '{ task }'
          - description: Maximum number of tasks that can run concurrently in this executor.
            name: spark.executor.task.limit
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "12"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            unit: '{ task }'
          - description: Number of tasks with a specific result in this executor.
            name: spark.executor.task.result
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "212"
                  attributes:
                    - key: result
                      value:
                        stringValue: completed
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
                - asInt: "0"
                  attributes:
                    - key: result
                      value:
                        stringValue: failed
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
              isMonotonic: true
            unit: '{ task }'
          - description: Elapsed time the JVM spent executing tasks in this executor.
            name: spark.executor.time
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "149891"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
              isMonotonic: true
            unit: ms
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/apachesparkreceiver
          version: latest
  - resource:
      attributes:
        - key: spark.application.id
          value:
            stringValue: local-1682603253681
        - key: spark.application.name
          value:
            stringValue: streaming-example
        - key: spark.job.id
          value:
            intValue: "0"
    scopeMetrics:
      - metrics:
          - description: Number of active stages in this job.
            name: spark.job.stage.active
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            unit: '{ stage }'
          - description: Number of stages with a specific result in this job.
            name: spark.job.stage.result
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "2"
                  attributes:
                    - key: result
                      value:
                        stringValue: completed
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
                - asInt: "0"
                  attributes:
                    - key: result
                      value:
                        stringValue: failed
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
                - asInt: "0"
                  attributes:
                    - key: result
                      value:
                        stringValue: skipped
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
              isMonotonic: true
            unit: '{ stage }'
          - description: Number of active tasks in this job.
            name: spark.job.task.active
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            unit: '{ task }'
          - description: Number of tasks with a specific result in this job.
            name: spark.job.task.result
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "212"
                  attributes:
                    - key: result
                      value:
                        stringValue: completed
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
                - asInt: "0"
                  attributes:
                    - key: result
                      value:
                        stringValue: failed
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
                - asInt: "0"
                  attributes:
                    - key: result
                      value:
                        stringValue: skipped
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
              isMonotonic: true
            unit: '{ task }'
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/apachesparkreceiver
          version: latest
  - resource:
      attributes:
        - key: spark.application.id
          value:
            stringValue: local-1682603253681
        - key: spark.application.name
          value:
            stringValue: streaming-example
        - key: spark.stage.id
          value:
            intValue: "0"
    scopeMetrics:
      - metrics:
          - description: The amount of disk space used for storing portions of overly large data chunks that couldn't fit in memory in this stage.
            name: spark.stage.disk.spilled
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
              isMonotonic: true
            unit: bytes
          - description: CPU time spent by the executor in this stage.
            name: spark.stage.executor.cpu_time
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "1505965000"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
              isMonotonic: true
            unit: ns
          - description: Amount of time spent by the executor in this stage.
            name: spark.stage.executor.run_time
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "8927"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
              isMonotonic: true
            unit: ms
          - description: Number of records written and read in this stage.
            name: spark.stage.io.records
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "0"
                  attributes:
                    - key: direction
                      value:
                        stringValue: in
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
                - asInt: "0"
                  attributes:
                    - key: direction
                      value:
                        stringValue: out
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
              isMonotonic: true
            unit: '{ record }'
          - description: Amount of data written and read at this stage.
            name: spark.stage.io.size
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "0"
                  attributes:
                    - key: direction
                      value:
                        stringValue: in
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
                - asInt: "0"
                  attributes:
                    - key: direction
                      value:
                        stringValue: out
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
              isMonotonic: true
            unit: bytes
          - description: The amount of time the JVM spent on garbage collection in this stage.
            name: spark.stage.jvm_gc_time
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "156"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
              isMonotonic: true
            unit: ms
          - description: Peak memory used by internal data structures created during shuffles, aggregations and joins in this stage.
            name: spark.stage.memory.peak
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "3145728"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
              isMonotonic: true
            unit: bytes
          - description: The amount of memory moved to disk due to size constraints (spilled) in this stage.
            name: spark.stage.memory.spilled
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
              isMonotonic: true
            unit: bytes
          - description: Number of blocks fetched in shuffle operations in this stage.
            name: spark.stage.shuffle.blocks_fetched
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "0"
                  attributes:
                    - key: source
                      value:
                        stringValue: local
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
                - asInt: "0"
                  attributes:
                    - key: source
                      value:
                        stringValue: remote
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
              isMonotonic: true
            unit: '{ block }'
          - description: Time spent in this stage waiting for remote shuffle blocks.
            name: spark.stage.shuffle.fetch_wait_time
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
              isMonotonic: true
            unit: ms
          - description: Amount of data read to disk in shuffle operations (sometimes required for large blocks, as opposed to the default behavior of reading into memory).
            name: spark.stage.shuffle.io.disk
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
              isMonotonic: true
            unit: bytes
          - description: Amount of data read in shuffle operations in this stage.
            name: spark.stage.shuffle.io.read.size
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "0"
                  attributes:
                    - key: source
                      value:
                        stringValue: local
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
                - asInt: "0"
                  attributes:
                    - key: source
                      value:
                        stringValue: remote
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
              isMonotonic: true
            unit: bytes
          - description: Number of records written or read in shuffle operations in this stage.
            name: spark.stage.shuffle.io.records
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "0"
                  attributes:
                    - key: direction
                      value:
                        stringValue: in
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
                - asInt: "0"
                  attributes:
                    - key: direction
                      value:
                        stringValue: out
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
              isMonotonic: true
            unit: '{ record }'
          - description: Amount of data written in shuffle operations in this stage.
            name: spark.stage.shuffle.io.write.size
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
              isMonotonic: true
            unit: bytes
          - description: Time spent blocking on writes to disk or buffer cache in this stage.
            name: spark.stage.shuffle.write_time
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
              isMonotonic: true
            unit: ns
          - description: A one-hot encoding representing the status of this stage.
            name: spark.stage.status
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "0"
                  attributes:
                    - key: active
                      value:
                        boolValue: false
                    - key: complete
                      value:
                        boolValue: true
                    - key: failed
                      value:
                        boolValue: false
                    - key: pending
                      value:
                        boolValue: false
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            unit: '{ status }'
          - description: Number of active tasks in this stage.
            name: spark.stage.task.active
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            unit: '{ task }'
          - description: Number of tasks with a specific result in this stage.
            name: spark.stage.task.result
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "12"
                  attributes:
                    - key: result
                      value:
                        stringValue: completed
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
                - asInt: "0"
                  attributes:
                    - key: result
                      value:
                        stringValue: failed
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
                - asInt: "0"
                  attributes:
                    - key: result
                      value:
                        stringValue: killed
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
              isMonotonic: true
            unit: '{ task }'
          - description: The amount of data transmitted back to the driver by all the tasks in this stage.
            name: spark.stage.task.result_size
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "29568"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
              isMonotonic: true
            unit: bytes
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/apachesparkreceiver
          version: latest
  - resource:
      attributes:
        - key: spark.application.id
          value:
            stringValue: local-1682603253681
        - key: spark.application.name
          value:
            stringValue: streaming-example
        - key: spark.stage.id
          value:
            intValue: "1"
    scopeMetrics:
      - metrics:
          - description: The amount of disk space used for storing portions of overly large data chunks that couldn't fit in memory in this stage.
            name: spark.stage.disk.spilled
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
              isMonotonic: true
            unit: bytes
          - description: CPU time spent by the executor in this stage.
            name: spark.stage.executor.cpu_time
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "8516746000"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
              isMonotonic: true
            unit: ns
          - description: Amount of time spent by the executor in this stage.
            name: spark.stage.executor.run_time
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "96205"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
              isMonotonic: true
            unit: ms
          - description: Number of records written and read in this stage.
            name: spark.stage.io.records
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "0"
                  attributes:
                    - key: direction
                      value:
                        stringValue: in
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
                - asInt: "0"
                  attributes:
                    - key: direction
                      value:
                        stringValue: out
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
              isMonotonic: true
            unit: '{ record }'
          - description: Amount of data written and read at this stage.
            name: spark.stage.io.size
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "0"
                  attributes:
                    - key: direction
                      value:
                        stringValue: in
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
                - asInt: "0"
                  attributes:
                    - key: direction
                      value:
                        stringValue: out
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
              isMonotonic: true
            unit: bytes
          - description: The amount of time the JVM spent on garbage collection in this stage.
            name: spark.stage.jvm_gc_time
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "1370"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
              isMonotonic: true
            unit: ms
          - description: Peak memory used by internal data structures created during shuffles, aggregations and joins in this stage.
            name: spark.stage.memory.peak
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "157286400"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
              isMonotonic: true
            unit: bytes
          - description: The amount of memory moved to disk due to size constraints (spilled) in this stage.
            name: spark.stage.memory.spilled
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
              isMonotonic: true
            unit: bytes
          - description: Number of blocks fetched in shuffle operations in this stage.
            name: spark.stage.shuffle.blocks_fetched
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "0"
                  attributes:
                    - key: source
                      value:
                        stringValue: local
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
                - asInt: "0"
                  attributes:
                    - key: source
                      value:
                        stringValue: remote
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
              isMonotonic: true
            unit: '{ block }'
          - description: Time spent in this stage waiting for remote shuffle blocks.
            name: spark.stage.shuffle.fetch_wait_time
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
              isMonotonic: true
            unit: ms
          - description: Amount of data read to disk in shuffle operations (sometimes required for large blocks, as opposed to the default behavior of reading into memory).
            name: spark.stage.shuffle.io.disk
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
              isMonotonic: true
            unit: bytes
          - description: Amount of data read in shuffle operations in this stage.
            name: spark.stage.shuffle.io.read.size
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "0"
                  attributes:
                    - key: source
                      value:
                        stringValue: local
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
                - asInt: "0"
                  attributes:
                    - key: source
                      value:
                        stringValue: remote
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
              isMonotonic: true
            unit: bytes
          - description: Number of records written or read in shuffle operations in this stage.
            name: spark.stage.shuffle.io.records
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "0"
                  attributes:
                    - key: direction
                      value:
                        stringValue: in
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
                - asInt: "0"
                  attributes:
                    - key: direction
                      value:
                        stringValue: out
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
              isMonotonic: true
            unit: '{ record }'
          - description: Amount of data written in shuffle operations in this stage.
            name: spark.stage.shuffle.io.write.size
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
              isMonotonic: true
            unit: bytes
          - description: Time spent blocking on writes to disk or buffer cache in this stage.
            name: spark.stage.shuffle.write_time
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
              isMonotonic: true
            unit: ns
          - description: A one-hot encoding representing the status of this stage.
            name: spark.stage.status
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "0"
                  attributes:
                    - key: active
                      value:
                        boolValue: false
                    - key: complete
                      value:
                        boolValue: true
                    - key: failed
                      value:
                        boolValue: false
                    - key: pending
                      value:
                        boolValue: false
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            unit: '{ status }'
          - description: Number of active tasks in this stage.
            name: spark.stage.task.active
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            unit: '{ task }'
          - description: Number of tasks with a specific result in this stage.
            name: spark.stage.task.result
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "200"
                  attributes:
                    - key: result
                      value:
                        stringValue: completed
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
                - asInt: "0"
                  attributes:
                    - key: result
                      value:
                        stringValue: failed
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
                - asInt: "0"
                  attributes:
                    - key: result
                      value:
                        stringValue: killed
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
              isMonotonic: true
            unit: '{ task }'
          - description: The amount of data transmitted back to the driver by all the tasks in this stage.
            name: spark.stage.task.result_size
            sum:
              aggregationTemporality: 2
              dataPoints:
                - asInt: "1478081"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
              isMonotonic: true
            unit: bytes
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/apachesparkreceiver
          version: latest
